{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook covers all the optimization steps for training an encoder-decoder transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "IPython.notebook.set_autosave_interval(300000)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 300 seconds\n"
     ]
    }
   ],
   "source": [
    "%autosave 300\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/batch/tasks/shared/LS_root/mounts/clusters/insights-model-run2/code/Users/soutrik.chowdhury/EraV2_Transformers\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\n",
    "    \"/mnt/batch/tasks/shared/LS_root/mounts/clusters/insights-model-run2/code/Users/soutrik.chowdhury/EraV2_Transformers\"\n",
    ")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from S18_code.config import get_config, get_weights_file_path\n",
    "from S18_code.model import build_transformer\n",
    "import torch\n",
    "from S18_code.dataloader import get_ds\n",
    "from datasets import load_dataset\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchinfo import summary\n",
    "from torch.optim.lr_scheduler import CyclicLR\n",
    "from torch.optim import Adam\n",
    "from torchmetrics.text import BLEUScore\n",
    "from torch.cuda.amp import GradScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = get_config()[\"device\"]\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0\n",
      "Tesla V100-PCIE-16GB\n",
      "0\n",
      "0\n",
      "Tesla V100-PCIE-16GB\n",
      "15.7725830078125\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n",
      "(7, 0)\n",
      "_CudaDeviceProperties(name='Tesla V100-PCIE-16GB', major=7, minor=0, total_memory=16151MB, multi_processor_count=80)\n",
      "80\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.get_device_name())\n",
    "print(torch.cuda.memory_allocated())\n",
    "print(torch.cuda.memory_reserved())\n",
    "print(torch.cuda.get_device_name(device.index))\n",
    "print(torch.cuda.get_device_properties(device.index).total_memory / 1024**3)\n",
    "print(torch.cuda.memory_summary(device=device.index))\n",
    "print(torch.cuda.get_device_capability(device.index))\n",
    "print(torch.cuda.get_device_properties(device.index))\n",
    "print(torch.cuda.get_device_properties(device.index).multi_processor_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download the dataset and create data via dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_raw = load_dataset(\n",
    "    get_config()[\"datasource\"],\n",
    "    f\"{get_config()['src_lang']}-{get_config()['tgt_lang']}\",\n",
    "    split=\"train\",\n",
    ")\n",
    "print(len(ds_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch wise padding data loading\n",
    "train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(\n",
    "    ds_raw, get_config()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in train_dataloader:\n",
    "    print(data[\"encoder_input\"].shape)\n",
    "    print(data[\"decoder_input\"].shape)\n",
    "    print(data[\"encoder_mask\"].shape)\n",
    "    print(data[\"decoder_mask\"].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in val_dataloader:\n",
    "    print(data[\"encoder_input\"].shape)\n",
    "    print(data[\"decoder_input\"].shape)\n",
    "    print(data[\"encoder_mask\"].shape)\n",
    "    print(data[\"decoder_mask\"].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Initialization and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model = build_transformer(\n",
    "    src_vocab_size=tokenizer_src.get_vocab_size(),\n",
    "    tgt_vocab_size=tokenizer_tgt.get_vocab_size(),\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(transformer_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will do basic training and validation based on:\n",
    "* Application of basic training loop\n",
    "* Application of custom scheduler for learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from S18_code.trainer import (\n",
    "    CustomLRScheduler,\n",
    "    TranslationLoss,\n",
    "    run_training_loop_basic,\n",
    "    run_validation_loop,\n",
    "    greedy_decode,\n",
    "    run_inference_loop,\n",
    "    run_training_loop_opt,\n",
    ")\n",
    "\n",
    "from S18_code.utils import start_timer, end_timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_idx = tokenizer_src.token_to_id(\"[PAD]\")\n",
    "sos_idx = tokenizer_src.token_to_id(\"[SOS]\")\n",
    "eos_idx = tokenizer_src.token_to_id(\"[EOS]\")\n",
    "label_smoothing = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer Adam with weight decay\n",
    "optimizer = Adam(\n",
    "    transformer_model.parameters(),\n",
    "    betas=(0.9, 0.98),\n",
    "    eps=1.0e-9,\n",
    "    lr=get_config()[\"learning_rate\"],\n",
    ")\n",
    "# Learning rate scheduler\n",
    "scheduler = CustomLRScheduler(optimizer, get_config()[\"d_model\"], 1000)\n",
    "# Loss function\n",
    "criterion = TranslationLoss(pad_idx, label_smoothing, tokenizer_tgt)\n",
    "# BLEU score metric\n",
    "metric = BLEUScore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in range(get_config()[\"num_epochs\"]):\n",
    "#     global_step = 0\n",
    "#     train_loss, global_step = run_training_loop_basic(\n",
    "#         transformer_model,\n",
    "#         train_dataloader,\n",
    "#         optimizer,\n",
    "#         criterion,\n",
    "#         device,\n",
    "#         global_step,\n",
    "#         scheduler,\n",
    "#     )\n",
    "#     print(f\"Epoch: {epoch}, Train loss: {train_loss}\")\n",
    "\n",
    "#     val_loss = run_validation_loop(transformer_model, val_dataloader, criterion, device)\n",
    "#     print(f\"Epoch: {epoch}, Validation loss: {val_loss}\")\n",
    "\n",
    "#     run_inference_loop(\n",
    "#         transformer_model,\n",
    "#         val_dataloader,\n",
    "#         tokenizer_tgt,\n",
    "#         device,\n",
    "#         5,\n",
    "#         metric,\n",
    "#         sos_idx,\n",
    "#         eos_idx,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Application of optimization techniques on training loop to improve performance:\n",
    "* Automatic Mixed Precision\n",
    "* Gradient Scaling (Gradient Clipping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_timer()\n",
    "\n",
    "for epoch in range(get_config()[\"num_epochs\"]):\n",
    "    global_step = 0\n",
    "    train_loss, global_step = run_training_loop_opt(\n",
    "        transformer_model,\n",
    "        train_dataloader,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        device,\n",
    "        global_step,\n",
    "        scaler,\n",
    "        scheduler,\n",
    "    )\n",
    "    print(f\"Epoch: {epoch}, Train loss: {train_loss}\")\n",
    "\n",
    "    val_loss = run_validation_loop(transformer_model, val_dataloader, criterion, device)\n",
    "    print(f\"Epoch: {epoch}, Validation loss: {val_loss}\")\n",
    "\n",
    "    run_inference_loop(\n",
    "        transformer_model,\n",
    "        val_dataloader,\n",
    "        tokenizer_tgt,\n",
    "        device,\n",
    "        5,\n",
    "        metric,\n",
    "        sos_idx,\n",
    "        eos_idx,\n",
    "    )\n",
    "\n",
    "end_timer(\"Simple Neural Network\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
