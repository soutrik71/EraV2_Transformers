{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "IPython.notebook.set_autosave_interval(300000)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 300 seconds\n"
     ]
    }
   ],
   "source": [
    "%autosave 300\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/batch/tasks/shared/LS_root/mounts/clusters/insights-model-run2/code/Users/soutrik.chowdhury/EraV2_Transformers\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\n",
    "    \"/mnt/batch/tasks/shared/LS_root/mounts/clusters/insights-model-run2/code/Users/soutrik.chowdhury/EraV2_Transformers\"\n",
    ")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Conventional way - Spacy tokenization and collate batch\n",
    "https://kikaben.com/transformers-data-loader/m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import IterableDataset\n",
    "from torchtext import datasets\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(\n",
    "    name: str, split: str, language_pair: Tuple[str, str]\n",
    ") -> IterableDataset:\n",
    "    dataset_class = eval(f\"datasets.{name}\")\n",
    "    dataset = dataset_class(split=split, language_pair=language_pair)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset has 29K pairs of German and English sentences.\n",
    "dataset = load_dataset(\"Multi30k\", \"train\", (\"de\", \"en\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.',\n",
       " 'Two young, White males are outside near many bushes.')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche. \n",
      " Two young, White males are outside near many bushes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/torch_env/lib/python3.10/site-packages/torch/utils/data/datapipes/iter/combining.py:337: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
      "  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n"
     ]
    }
   ],
   "source": [
    "for de_text, en_text in dataset:\n",
    "    print(de_text, \"\\n\", en_text)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization using spacy\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_tokenizer = spacy.load(\"de_core_news_sm\")\n",
    "en_tokenizer = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Zwei', 'junge', 'weiße', 'Männer', 'sind', 'im', 'Freien', 'in', 'der', 'Nähe', 'vieler', 'Büsche', '.']\n",
      "['Two', 'young', ',', 'White', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.']\n"
     ]
    }
   ],
   "source": [
    "print([token.text for token in de_tokenizer(de_text)])\n",
    "print([token.text for token in en_tokenizer(en_text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engTokenize(text):\n",
    "    \"\"\"\n",
    "    Tokenize an English text and return a list of tokens\n",
    "    \"\"\"\n",
    "    return [token.text for token in en_tokenizer.tokenizer(text)]\n",
    "\n",
    "\n",
    "def deTokenize(text):\n",
    "    \"\"\"\n",
    "    Tokenize a German text and return a list of tokens\n",
    "    \"\"\"\n",
    "    return [token.text for token in de_tokenizer.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Two', 'young', ',', 'White', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.']\n",
      "['Zwei', 'junge', 'weiße', 'Männer', 'sind', 'im', 'Freien', 'in', 'der', 'Nähe', 'vieler', 'Büsche', '.']\n"
     ]
    }
   ],
   "source": [
    "print(engTokenize(en_text))\n",
    "print(deTokenize(de_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/torch_env/lib/python3.10/site-packages/torch/utils/data/datapipes/iter/combining.py:337: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
      "  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n"
     ]
    }
   ],
   "source": [
    "# First we split out the source text and target text\n",
    "source_text = [en_text for de_text, en_text in dataset]\n",
    "target_text = [de_text for de_text, en_text in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29001, 29001)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(source_text), len(target_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_tokens(tokenizer, texts):\n",
    "    \"\"\"\n",
    "    Return a list of unique tokens in the texts\n",
    "    \"\"\"\n",
    "    counter = Counter()\n",
    "    for doc in tokenizer.pipe(texts):\n",
    "        token_texts = []\n",
    "        for token in doc:\n",
    "            token_text = token.text.strip()\n",
    "            if len(token_text) > 0:  # not a white space\n",
    "                token_texts.append(token_text)\n",
    "        counter.update(token_texts)\n",
    "\n",
    "    # unique tokens\n",
    "    tokens = [token for token, count in counter.most_common()]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_tokens = unique_tokens(en_tokenizer, source_text)\n",
    "de_tokens = unique_tokens(de_tokenizer, target_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10832, 19207)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(en_tokens), len(de_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# special token indices\n",
    "UNK_IDX = 0\n",
    "PAD_IDX = 1\n",
    "SOS_IDX = 2\n",
    "EOS_IDX = 3\n",
    "\n",
    "UNK = \"<unk>\"  # Unknown\n",
    "PAD = \"<pad>\"  # Padding\n",
    "SOS = \"<sos>\"  # Start of sentence\n",
    "EOS = \"<eos>\"  # End of sentence\n",
    "\n",
    "SPECIAL_TOKENS = [UNK, PAD, SOS, EOS]\n",
    "\n",
    "\n",
    "class Vocab:\n",
    "    def __init__(\n",
    "        self, tokenizer: spacy.language.Language, tokens: List[str] = []\n",
    "    ) -> None:\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokens = SPECIAL_TOKENS + tokens  # addition of special tokens\n",
    "        self.index_lookup = {self.tokens[i]\n",
    "            : i for i in range(len(self.tokens))}\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.tokens)  # vocab size\n",
    "\n",
    "    def __call__(self, text: str) -> List[int]:\n",
    "        text = text.strip()\n",
    "        return [self.to_index(token.text) for token in self.tokenizer(text)]\n",
    "\n",
    "    def to_index(self, token: str) -> int:\n",
    "        return self.index_lookup[token] if token in self.index_lookup else UNK_IDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_vocab = Vocab(en_tokenizer, en_tokens)\n",
    "de_vocab = Vocab(de_tokenizer, de_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# en_vocab(en_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# de_vocab(de_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch’s DataLoader and collate_fn to encapsulate tokenization and token index processing\n",
    "# We prepend SOS_IDX and append EOS_IDX for target sentences. Finally, we convert token indices into Tensor and keep them in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomData(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        source_text,\n",
    "        target_text,\n",
    "        source_vocab,\n",
    "        target_vocab,\n",
    "        pad_idx,\n",
    "        sos_idx,\n",
    "        eos_idx,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.source_vocab = source_vocab\n",
    "        self.target_vocab = target_vocab\n",
    "        self.pad_idx = pad_idx\n",
    "        self.sos_idx = sos_idx\n",
    "        self.eos_idx = eos_idx\n",
    "        self.source_text = source_text\n",
    "        self.target_text = target_text\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.source_text)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        source_text_sample = self.source_text[idx]\n",
    "        target_text_sample = self.target_text[idx]\n",
    "\n",
    "        # convert text to token indices\n",
    "        source_token_indices = self.source_vocab(source_text_sample)\n",
    "        target_token_indices = self.target_vocab(target_text_sample)\n",
    "\n",
    "        # prepend SOS_IDX and append EOS_IDX for source sentences\n",
    "        source_token_indices = [self.sos_idx] + source_token_indices + [self.eos_idx]\n",
    "        # prepend SOS_IDX and append EOS_IDX for target sentences\n",
    "        target_token_indices = [self.sos_idx] + target_token_indices + [self.eos_idx]\n",
    "\n",
    "        return {\n",
    "            \"encoder_input\": torch.tensor(source_token_indices, dtype=torch.long),\n",
    "            \"decoder_input\": torch.tensor(target_token_indices, dtype=torch.long),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_dataset = CustomData(\n",
    "    source_text=source_text,\n",
    "    target_text=target_text,\n",
    "    source_vocab=en_vocab,\n",
    "    target_vocab=de_vocab,\n",
    "    pad_idx=PAD_IDX,\n",
    "    sos_idx=SOS_IDX,\n",
    "    eos_idx=EOS_IDX,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(src_batch: torch.Tensor, tgt_batch: torch.Tensor):\n",
    "    # ----------------------------------------------------------------------\n",
    "    # [1] padding mask\n",
    "    # ----------------------------------------------------------------------\n",
    "\n",
    "    # (batch_size, 1, max_tgt_seq_len)\n",
    "    src_pad_mask = (src_batch != PAD_IDX).unsqueeze(1)\n",
    "\n",
    "    # (batch_size, 1, max_src_seq_len)\n",
    "    tgt_pad_mask = (tgt_batch != PAD_IDX).unsqueeze(1)\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    # [2] subsequent mask for decoder inputs\n",
    "    # ----------------------------------------------------------------------\n",
    "    max_tgt_sequence_length = tgt_batch.shape[1]\n",
    "    tgt_attention_square = (max_tgt_sequence_length, max_tgt_sequence_length)\n",
    "\n",
    "    # full attention\n",
    "    full_mask = torch.full(tgt_attention_square, 1)\n",
    "\n",
    "    # subsequent sequence should be invisible to each token position\n",
    "    subsequent_mask = torch.tril(full_mask)\n",
    "\n",
    "    # add a batch dim (1, max_tgt_seq_len, max_tgt_seq_len)\n",
    "    subsequent_mask = subsequent_mask.unsqueeze(0)\n",
    "\n",
    "    return src_pad_mask, tgt_pad_mask & subsequent_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    source_batch = [sample[\"encoder_input\"] for sample in batch]\n",
    "    target_batch = [sample[\"decoder_input\"] for sample in batch]\n",
    "\n",
    "    source_batch = pad_sequence(source_batch, padding_value=PAD_IDX, batch_first=True)\n",
    "    target_batch = pad_sequence(target_batch, padding_value=PAD_IDX, batch_first=True)\n",
    "\n",
    "    label_batch = target_batch[:, 1:]  # remove <sos> from target\n",
    "    target_batch = target_batch[:, :-1]  # remove <eos> from target\n",
    "\n",
    "    source_mask, target_mask = create_masks(source_batch, target_batch)\n",
    "\n",
    "    all_batches = [source_batch, target_batch, label_batch, source_mask, target_mask]\n",
    "\n",
    "    # move everything to the target device\n",
    "    return [x.to(device) for x in all_batches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_dataloader = DataLoader(\n",
    "    custom_dataset, batch_size=10, shuffle=True, collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source torch.Size([10, 20])\n",
      "target torch.Size([10, 18])\n",
      "label torch.Size([10, 18])\n",
      "source_mask torch.Size([10, 1, 20])\n",
      "target_mask torch.Size([10, 18, 18])\n",
      "\n",
      "\n",
      "source torch.Size([10, 24])\n",
      "target torch.Size([10, 22])\n",
      "label torch.Size([10, 22])\n",
      "source_mask torch.Size([10, 1, 24])\n",
      "target_mask torch.Size([10, 22, 22])\n",
      "\n",
      "\n",
      "source torch.Size([10, 19])\n",
      "target torch.Size([10, 19])\n",
      "label torch.Size([10, 19])\n",
      "source_mask torch.Size([10, 1, 19])\n",
      "target_mask torch.Size([10, 19, 19])\n",
      "\n",
      "\n",
      "source torch.Size([10, 23])\n",
      "target torch.Size([10, 18])\n",
      "label torch.Size([10, 18])\n",
      "source_mask torch.Size([10, 1, 23])\n",
      "target_mask torch.Size([10, 18, 18])\n",
      "\n",
      "\n",
      "source torch.Size([10, 20])\n",
      "target torch.Size([10, 22])\n",
      "label torch.Size([10, 22])\n",
      "source_mask torch.Size([10, 1, 20])\n",
      "target_mask torch.Size([10, 22, 22])\n",
      "\n",
      "\n",
      "source torch.Size([10, 26])\n",
      "target torch.Size([10, 18])\n",
      "label torch.Size([10, 18])\n",
      "source_mask torch.Size([10, 1, 26])\n",
      "target_mask torch.Size([10, 18, 18])\n",
      "\n",
      "\n",
      "source torch.Size([10, 26])\n",
      "target torch.Size([10, 22])\n",
      "label torch.Size([10, 22])\n",
      "source_mask torch.Size([10, 1, 26])\n",
      "target_mask torch.Size([10, 22, 22])\n",
      "\n",
      "\n",
      "source torch.Size([10, 22])\n",
      "target torch.Size([10, 19])\n",
      "label torch.Size([10, 19])\n",
      "source_mask torch.Size([10, 1, 22])\n",
      "target_mask torch.Size([10, 19, 19])\n",
      "\n",
      "\n",
      "source torch.Size([10, 20])\n",
      "target torch.Size([10, 17])\n",
      "label torch.Size([10, 17])\n",
      "source_mask torch.Size([10, 1, 20])\n",
      "target_mask torch.Size([10, 17, 17])\n",
      "\n",
      "\n",
      "source torch.Size([10, 19])\n",
      "target torch.Size([10, 19])\n",
      "label torch.Size([10, 19])\n",
      "source_mask torch.Size([10, 1, 19])\n",
      "target_mask torch.Size([10, 19, 19])\n",
      "\n",
      "\n",
      "source torch.Size([10, 26])\n",
      "target torch.Size([10, 23])\n",
      "label torch.Size([10, 23])\n",
      "source_mask torch.Size([10, 1, 26])\n",
      "target_mask torch.Size([10, 23, 23])\n",
      "\n",
      "\n",
      "source torch.Size([10, 30])\n",
      "target torch.Size([10, 29])\n",
      "label torch.Size([10, 29])\n",
      "source_mask torch.Size([10, 1, 30])\n",
      "target_mask torch.Size([10, 29, 29])\n",
      "\n",
      "\n",
      "source torch.Size([10, 22])\n",
      "target torch.Size([10, 20])\n",
      "label torch.Size([10, 20])\n",
      "source_mask torch.Size([10, 1, 22])\n",
      "target_mask torch.Size([10, 20, 20])\n",
      "\n",
      "\n",
      "source torch.Size([10, 19])\n",
      "target torch.Size([10, 17])\n",
      "label torch.Size([10, 17])\n",
      "source_mask torch.Size([10, 1, 19])\n",
      "target_mask torch.Size([10, 17, 17])\n",
      "\n",
      "\n",
      "source torch.Size([10, 18])\n",
      "target torch.Size([10, 16])\n",
      "label torch.Size([10, 16])\n",
      "source_mask torch.Size([10, 1, 18])\n",
      "target_mask torch.Size([10, 16, 16])\n",
      "\n",
      "\n",
      "source torch.Size([10, 28])\n",
      "target torch.Size([10, 29])\n",
      "label torch.Size([10, 29])\n",
      "source_mask torch.Size([10, 1, 28])\n",
      "target_mask torch.Size([10, 29, 29])\n",
      "\n",
      "\n",
      "source torch.Size([10, 22])\n",
      "target torch.Size([10, 22])\n",
      "label torch.Size([10, 22])\n",
      "source_mask torch.Size([10, 1, 22])\n",
      "target_mask torch.Size([10, 22, 22])\n",
      "\n",
      "\n",
      "source torch.Size([10, 20])\n",
      "target torch.Size([10, 27])\n",
      "label torch.Size([10, 27])\n",
      "source_mask torch.Size([10, 1, 20])\n",
      "target_mask torch.Size([10, 27, 27])\n",
      "\n",
      "\n",
      "source torch.Size([10, 19])\n",
      "target torch.Size([10, 19])\n",
      "label torch.Size([10, 19])\n",
      "source_mask torch.Size([10, 1, 19])\n",
      "target_mask torch.Size([10, 19, 19])\n",
      "\n",
      "\n",
      "source torch.Size([10, 20])\n",
      "target torch.Size([10, 17])\n",
      "label torch.Size([10, 17])\n",
      "source_mask torch.Size([10, 1, 20])\n",
      "target_mask torch.Size([10, 17, 17])\n",
      "\n",
      "\n",
      "source torch.Size([10, 22])\n",
      "target torch.Size([10, 20])\n",
      "label torch.Size([10, 20])\n",
      "source_mask torch.Size([10, 1, 22])\n",
      "target_mask torch.Size([10, 20, 20])\n",
      "\n",
      "\n",
      "source torch.Size([10, 30])\n",
      "target torch.Size([10, 29])\n",
      "label torch.Size([10, 29])\n",
      "source_mask torch.Size([10, 1, 30])\n",
      "target_mask torch.Size([10, 29, 29])\n",
      "\n",
      "\n",
      "source torch.Size([10, 21])\n",
      "target torch.Size([10, 20])\n",
      "label torch.Size([10, 20])\n",
      "source_mask torch.Size([10, 1, 21])\n",
      "target_mask torch.Size([10, 20, 20])\n",
      "\n",
      "\n",
      "source torch.Size([10, 22])\n",
      "target torch.Size([10, 20])\n",
      "label torch.Size([10, 20])\n",
      "source_mask torch.Size([10, 1, 22])\n",
      "target_mask torch.Size([10, 20, 20])\n",
      "\n",
      "\n",
      "source torch.Size([10, 24])\n",
      "target torch.Size([10, 20])\n",
      "label torch.Size([10, 20])\n",
      "source_mask torch.Size([10, 1, 24])\n",
      "target_mask torch.Size([10, 20, 20])\n",
      "\n",
      "\n",
      "source torch.Size([10, 22])\n",
      "target torch.Size([10, 20])\n",
      "label torch.Size([10, 20])\n",
      "source_mask torch.Size([10, 1, 22])\n",
      "target_mask torch.Size([10, 20, 20])\n",
      "\n",
      "\n",
      "source torch.Size([10, 19])\n",
      "target torch.Size([10, 19])\n",
      "label torch.Size([10, 19])\n",
      "source_mask torch.Size([10, 1, 19])\n",
      "target_mask torch.Size([10, 19, 19])\n",
      "\n",
      "\n",
      "source torch.Size([10, 17])\n",
      "target torch.Size([10, 16])\n",
      "label torch.Size([10, 16])\n",
      "source_mask torch.Size([10, 1, 17])\n",
      "target_mask torch.Size([10, 16, 16])\n",
      "\n",
      "\n",
      "source torch.Size([10, 23])\n",
      "target torch.Size([10, 22])\n",
      "label torch.Size([10, 22])\n",
      "source_mask torch.Size([10, 1, 23])\n",
      "target_mask torch.Size([10, 22, 22])\n",
      "\n",
      "\n",
      "source torch.Size([10, 22])\n",
      "target torch.Size([10, 25])\n",
      "label torch.Size([10, 25])\n",
      "source_mask torch.Size([10, 1, 22])\n",
      "target_mask torch.Size([10, 25, 25])\n",
      "\n",
      "\n",
      "source torch.Size([10, 21])\n",
      "target torch.Size([10, 20])\n",
      "label torch.Size([10, 20])\n",
      "source_mask torch.Size([10, 1, 21])\n",
      "target_mask torch.Size([10, 20, 20])\n",
      "\n",
      "\n",
      "source torch.Size([10, 17])\n",
      "target torch.Size([10, 17])\n",
      "label torch.Size([10, 17])\n",
      "source_mask torch.Size([10, 1, 17])\n",
      "target_mask torch.Size([10, 17, 17])\n",
      "\n",
      "\n",
      "source torch.Size([10, 19])\n",
      "target torch.Size([10, 17])\n",
      "label torch.Size([10, 17])\n",
      "source_mask torch.Size([10, 1, 19])\n",
      "target_mask torch.Size([10, 17, 17])\n",
      "\n",
      "\n",
      "source torch.Size([10, 23])\n",
      "target torch.Size([10, 24])\n",
      "label torch.Size([10, 24])\n",
      "source_mask torch.Size([10, 1, 23])\n",
      "target_mask torch.Size([10, 24, 24])\n",
      "\n",
      "\n",
      "source torch.Size([10, 22])\n",
      "target torch.Size([10, 20])\n",
      "label torch.Size([10, 20])\n",
      "source_mask torch.Size([10, 1, 22])\n",
      "target_mask torch.Size([10, 20, 20])\n",
      "\n",
      "\n",
      "source torch.Size([10, 18])\n",
      "target torch.Size([10, 20])\n",
      "label torch.Size([10, 20])\n",
      "source_mask torch.Size([10, 1, 18])\n",
      "target_mask torch.Size([10, 20, 20])\n",
      "\n",
      "\n",
      "source torch.Size([10, 26])\n",
      "target torch.Size([10, 25])\n",
      "label torch.Size([10, 25])\n",
      "source_mask torch.Size([10, 1, 26])\n",
      "target_mask torch.Size([10, 25, 25])\n",
      "\n",
      "\n",
      "source torch.Size([10, 22])\n",
      "target torch.Size([10, 19])\n",
      "label torch.Size([10, 19])\n",
      "source_mask torch.Size([10, 1, 22])\n",
      "target_mask torch.Size([10, 19, 19])\n",
      "\n",
      "\n",
      "source torch.Size([10, 30])\n",
      "target torch.Size([10, 23])\n",
      "label torch.Size([10, 23])\n",
      "source_mask torch.Size([10, 1, 30])\n",
      "target_mask torch.Size([10, 23, 23])\n",
      "\n",
      "\n",
      "source torch.Size([10, 19])\n",
      "target torch.Size([10, 18])\n",
      "label torch.Size([10, 18])\n",
      "source_mask torch.Size([10, 1, 19])\n",
      "target_mask torch.Size([10, 18, 18])\n",
      "\n",
      "\n",
      "source torch.Size([10, 21])\n",
      "target torch.Size([10, 20])\n",
      "label torch.Size([10, 20])\n",
      "source_mask torch.Size([10, 1, 21])\n",
      "target_mask torch.Size([10, 20, 20])\n",
      "\n",
      "\n",
      "source torch.Size([10, 37])\n",
      "target torch.Size([10, 35])\n",
      "label torch.Size([10, 35])\n",
      "source_mask torch.Size([10, 1, 37])\n",
      "target_mask torch.Size([10, 35, 35])\n",
      "\n",
      "\n",
      "source torch.Size([10, 25])\n",
      "target torch.Size([10, 28])\n",
      "label torch.Size([10, 28])\n",
      "source_mask torch.Size([10, 1, 25])\n",
      "target_mask torch.Size([10, 28, 28])\n",
      "\n",
      "\n",
      "source torch.Size([10, 26])\n",
      "target torch.Size([10, 21])\n",
      "label torch.Size([10, 21])\n",
      "source_mask torch.Size([10, 1, 26])\n",
      "target_mask torch.Size([10, 21, 21])\n",
      "\n",
      "\n",
      "source torch.Size([10, 21])\n",
      "target torch.Size([10, 22])\n",
      "label torch.Size([10, 22])\n",
      "source_mask torch.Size([10, 1, 21])\n",
      "target_mask torch.Size([10, 22, 22])\n",
      "\n",
      "\n",
      "source torch.Size([10, 20])\n",
      "target torch.Size([10, 27])\n",
      "label torch.Size([10, 27])\n",
      "source_mask torch.Size([10, 1, 20])\n",
      "target_mask torch.Size([10, 27, 27])\n",
      "\n",
      "\n",
      "source torch.Size([10, 20])\n",
      "target torch.Size([10, 18])\n",
      "label torch.Size([10, 18])\n",
      "source_mask torch.Size([10, 1, 20])\n",
      "target_mask torch.Size([10, 18, 18])\n",
      "\n",
      "\n",
      "source torch.Size([10, 24])\n",
      "target torch.Size([10, 21])\n",
      "label torch.Size([10, 21])\n",
      "source_mask torch.Size([10, 1, 24])\n",
      "target_mask torch.Size([10, 21, 21])\n",
      "\n",
      "\n",
      "source torch.Size([10, 24])\n",
      "target torch.Size([10, 23])\n",
      "label torch.Size([10, 23])\n",
      "source_mask torch.Size([10, 1, 24])\n",
      "target_mask torch.Size([10, 23, 23])\n",
      "\n",
      "\n",
      "source torch.Size([10, 18])\n",
      "target torch.Size([10, 16])\n",
      "label torch.Size([10, 16])\n",
      "source_mask torch.Size([10, 1, 18])\n",
      "target_mask torch.Size([10, 16, 16])\n",
      "\n",
      "\n",
      "source torch.Size([10, 29])\n",
      "target torch.Size([10, 29])\n",
      "label torch.Size([10, 29])\n",
      "source_mask torch.Size([10, 1, 29])\n",
      "target_mask torch.Size([10, 29, 29])\n",
      "\n",
      "\n",
      "source torch.Size([10, 24])\n",
      "target torch.Size([10, 24])\n",
      "label torch.Size([10, 24])\n",
      "source_mask torch.Size([10, 1, 24])\n",
      "target_mask torch.Size([10, 24, 24])\n",
      "\n",
      "\n",
      "source torch.Size([10, 26])\n",
      "target torch.Size([10, 21])\n",
      "label torch.Size([10, 21])\n",
      "source_mask torch.Size([10, 1, 26])\n",
      "target_mask torch.Size([10, 21, 21])\n",
      "\n",
      "\n",
      "source torch.Size([10, 17])\n",
      "target torch.Size([10, 18])\n",
      "label torch.Size([10, 18])\n",
      "source_mask torch.Size([10, 1, 17])\n",
      "target_mask torch.Size([10, 18, 18])\n",
      "\n",
      "\n",
      "source torch.Size([10, 24])\n",
      "target torch.Size([10, 22])\n",
      "label torch.Size([10, 22])\n",
      "source_mask torch.Size([10, 1, 24])\n",
      "target_mask torch.Size([10, 22, 22])\n",
      "\n",
      "\n",
      "source torch.Size([10, 20])\n",
      "target torch.Size([10, 18])\n",
      "label torch.Size([10, 18])\n",
      "source_mask torch.Size([10, 1, 20])\n",
      "target_mask torch.Size([10, 18, 18])\n",
      "\n",
      "\n",
      "source torch.Size([10, 27])\n",
      "target torch.Size([10, 25])\n",
      "label torch.Size([10, 25])\n",
      "source_mask torch.Size([10, 1, 27])\n",
      "target_mask torch.Size([10, 25, 25])\n",
      "\n",
      "\n",
      "source torch.Size([10, 21])\n",
      "target torch.Size([10, 20])\n",
      "label torch.Size([10, 20])\n",
      "source_mask torch.Size([10, 1, 21])\n",
      "target_mask torch.Size([10, 20, 20])\n",
      "\n",
      "\n",
      "source torch.Size([10, 24])\n",
      "target torch.Size([10, 24])\n",
      "label torch.Size([10, 24])\n",
      "source_mask torch.Size([10, 1, 24])\n",
      "target_mask torch.Size([10, 24, 24])\n",
      "\n",
      "\n",
      "source torch.Size([10, 19])\n",
      "target torch.Size([10, 18])\n",
      "label torch.Size([10, 18])\n",
      "source_mask torch.Size([10, 1, 19])\n",
      "target_mask torch.Size([10, 18, 18])\n",
      "\n",
      "\n",
      "source torch.Size([10, 17])\n",
      "target torch.Size([10, 14])\n",
      "label torch.Size([10, 14])\n",
      "source_mask torch.Size([10, 1, 17])\n",
      "target_mask torch.Size([10, 14, 14])\n",
      "\n",
      "\n",
      "source torch.Size([10, 26])\n",
      "target torch.Size([10, 25])\n",
      "label torch.Size([10, 25])\n",
      "source_mask torch.Size([10, 1, 26])\n",
      "target_mask torch.Size([10, 25, 25])\n",
      "\n",
      "\n",
      "source torch.Size([10, 20])\n",
      "target torch.Size([10, 19])\n",
      "label torch.Size([10, 19])\n",
      "source_mask torch.Size([10, 1, 20])\n",
      "target_mask torch.Size([10, 19, 19])\n",
      "\n",
      "\n",
      "source torch.Size([10, 25])\n",
      "target torch.Size([10, 19])\n",
      "label torch.Size([10, 19])\n",
      "source_mask torch.Size([10, 1, 25])\n",
      "target_mask torch.Size([10, 19, 19])\n",
      "\n",
      "\n",
      "source torch.Size([10, 23])\n",
      "target torch.Size([10, 19])\n",
      "label torch.Size([10, 19])\n",
      "source_mask torch.Size([10, 1, 23])\n",
      "target_mask torch.Size([10, 19, 19])\n",
      "\n",
      "\n",
      "source torch.Size([10, 22])\n",
      "target torch.Size([10, 24])\n",
      "label torch.Size([10, 24])\n",
      "source_mask torch.Size([10, 1, 22])\n",
      "target_mask torch.Size([10, 24, 24])\n",
      "\n",
      "\n",
      "source torch.Size([10, 25])\n",
      "target torch.Size([10, 25])\n",
      "label torch.Size([10, 25])\n",
      "source_mask torch.Size([10, 1, 25])\n",
      "target_mask torch.Size([10, 25, 25])\n",
      "\n",
      "\n",
      "source torch.Size([10, 35])\n",
      "target torch.Size([10, 31])\n",
      "label torch.Size([10, 31])\n",
      "source_mask torch.Size([10, 1, 35])\n",
      "target_mask torch.Size([10, 31, 31])\n",
      "\n",
      "\n",
      "source torch.Size([10, 22])\n",
      "target torch.Size([10, 19])\n",
      "label torch.Size([10, 19])\n",
      "source_mask torch.Size([10, 1, 22])\n",
      "target_mask torch.Size([10, 19, 19])\n",
      "\n",
      "\n",
      "source torch.Size([10, 21])\n",
      "target torch.Size([10, 18])\n",
      "label torch.Size([10, 18])\n",
      "source_mask torch.Size([10, 1, 21])\n",
      "target_mask torch.Size([10, 18, 18])\n",
      "\n",
      "\n",
      "source torch.Size([10, 19])\n",
      "target torch.Size([10, 15])\n",
      "label torch.Size([10, 15])\n",
      "source_mask torch.Size([10, 1, 19])\n",
      "target_mask torch.Size([10, 15, 15])\n",
      "\n",
      "\n",
      "source torch.Size([10, 26])\n",
      "target torch.Size([10, 23])\n",
      "label torch.Size([10, 23])\n",
      "source_mask torch.Size([10, 1, 26])\n",
      "target_mask torch.Size([10, 23, 23])\n",
      "\n",
      "\n",
      "source torch.Size([10, 22])\n",
      "target torch.Size([10, 16])\n",
      "label torch.Size([10, 16])\n",
      "source_mask torch.Size([10, 1, 22])\n",
      "target_mask torch.Size([10, 16, 16])\n",
      "\n",
      "\n",
      "source torch.Size([10, 19])\n",
      "target torch.Size([10, 24])\n",
      "label torch.Size([10, 24])\n",
      "source_mask torch.Size([10, 1, 19])\n",
      "target_mask torch.Size([10, 24, 24])\n",
      "\n",
      "\n",
      "source torch.Size([10, 22])\n",
      "target torch.Size([10, 21])\n",
      "label torch.Size([10, 21])\n",
      "source_mask torch.Size([10, 1, 22])\n",
      "target_mask torch.Size([10, 21, 21])\n",
      "\n",
      "\n",
      "source torch.Size([10, 21])\n",
      "target torch.Size([10, 20])\n",
      "label torch.Size([10, 20])\n",
      "source_mask torch.Size([10, 1, 21])\n",
      "target_mask torch.Size([10, 20, 20])\n",
      "\n",
      "\n",
      "source torch.Size([10, 21])\n",
      "target torch.Size([10, 21])\n",
      "label torch.Size([10, 21])\n",
      "source_mask torch.Size([10, 1, 21])\n",
      "target_mask torch.Size([10, 21, 21])\n",
      "\n",
      "\n",
      "source torch.Size([10, 23])\n",
      "target torch.Size([10, 26])\n",
      "label torch.Size([10, 26])\n",
      "source_mask torch.Size([10, 1, 23])\n",
      "target_mask torch.Size([10, 26, 26])\n",
      "\n",
      "\n",
      "source torch.Size([10, 31])\n",
      "target torch.Size([10, 30])\n",
      "label torch.Size([10, 30])\n",
      "source_mask torch.Size([10, 1, 31])\n",
      "target_mask torch.Size([10, 30, 30])\n",
      "\n",
      "\n",
      "source torch.Size([10, 22])\n",
      "target torch.Size([10, 20])\n",
      "label torch.Size([10, 20])\n",
      "source_mask torch.Size([10, 1, 22])\n",
      "target_mask torch.Size([10, 20, 20])\n",
      "\n",
      "\n",
      "source torch.Size([10, 26])\n",
      "target torch.Size([10, 24])\n",
      "label torch.Size([10, 24])\n",
      "source_mask torch.Size([10, 1, 26])\n",
      "target_mask torch.Size([10, 24, 24])\n",
      "\n",
      "\n",
      "source torch.Size([10, 24])\n",
      "target torch.Size([10, 20])\n",
      "label torch.Size([10, 20])\n",
      "source_mask torch.Size([10, 1, 24])\n",
      "target_mask torch.Size([10, 20, 20])\n",
      "\n",
      "\n",
      "source torch.Size([10, 24])\n",
      "target torch.Size([10, 23])\n",
      "label torch.Size([10, 23])\n",
      "source_mask torch.Size([10, 1, 24])\n",
      "target_mask torch.Size([10, 23, 23])\n",
      "\n",
      "\n",
      "source torch.Size([10, 19])\n",
      "target torch.Size([10, 15])\n",
      "label torch.Size([10, 15])\n",
      "source_mask torch.Size([10, 1, 19])\n",
      "target_mask torch.Size([10, 15, 15])\n",
      "\n",
      "\n",
      "source torch.Size([10, 17])\n",
      "target torch.Size([10, 15])\n",
      "label torch.Size([10, 15])\n",
      "source_mask torch.Size([10, 1, 17])\n",
      "target_mask torch.Size([10, 15, 15])\n",
      "\n",
      "\n",
      "source torch.Size([10, 18])\n",
      "target torch.Size([10, 16])\n",
      "label torch.Size([10, 16])\n",
      "source_mask torch.Size([10, 1, 18])\n",
      "target_mask torch.Size([10, 16, 16])\n",
      "\n",
      "\n",
      "source torch.Size([10, 28])\n",
      "target torch.Size([10, 36])\n",
      "label torch.Size([10, 36])\n",
      "source_mask torch.Size([10, 1, 28])\n",
      "target_mask torch.Size([10, 36, 36])\n",
      "\n",
      "\n",
      "source torch.Size([10, 27])\n",
      "target torch.Size([10, 24])\n",
      "label torch.Size([10, 24])\n",
      "source_mask torch.Size([10, 1, 27])\n",
      "target_mask torch.Size([10, 24, 24])\n",
      "\n",
      "\n",
      "source torch.Size([10, 23])\n",
      "target torch.Size([10, 20])\n",
      "label torch.Size([10, 20])\n",
      "source_mask torch.Size([10, 1, 23])\n",
      "target_mask torch.Size([10, 20, 20])\n",
      "\n",
      "\n",
      "source torch.Size([10, 20])\n",
      "target torch.Size([10, 16])\n",
      "label torch.Size([10, 16])\n",
      "source_mask torch.Size([10, 1, 20])\n",
      "target_mask torch.Size([10, 16, 16])\n",
      "\n",
      "\n",
      "source torch.Size([10, 19])\n",
      "target torch.Size([10, 16])\n",
      "label torch.Size([10, 16])\n",
      "source_mask torch.Size([10, 1, 19])\n",
      "target_mask torch.Size([10, 16, 16])\n",
      "\n",
      "\n",
      "source torch.Size([10, 24])\n",
      "target torch.Size([10, 24])\n",
      "label torch.Size([10, 24])\n",
      "source_mask torch.Size([10, 1, 24])\n",
      "target_mask torch.Size([10, 24, 24])\n",
      "\n",
      "\n",
      "source torch.Size([10, 22])\n",
      "target torch.Size([10, 17])\n",
      "label torch.Size([10, 17])\n",
      "source_mask torch.Size([10, 1, 22])\n",
      "target_mask torch.Size([10, 17, 17])\n",
      "\n",
      "\n",
      "source torch.Size([10, 22])\n",
      "target torch.Size([10, 19])\n",
      "label torch.Size([10, 19])\n",
      "source_mask torch.Size([10, 1, 22])\n",
      "target_mask torch.Size([10, 19, 19])\n",
      "\n",
      "\n",
      "source torch.Size([10, 19])\n",
      "target torch.Size([10, 16])\n",
      "label torch.Size([10, 16])\n",
      "source_mask torch.Size([10, 1, 19])\n",
      "target_mask torch.Size([10, 16, 16])\n",
      "\n",
      "\n",
      "source torch.Size([10, 17])\n",
      "target torch.Size([10, 17])\n",
      "label torch.Size([10, 17])\n",
      "source_mask torch.Size([10, 1, 17])\n",
      "target_mask torch.Size([10, 17, 17])\n",
      "\n",
      "\n",
      "source torch.Size([10, 27])\n",
      "target torch.Size([10, 27])\n",
      "label torch.Size([10, 27])\n",
      "source_mask torch.Size([10, 1, 27])\n",
      "target_mask torch.Size([10, 27, 27])\n",
      "\n",
      "\n",
      "source torch.Size([10, 30])\n",
      "target torch.Size([10, 34])\n",
      "label torch.Size([10, 34])\n",
      "source_mask torch.Size([10, 1, 30])\n",
      "target_mask torch.Size([10, 34, 34])\n",
      "\n",
      "\n",
      "source torch.Size([10, 20])\n",
      "target torch.Size([10, 19])\n",
      "label torch.Size([10, 19])\n",
      "source_mask torch.Size([10, 1, 20])\n",
      "target_mask torch.Size([10, 19, 19])\n",
      "\n",
      "\n",
      "source torch.Size([10, 19])\n",
      "target torch.Size([10, 19])\n",
      "label torch.Size([10, 19])\n",
      "source_mask torch.Size([10, 1, 19])\n",
      "target_mask torch.Size([10, 19, 19])\n",
      "\n",
      "\n",
      "source torch.Size([10, 19])\n",
      "target torch.Size([10, 17])\n",
      "label torch.Size([10, 17])\n",
      "source_mask torch.Size([10, 1, 19])\n",
      "target_mask torch.Size([10, 17, 17])\n"
     ]
    }
   ],
   "source": [
    "for idx, (source, target, label, source_mask, target_mask) in enumerate(\n",
    "    custom_dataloader\n",
    "):\n",
    "    print(\"source\", source.shape)\n",
    "    print(\"target\", target.shape)\n",
    "    print(\"label\", label.shape)\n",
    "    print(\"source_mask\", source_mask.shape)\n",
    "    print(\"target_mask\", target_mask.shape)\n",
    "\n",
    "    if idx == 100:\n",
    "        break\n",
    "\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimized code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import IterableDataset\n",
    "from torchtext import datasets\n",
    "from typing import Tuple\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from typing import List\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche. \n",
      " Two young, White males are outside near many bushes.\n"
     ]
    }
   ],
   "source": [
    "# Function to load the dataset based on given parameters\n",
    "def load_dataset(\n",
    "    name: str, split: str, language_pair: Tuple[str, str]\n",
    ") -> IterableDataset:\n",
    "    \"\"\"Loads a dataset from torchtext.datasets based on the provided name, split, and language pair\"\"\"\n",
    "    dataset_class = eval(f\"datasets.{name}\")\n",
    "    dataset = dataset_class(split=split, language_pair=language_pair)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# Load the Multi30k dataset (German-English)\n",
    "dataset = load_dataset(\"Multi30k\", \"train\", (\"de\", \"en\"))\n",
    "\n",
    "# Display the first sample from the dataset to verify the loading process\n",
    "for de_text, en_text in dataset:\n",
    "    print(de_text, \"\\n\", en_text)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Zwei', 'junge', 'weiße', 'Männer', 'sind', 'im', 'Freien', 'in', 'der', 'Nähe', 'vieler', 'Büsche', '.']\n",
      "['Two', 'young', ',', 'White', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.']\n",
      "['Two', 'young', ',', 'White', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.']\n",
      "['Zwei', 'junge', 'weiße', 'Männer', 'sind', 'im', 'Freien', 'in', 'der', 'Nähe', 'vieler', 'Büsche', '.']\n"
     ]
    }
   ],
   "source": [
    "# Initialize SpaCy tokenizers for German and English\n",
    "de_tokenizer = spacy.load(\"de_core_news_sm\")\n",
    "en_tokenizer = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Tokenize and display tokens for a sample text\n",
    "print([token.text for token in de_tokenizer(de_text)])\n",
    "print([token.text for token in en_tokenizer(en_text)])\n",
    "\n",
    "\n",
    "# Function to tokenize English text\n",
    "def engTokenize(text):\n",
    "    \"\"\"Tokenize an English text and return a list of tokens\"\"\"\n",
    "    return [token.text for token in en_tokenizer.tokenizer(text)]\n",
    "\n",
    "\n",
    "# Function to tokenize German text\n",
    "def deTokenize(text):\n",
    "    \"\"\"Tokenize a German text and return a list of tokens\"\"\"\n",
    "    return [token.text for token in de_tokenizer.tokenizer(text)]\n",
    "\n",
    "\n",
    "# Verify tokenization functions\n",
    "print(engTokenize(en_text))\n",
    "print(deTokenize(de_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/torch_env/lib/python3.10/site-packages/torch/utils/data/datapipes/iter/combining.py:337: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
      "  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29001 29001\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into source and target texts\n",
    "source_text = [en_text for de_text, en_text in dataset]\n",
    "target_text = [de_text for de_text, en_text in dataset]\n",
    "print(len(source_text), len(target_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10832 19207\n"
     ]
    }
   ],
   "source": [
    "# Function to return unique tokens in the texts\n",
    "def unique_tokens(tokenizer, texts):\n",
    "    \"\"\"Return a list of unique tokens in the texts\"\"\"\n",
    "    counter = Counter()\n",
    "    for doc in tokenizer.pipe(texts):\n",
    "        token_texts = []\n",
    "        for token in doc:\n",
    "            token_text = token.text.strip()\n",
    "            if len(token_text) > 0:  # Not a white space\n",
    "                token_texts.append(token_text)\n",
    "        counter.update(token_texts)\n",
    "\n",
    "    # Unique tokens\n",
    "    tokens = [token for token, count in counter.most_common()]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# Get unique tokens for English and German texts\n",
    "en_tokens = unique_tokens(en_tokenizer, source_text)\n",
    "de_tokens = unique_tokens(de_tokenizer, target_text)\n",
    "print(len(en_tokens), len(de_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Special token indices\n",
    "UNK_IDX = 0\n",
    "PAD_IDX = 1\n",
    "SOS_IDX = 2\n",
    "EOS_IDX = 3\n",
    "\n",
    "# Special tokens\n",
    "UNK = \"<unk>\"  # Unknown\n",
    "PAD = \"<pad>\"  # Padding\n",
    "SOS = \"<sos>\"  # Start of sentence\n",
    "EOS = \"<eos>\"  # End of sentence\n",
    "\n",
    "SPECIAL_TOKENS = [UNK, PAD, SOS, EOS]\n",
    "\n",
    "\n",
    "# Vocabulary class\n",
    "class Vocab:\n",
    "    def __init__(\n",
    "        self, tokenizer: spacy.language.Language, tokens: List[str] = []\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize the vocabulary with a tokenizer and an optional list of tokens\"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokens = SPECIAL_TOKENS + tokens  # Addition of special tokens\n",
    "        self.index_lookup = {self.tokens[i]: i for i in range(len(self.tokens))}\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Return the size of the vocabulary\"\"\"\n",
    "        return len(self.tokens)\n",
    "\n",
    "    def __call__(self, text: str) -> List[int]:\n",
    "        \"\"\"Convert a text to a list of token indices\"\"\"\n",
    "        text = text.strip()\n",
    "        return [self.to_index(token.text) for token in self.tokenizer(text)]\n",
    "\n",
    "    def to_index(self, token: str) -> int:\n",
    "        \"\"\"Convert a token to its corresponding index\"\"\"\n",
    "        return self.index_lookup[token] if token in self.index_lookup else UNK_IDX\n",
    "\n",
    "\n",
    "# Create vocabularies for English and German\n",
    "en_vocab = Vocab(en_tokenizer, en_tokens)\n",
    "de_vocab = Vocab(de_tokenizer, de_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Device configuration (use GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset class for translation data\n",
    "class SpacyData(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        source_text,\n",
    "        target_text,\n",
    "        source_vocab,\n",
    "        target_vocab,\n",
    "        pad_idx,\n",
    "        sos_idx,\n",
    "        eos_idx,\n",
    "    ):\n",
    "        \"\"\"Initialize the dataset with source and target texts, vocabularies, and special token indices\"\"\"\n",
    "        super().__init__()\n",
    "        self.source_vocab = source_vocab\n",
    "        self.target_vocab = target_vocab\n",
    "        self.pad_idx = pad_idx\n",
    "        self.sos_idx = sos_idx\n",
    "        self.eos_idx = eos_idx\n",
    "        self.source_text = source_text\n",
    "        self.target_text = target_text\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of samples in the dataset\"\"\"\n",
    "        return len(self.source_text)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Return a sample from the dataset at the given index\"\"\"\n",
    "        source_text_sample = self.source_text[idx]\n",
    "        target_text_sample = self.target_text[idx]\n",
    "\n",
    "        # Convert text to token indices\n",
    "        source_token_indices = self.source_vocab(source_text_sample)\n",
    "        target_token_indices = self.target_vocab(target_text_sample)\n",
    "\n",
    "        # Prepend SOS_IDX and append EOS_IDX for source and target sentences\n",
    "        source_token_indices = [self.sos_idx] + source_token_indices + [self.eos_idx]\n",
    "        target_token_indices = [self.sos_idx] + target_token_indices + [self.eos_idx]\n",
    "\n",
    "        return {\n",
    "            \"encoder_input\": torch.tensor(source_token_indices, dtype=torch.long),\n",
    "            \"decoder_input\": torch.tensor(target_token_indices, dtype=torch.long),\n",
    "            \"src_texts\": source_text_sample,\n",
    "            \"tgt_texts\": target_text_sample,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom dataset\n",
    "custom_dataset = SpacyData(\n",
    "    source_text=source_text,\n",
    "    target_text=target_text,\n",
    "    source_vocab=en_vocab,\n",
    "    target_vocab=de_vocab,\n",
    "    pad_idx=PAD_IDX,\n",
    "    sos_idx=SOS_IDX,\n",
    "    eos_idx=EOS_IDX,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create masks for source and target sequences\n",
    "def create_masks(src_batch: torch.Tensor, tgt_batch: torch.Tensor):\n",
    "    # Padding masks\n",
    "    src_pad_mask = (src_batch != PAD_IDX).unsqueeze(1)\n",
    "    tgt_pad_mask = (tgt_batch != PAD_IDX).unsqueeze(1)\n",
    "\n",
    "    # Subsequent mask for decoder inputs (causal mask)\n",
    "    max_tgt_sequence_length = tgt_batch.shape[1]\n",
    "    tgt_attention_square = (max_tgt_sequence_length, max_tgt_sequence_length)\n",
    "\n",
    "    full_mask = torch.full(tgt_attention_square, 1)\n",
    "    subsequent_mask = torch.tril(full_mask).unsqueeze(0)\n",
    "\n",
    "    return src_pad_mask, tgt_pad_mask & subsequent_mask\n",
    "\n",
    "\n",
    "# Collate function for batching\n",
    "def collate_fn(batch):\n",
    "    source_batch = [sample[\"encoder_input\"] for sample in batch]\n",
    "    target_batch = [sample[\"decoder_input\"] for sample in batch]\n",
    "    src_texts = [sample[\"src_texts\"] for sample in batch]\n",
    "    tgt_texts = [sample[\"tgt_texts\"] for sample in batch]\n",
    "\n",
    "    # Pad sequences to the same length within the batch\n",
    "    source_batch = pad_sequence(source_batch, padding_value=PAD_IDX, batch_first=True)\n",
    "    target_batch = pad_sequence(target_batch, padding_value=PAD_IDX, batch_first=True)\n",
    "\n",
    "    # Remove <sos> from target and create label batch\n",
    "    label_batch = target_batch[:, 1:]\n",
    "    target_batch = target_batch[:, :-1]\n",
    "\n",
    "    # Create masks for source and target sequences\n",
    "    source_mask, target_mask = create_masks(source_batch, target_batch)\n",
    "\n",
    "    # Move all batches to the target device (GPU or CPU)\n",
    "    all_batches = [source_batch, target_batch, label_batch, source_mask, target_mask]\n",
    "    return [x.to(device) for x in all_batches] + [src_texts, tgt_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source torch.Size([10, 24])\n",
      "target torch.Size([10, 33])\n",
      "label torch.Size([10, 33])\n",
      "source_mask torch.Size([10, 1, 24])\n",
      "target_mask torch.Size([10, 33, 33])\n",
      "src_texts ['The most amazing recreational activity outdoor camping.', 'Two girls standing on the side of the road.', 'Four girls in a swimming pool the one getting ready to jump in.', 'People staring up at the fair.', 'A man wearing a hoodie and jacket is sitting on a bench in a park.', 'Two young ladies, one blond talking on the cellphone, one brunette texting, are walking on the summer street.', 'A man and a woman carrying bags are walking down the sidewalk.', 'Several individuals getting together to play the bagpipes and eat some cuisine.', 'A boy in a black helmet jumps a bicycle with a small town visible in the background.', 'A woman and a man are dancing in public']\n",
      "tgt_texts ['Eine absolut faszinierend Freizeitaktivität: im Freien Campen.', 'Zwei Mädchen stehen am Straßenrand.', 'Vier Mädchen in einem Schwimmbad, von denen eines gleich ins Becken springen wird.', 'Personen starren das Fahrgeschäft an.', 'Ein Mann in Kapuzenpulli und Jacke sitzt auf einer Bank in einem Park.', 'Zwei junge Frauen gehen eine sommerliche Straße entlang – eine mit blonden Haaren, die mit dem Handy telefoniert, und eine mit braunen Haaren, die gerade eine Textnachricht schreibt.', 'Ein Mann und eine Frau, die den Bürgersteig entlang gehen und Taschen tragen.', 'Mehrere Personen treffen sich, um Dudelsack zu spielen und zu essen.', 'Ein Junge mit einem schwarzen Helm macht einen Sprung mit seinem Fahrrad, im Hintergrund ist eine Kleinstadt zu sehen.', 'Eine Frau und ein Mann tanzen in der Öffentlichkeit.']\n"
     ]
    }
   ],
   "source": [
    "# Create DataLoader\n",
    "custom_dataloader = DataLoader(\n",
    "    custom_dataset, batch_size=10, shuffle=True, collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "# Iterate over DataLoader and print batch shapes for debugging\n",
    "for idx, (\n",
    "    source,\n",
    "    target,\n",
    "    label,\n",
    "    source_mask,\n",
    "    target_mask,\n",
    "    src_texts,\n",
    "    tgt_texts,\n",
    ") in enumerate(custom_dataloader):\n",
    "    print(\"source\", source.shape)\n",
    "    print(\"target\", target.shape)\n",
    "    print(\"label\", label.shape)\n",
    "    print(\"source_mask\", source_mask.shape)\n",
    "    print(\"target_mask\", target_mask.shape)\n",
    "    print(\"src_texts\", src_texts)\n",
    "    print(\"tgt_texts\", tgt_texts)\n",
    "\n",
    "    if idx == 0:\n",
    "        break\n",
    "\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an analysis of the entire code provided, explaining what each part is doing and why it's necessary for training a transformer model in PyTorch:\n",
    "\n",
    "### Loading the Dataset\n",
    "```python\n",
    "from torch.utils.data import IterableDataset\n",
    "from torchtext import datasets\n",
    "from typing import Tuple\n",
    "\n",
    "def load_dataset(name: str, split: str, language_pair: Tuple[str, str]) -> IterableDataset:\n",
    "    \"\"\"Loads a dataset from torchtext.datasets based on the provided name, split, and language pair\"\"\"\n",
    "    dataset_class = eval(f\"datasets.{name}\")\n",
    "    dataset = dataset_class(split=split, language_pair=language_pair)\n",
    "    return dataset\n",
    "\n",
    "# Load the Multi30k dataset (German-English)\n",
    "dataset = load_dataset(\"Multi30k\", \"train\", (\"de\", \"en\"))\n",
    "\n",
    "# Display the first sample from the dataset to verify the loading process\n",
    "for de_text, en_text in dataset:\n",
    "    print(de_text, \"\\n\", en_text)\n",
    "    break\n",
    "```\n",
    "- **Purpose**: This block defines a function to load datasets from the `torchtext.datasets` module. The `load_dataset` function is flexible, allowing different dataset names, splits (e.g., train, test), and language pairs.\n",
    "- **Verification**: The first sample from the loaded dataset is printed to verify that the dataset is loaded correctly.\n",
    "\n",
    "### Tokenization\n",
    "```python\n",
    "# Initialize SpaCy tokenizers for German and English\n",
    "import spacy\n",
    "de_tokenizer = spacy.load(\"de_core_news_sm\")\n",
    "en_tokenizer = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Tokenize and display tokens for a sample text\n",
    "print([token.text for token in de_tokenizer(de_text)])\n",
    "print([token.text for token in en_tokenizer(en_text)])\n",
    "\n",
    "# Function to tokenize English text\n",
    "def engTokenize(text):\n",
    "    \"\"\"Tokenize an English text and return a list of tokens\"\"\"\n",
    "    return [token.text for token in en_tokenizer.tokenizer(text)]\n",
    "\n",
    "# Function to tokenize German text\n",
    "def deTokenize(text):\n",
    "    \"\"\"Tokenize a German text and return a list of tokens\"\"\"\n",
    "    return [token.text for token in de_tokenizer.tokenizer(text)]\n",
    "\n",
    "# Verify tokenization functions\n",
    "print(engTokenize(en_text))\n",
    "print(deTokenize(de_text))\n",
    "```\n",
    "- **Purpose**: This section initializes SpaCy tokenizers for German and English and defines functions to tokenize text in both languages.\n",
    "- **Verification**: Sample tokenizations are printed to ensure that the tokenizers work correctly.\n",
    "\n",
    "### Splitting and Analyzing the Dataset\n",
    "```python\n",
    "# Split the dataset into source and target texts\n",
    "source_text = [en_text for de_text, en_text in dataset]\n",
    "target_text = [de_text for de_text, en_text in dataset]\n",
    "print(len(source_text), len(target_text))\n",
    "\n",
    "# Function to return unique tokens in the texts\n",
    "from collections import Counter\n",
    "def unique_tokens(tokenizer, texts):\n",
    "    \"\"\"Return a list of unique tokens in the texts\"\"\"\n",
    "    counter = Counter()\n",
    "    for doc in tokenizer.pipe(texts):\n",
    "        token_texts = []\n",
    "        for token in doc:\n",
    "            token_text = token.text.strip()\n",
    "            if len(token_text) > 0:  # Not a white space\n",
    "                token_texts.append(token_text)\n",
    "        counter.update(token_texts)\n",
    "\n",
    "    # Unique tokens\n",
    "    tokens = [token for token, count in counter.most_common()]\n",
    "    return tokens\n",
    "\n",
    "# Get unique tokens for English and German texts\n",
    "en_tokens = unique_tokens(en_tokenizer, source_text)\n",
    "de_tokens = unique_tokens(de_tokenizer, target_text)\n",
    "print(len(en_tokens), len(de_tokens))\n",
    "```\n",
    "- **Purpose**: This block splits the dataset into source and target texts (English and German, respectively) and calculates unique tokens in both languages using the tokenizers.\n",
    "- **Verification**: The lengths of the source and target texts, as well as the number of unique tokens, are printed for validation.\n",
    "\n",
    "### Vocabulary Creation\n",
    "```python\n",
    "# Special token indices\n",
    "UNK_IDX = 0\n",
    "PAD_IDX = 1\n",
    "SOS_IDX = 2\n",
    "EOS_IDX = 3\n",
    "\n",
    "# Special tokens\n",
    "UNK = \"<unk>\"  # Unknown\n",
    "PAD = \"<pad>\"  # Padding\n",
    "SOS = \"<sos>\"  # Start of sentence\n",
    "EOS = \"<eos>\"  # End of sentence\n",
    "\n",
    "SPECIAL_TOKENS = [UNK, PAD, SOS, EOS]\n",
    "\n",
    "# Vocabulary class\n",
    "class Vocab:\n",
    "    def __init__(self, tokenizer: spacy.language.Language, tokens: List[str] = []) -> None:\n",
    "        \"\"\"Initialize the vocabulary with a tokenizer and an optional list of tokens\"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokens = SPECIAL_TOKENS + tokens  # Addition of special tokens\n",
    "        self.index_lookup = {self.tokens[i]: i for i in range(len(self.tokens))}\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Return the size of the vocabulary\"\"\"\n",
    "        return len(self.tokens)\n",
    "\n",
    "    def __call__(self, text: str) -> List[int]:\n",
    "        \"\"\"Convert a text to a list of token indices\"\"\"\n",
    "        text = text.strip()\n",
    "        return [self.to_index(token.text) for token in self.tokenizer(text)]\n",
    "\n",
    "    def to_index(self, token: str) -> int:\n",
    "        \"\"\"Convert a token to its corresponding index\"\"\"\n",
    "        return self.index_lookup[token] if token in self.index_lookup else UNK_IDX\n",
    "\n",
    "# Create vocabularies for English and German\n",
    "en_vocab = Vocab(en_tokenizer, en_tokens)\n",
    "de_vocab = Vocab(de_tokenizer, de_tokens)\n",
    "```\n",
    "- **Purpose**: This section defines special tokens and their indices, then creates a `Vocab` class to handle token-to-index conversions, including special tokens for unknown words, padding, start of sentence, and end of sentence.\n",
    "- **Initialization**: Two vocabularies are created for English and German using the `Vocab` class.\n",
    "\n",
    "### Custom Dataset\n",
    "```python\n",
    "# Device configuration (use GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Custom dataset class for translation data\n",
    "class CustomData(Dataset):\n",
    "    def __init__(self, source_text, target_text, source_vocab, target_vocab, pad_idx, sos_idx, eos_idx):\n",
    "        \"\"\"Initialize the dataset with source and target texts, vocabularies, and special token indices\"\"\"\n",
    "        super().__init__()\n",
    "        self.source_vocab = source_vocab\n",
    "        self.target_vocab = target_vocab\n",
    "        self.pad_idx = pad_idx\n",
    "        self.sos_idx = sos_idx\n",
    "        self.eos_idx = eos_idx\n",
    "        self.source_text = source_text\n",
    "        self.target_text = target_text\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of samples in the dataset\"\"\"\n",
    "        return len(self.source_text)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Return a sample from the dataset at the given index\"\"\"\n",
    "        source_text_sample = self.source_text[idx]\n",
    "        target_text_sample = self.target_text[idx]\n",
    "\n",
    "        # Convert text to token indices\n",
    "        source_token_indices = self.source_vocab(source_text_sample)\n",
    "        target_token_indices = self.target_vocab(target_text_sample)\n",
    "\n",
    "        # Prepend SOS_IDX and append EOS_IDX for source and target sentences\n",
    "        source_token_indices = [self.sos_idx] + source_token_indices + [self.eos_idx]\n",
    "        target_token_indices = [self.sos_idx] + target_token_indices + [self.eos_idx]\n",
    "\n",
    "        return {\n",
    "            \"encoder_input\": torch.tensor(source_token_indices, dtype=torch.long),\n",
    "            \"decoder_input\": torch.tensor(target_token_indices, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "# Create custom dataset\n",
    "custom_dataset = CustomData(\n",
    "    source_text=source_text,\n",
    "    target_text=target_text,\n",
    "    source_vocab=en_vocab,\n",
    "    target_vocab=de_vocab,\n",
    "    pad_idx=PAD_IDX,\n",
    "    sos_idx=SOS_IDX,\n",
    "    eos_idx=EOS_IDX,\n",
    ")\n",
    "```\n",
    "- **Purpose**: This block defines a `CustomData` class to handle the translation data, converting texts to token indices and adding special tokens (SOS and EOS). It also initializes the custom dataset.\n",
    "- **Device Configuration**: The device (GPU or CPU) is configured based on availability.\n",
    "\n",
    "### Mask Creation and Collate Function\n",
    "```python\n",
    "# Function to create masks for source and target sequences\n",
    "def create_masks(src_batch: torch.Tensor, tgt_batch: torch.Tensor):\n",
    "    # Padding masks\n",
    "    src_pad_mask = (src_batch != PAD_IDX).unsqueeze(1)\n",
    "    tgt_pad_mask = (tgt_batch != PAD_IDX).unsqueeze(1)\n",
    "\n",
    "    # Subsequent mask for decoder inputs (causal mask)\n",
    "    max_tgt_sequence_length = tgt_batch.shape[1]\n",
    "    tgt_attention_square = (max_tgt_sequence_length, max_tgt_sequence_length)\n",
    "\n",
    "    full_mask = torch.full(tgt_attention_square, 1)\n",
    "    subsequent_mask = torch.tril(full_mask).unsqueeze(0)\n",
    "\n",
    "    return src_pad_mask, tgt_pad_mask & subsequent_mask\n",
    "\n",
    "# Collate function for batching\n",
    "def collate_fn(batch):\n",
    "    source_batch = [sample[\"encoder_input\"] for sample in batch]\n",
    "    target_batch = [sample[\"decoder_input\"] for sample in batch]\n",
    "\n",
    "    # Pad sequences to the same length within the batch\n",
    "    source_batch = pad_sequence(source_batch, padding_value=PAD_IDX, batch_first=True)\n",
    "    target_batch = pad_sequence(target_batch, padding_value=PAD_IDX, batch_first=True)\n",
    "\n",
    "    # Remove <sos> from target and create label batch\n",
    "    label_batch = target_batch[:, 1:]\n",
    "    target_batch = target_batch[:, :-1]\n",
    "\n",
    "    # Create masks for source and target sequences\n",
    "    source_mask, target_mask = create_masks(source_batch, target_batch)\n",
    "\n",
    "    # Move all batches to the target device (GPU or CPU)\n",
    "    all_batches = [source_batch, target_batch, label_batch, source_mask, target_mask]\n",
    "    return [x.to(device) for x in all_batches\n",
    "\n",
    "]\n",
    "```\n",
    "- **Purpose**: \n",
    "  - `create_masks` function creates padding masks for the source and target sequences and a subsequent mask for the target sequence, crucial for transformer models to handle variable-length sequences and maintain causality in the decoder.\n",
    "  - `collate_fn` function pads sequences within a batch to the same length, adjusts target sequences, creates masks, and moves all data to the target device.\n",
    "\n",
    "### DataLoader and Debugging\n",
    "```python\n",
    "# Create DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "custom_dataloader = DataLoader(custom_dataset, batch_size=10, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Iterate over DataLoader and print batch shapes for debugging\n",
    "for idx, (source, target, label, source_mask, target_mask) in enumerate(custom_dataloader):\n",
    "    print(\"source\", source.shape)\n",
    "    print(\"target\", target.shape)\n",
    "    print(\"label\", label.shape)\n",
    "    print(\"source_mask\", source_mask.shape)\n",
    "    print(\"target_mask\", target_mask.shape)\n",
    "\n",
    "    if idx == 100:\n",
    "        break\n",
    "\n",
    "    print(\"\\n\")\n",
    "\n",
    "# TODO: Test the above dataloader with a transformer model\n",
    "```\n",
    "- **Purpose**: \n",
    "  - A `DataLoader` is created for batching and shuffling the custom dataset.\n",
    "  - The loop iterates over batches from the `DataLoader` and prints their shapes for debugging purposes, ensuring the data is correctly batched and masked.\n",
    "\n",
    "### Summary\n",
    "The code systematically loads and processes a translation dataset, tokenizes the text, creates vocabularies, and defines a custom dataset class. It prepares data batches with padding and masks, suitable for feeding into a transformer model. The debugging step verifies the data shapes, ensuring everything is set up correctly before testing with a transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:Test the above dataloader with a transformer model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converntional way - using Hugging face tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from S16_code.config import get_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32332\n"
     ]
    }
   ],
   "source": [
    "ds_raw = load_dataset(\n",
    "    get_config()[\"datasource\"],\n",
    "    f\"{get_config()['src_lang']}-{get_config()['tgt_lang']}\",\n",
    "    split=\"train\",\n",
    ")\n",
    "print(len(ds_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_sentences(ds, lang):\n",
    "    \"\"\"Iterate over all sentences in the dataset and yield them.\"\"\"\n",
    "    for pair in ds:\n",
    "        yield pair[\"translation\"][lang]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-lang tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, normalizers\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.normalizers import Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tokenizer(ds, lang):\n",
    "    \"\"\"Function to build a tokenizer for the given language and dataset\"\"\"\n",
    "\n",
    "    print(f\"Building tokenizer for {lang}\")\n",
    "    tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "    tokenizer.normalizer = normalizers.Sequence([Lowercase()])\n",
    "    trainer = WordLevelTrainer(\n",
    "        special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], min_frequency=2\n",
    "    )\n",
    "    tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer=trainer)\n",
    "    os.makedirs(\"./tokenizer\", exist_ok=True)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tokenizer for en\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tokenizer for it\n"
     ]
    }
   ],
   "source": [
    "tokenizer_src = build_tokenizer(ds_raw, get_config()[\"src_lang\"])\n",
    "tokenizer_tgt = build_tokenizer(ds_raw, get_config()[\"tgt_lang\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14554"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_src.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21401"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_tgt.get_vocab_size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_mask(size):\n",
    "    # Creating a square matrix of dimensions 'size x size' filled with ones\n",
    "    mask = torch.triu(torch.ones(1, size, size), diagonal=1).type(torch.int)\n",
    "    return mask == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BilingualDataset(Dataset):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        ds,\n",
    "        tokenizer_src,\n",
    "        tokenizer_tgt,\n",
    "        src_lang,\n",
    "        tgt_lang,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ds = ds\n",
    "        self.tokenizer_src = tokenizer_src\n",
    "        self.tokenizer_tgt = tokenizer_tgt\n",
    "        self.src_lang = src_lang\n",
    "        self.tgt_lang = tgt_lang\n",
    "\n",
    "        self.sos_token = torch.tensor(\n",
    "            [tokenizer_tgt.token_to_id(\"[SOS]\")], dtype=torch.int64\n",
    "        )\n",
    "        self.eos_token = torch.tensor(\n",
    "            [tokenizer_tgt.token_to_id(\"[EOS]\")], dtype=torch.int64\n",
    "        )\n",
    "        self.pad_token = torch.tensor(\n",
    "            [tokenizer_tgt.token_to_id(\"[PAD]\")], dtype=torch.int64\n",
    "        )\n",
    "\n",
    "        # print(self.sos_token, self.eos_token, self.pad_token)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_target_pair = self.ds[idx]\n",
    "        src_text = src_target_pair[\"translation\"][self.src_lang]\n",
    "        tgt_text = src_target_pair[\"translation\"][self.tgt_lang]\n",
    "\n",
    "        # Transform the text into tokens\n",
    "        enc_input_tokens = self.tokenizer_src.encode(src_text).ids\n",
    "        dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n",
    "\n",
    "        # Add <s> and </s> token\n",
    "        encoder_input = torch.cat(\n",
    "            [\n",
    "                self.sos_token,\n",
    "                torch.tensor(enc_input_tokens, dtype=torch.int64),\n",
    "                self.eos_token,\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        # Add only <s> token\n",
    "        decoder_input = torch.cat(\n",
    "            [\n",
    "                self.sos_token,\n",
    "                torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        # Add only </s> token as it offset by 1 from the decoder_input\n",
    "        label = torch.cat(\n",
    "            [\n",
    "                torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
    "                self.eos_token,\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"encoder_input\": encoder_input,  # (seq_len)\n",
    "            \"decoder_input\": decoder_input,  # (seq_len)\n",
    "            \"label\": label,  # (seq_len)\n",
    "            \"src_text\": src_text,\n",
    "            \"tgt_text\": tgt_text,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    encoder_input = [sample[\"encoder_input\"] for sample in batch]\n",
    "    decoder_input = [sample[\"decoder_input\"] for sample in batch]\n",
    "    label = [sample[\"label\"] for sample in batch]\n",
    "\n",
    "    encoder_input = torch.nn.utils.rnn.pad_sequence(\n",
    "        encoder_input,\n",
    "        batch_first=True,\n",
    "        padding_value=tokenizer_tgt.token_to_id(\"[PAD]\"),\n",
    "    )\n",
    "    decoder_input = torch.nn.utils.rnn.pad_sequence(\n",
    "        decoder_input,\n",
    "        batch_first=True,\n",
    "        padding_value=tokenizer_tgt.token_to_id(\"[PAD]\"),\n",
    "    )\n",
    "    label = torch.nn.utils.rnn.pad_sequence(\n",
    "        label, batch_first=True, padding_value=tokenizer_tgt.token_to_id(\"[PAD]\")\n",
    "    )\n",
    "\n",
    "    # print(f\"encoder_input: {encoder_input.shape}\")\n",
    "    # print(f\"decoder_input: {decoder_input.shape}\")\n",
    "    # print(f\"label: {label.shape}\")\n",
    "\n",
    "    encoder_mask = (\n",
    "        (encoder_input != tokenizer_tgt.token_to_id(\"[PAD]\"))\n",
    "        .unsqueeze(1)\n",
    "        .type(torch.int)\n",
    "    )\n",
    "    # print(f\"encoder_mask: {encoder_mask.shape}\")\n",
    "\n",
    "    decoder_mask = (decoder_input != tokenizer_tgt.token_to_id(\"[PAD]\")).unsqueeze(\n",
    "        1\n",
    "    ).type(torch.int) & causal_mask(decoder_input.size(1))\n",
    "    # print(f\"decoder_mask: {decoder_mask.shape}\")\n",
    "\n",
    "    return encoder_input, decoder_input, label, encoder_mask, decoder_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_dataset = BilingualDataset(\n",
    "    ds_raw,\n",
    "    tokenizer_src,\n",
    "    tokenizer_tgt,\n",
    "    get_config()[\"src_lang\"],\n",
    "    get_config()[\"tgt_lang\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_dataloader = DataLoader(\n",
    "    custom_dataset, batch_size=10, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_input torch.Size([10, 97])\n",
      "decoder_input torch.Size([10, 77])\n",
      "label torch.Size([10, 77])\n",
      "src_mask torch.Size([10, 1, 97])\n",
      "tgt_mask torch.Size([10, 77, 77])\n",
      "\n",
      "\n",
      "encoder_input torch.Size([10, 85])\n",
      "decoder_input torch.Size([10, 85])\n",
      "label torch.Size([10, 85])\n",
      "src_mask torch.Size([10, 1, 85])\n",
      "tgt_mask torch.Size([10, 85, 85])\n",
      "\n",
      "\n",
      "encoder_input torch.Size([10, 127])\n",
      "decoder_input torch.Size([10, 124])\n",
      "label torch.Size([10, 124])\n",
      "src_mask torch.Size([10, 1, 127])\n",
      "tgt_mask torch.Size([10, 124, 124])\n",
      "\n",
      "\n",
      "encoder_input torch.Size([10, 63])\n",
      "decoder_input torch.Size([10, 55])\n",
      "label torch.Size([10, 55])\n",
      "src_mask torch.Size([10, 1, 63])\n",
      "tgt_mask torch.Size([10, 55, 55])\n",
      "\n",
      "\n",
      "encoder_input torch.Size([10, 90])\n",
      "decoder_input torch.Size([10, 92])\n",
      "label torch.Size([10, 92])\n",
      "src_mask torch.Size([10, 1, 90])\n",
      "tgt_mask torch.Size([10, 92, 92])\n",
      "\n",
      "\n",
      "encoder_input torch.Size([10, 72])\n",
      "decoder_input torch.Size([10, 65])\n",
      "label torch.Size([10, 65])\n",
      "src_mask torch.Size([10, 1, 72])\n",
      "tgt_mask torch.Size([10, 65, 65])\n",
      "\n",
      "\n",
      "encoder_input torch.Size([10, 43])\n",
      "decoder_input torch.Size([10, 36])\n",
      "label torch.Size([10, 36])\n",
      "src_mask torch.Size([10, 1, 43])\n",
      "tgt_mask torch.Size([10, 36, 36])\n",
      "\n",
      "\n",
      "encoder_input torch.Size([10, 64])\n",
      "decoder_input torch.Size([10, 50])\n",
      "label torch.Size([10, 50])\n",
      "src_mask torch.Size([10, 1, 64])\n",
      "tgt_mask torch.Size([10, 50, 50])\n",
      "\n",
      "\n",
      "encoder_input torch.Size([10, 46])\n",
      "decoder_input torch.Size([10, 35])\n",
      "label torch.Size([10, 35])\n",
      "src_mask torch.Size([10, 1, 46])\n",
      "tgt_mask torch.Size([10, 35, 35])\n",
      "\n",
      "\n",
      "encoder_input torch.Size([10, 55])\n",
      "decoder_input torch.Size([10, 58])\n",
      "label torch.Size([10, 58])\n",
      "src_mask torch.Size([10, 1, 55])\n",
      "tgt_mask torch.Size([10, 58, 58])\n",
      "\n",
      "\n",
      "encoder_input torch.Size([10, 103])\n",
      "decoder_input torch.Size([10, 67])\n",
      "label torch.Size([10, 67])\n",
      "src_mask torch.Size([10, 1, 103])\n",
      "tgt_mask torch.Size([10, 67, 67])\n"
     ]
    }
   ],
   "source": [
    "for idx, (encoder_input, decoder_input, label, src_mask, tgt_mask) in enumerate(\n",
    "    custom_dataloader\n",
    "):\n",
    "    print(\"encoder_input\", encoder_input.shape)\n",
    "    print(\"decoder_input\", decoder_input.shape)\n",
    "    print(\"label\", label.shape)\n",
    "    print(\"src_mask\", src_mask.shape)\n",
    "    print(\"tgt_mask\", tgt_mask.shape)\n",
    "\n",
    "    if idx == 10:\n",
    "        break\n",
    "\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Super Optimized Code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from datasets import load_dataset\n",
    "from S16_code.config import get_config\n",
    "import os\n",
    "from tokenizers import Tokenizer, normalizers\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.normalizers import Lowercase\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 32332\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset based on the configuration settings\n",
    "# The dataset is loaded using the Hugging Face datasets library, which simplifies accessing and managing datasets\n",
    "ds_raw = load_dataset(\n",
    "    get_config()[\"datasource\"],\n",
    "    f\"{get_config()['src_lang']}-{get_config()['tgt_lang']}\",\n",
    "    split=\"train\",\n",
    ")\n",
    "print(f\"Dataset size: {len(ds_raw)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_sentences(ds, lang):\n",
    "    \"\"\"Iterate over all sentences in the dataset and yield them.\"\"\"\n",
    "    for pair in ds:\n",
    "        yield pair[\"translation\"][lang]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_path = get_config()[\"tokenizer_path\"]\n",
    "\n",
    "\n",
    "def build_tokenizer(ds, lang):\n",
    "    \"\"\"Function to build a tokenizer for the given language and dataset\"\"\"\n",
    "    print(f\"Building tokenizer for {lang}\")\n",
    "\n",
    "    if os.path.exists(os.path.join(tokenizer_path, f\"tokenizer_{lang}.json\")):\n",
    "        print(f\"Tokenizer for {lang} already exists\")\n",
    "        tokenizer = Tokenizer.from_file(\n",
    "            os.path.join(tokenizer_path, f\"tokenizer_{lang}.json\")\n",
    "        )\n",
    "        return tokenizer\n",
    "\n",
    "    # Initialize a WordLevel tokenizer with an unknown token\n",
    "    tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "\n",
    "    # Use whitespace for tokenization\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "    # Convert all text to lowercase to ensure consistency\n",
    "    tokenizer.normalizer = normalizers.Sequence([Lowercase()])\n",
    "\n",
    "    # Define special tokens and set minimum frequency for words\n",
    "    trainer = WordLevelTrainer(\n",
    "        special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], min_frequency=2\n",
    "    )\n",
    "\n",
    "    # Train tokenizer on the provided dataset\n",
    "    tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer=trainer)\n",
    "\n",
    "    os.makedirs(\"./tokenizer\", exist_ok=True)\n",
    "    tokenizer.save(os.path.join(tokenizer_path, f\"tokenizer_{lang}.json\"))\n",
    "\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tokenizer for en\n",
      "Tokenizer for en already exists\n",
      "Building tokenizer for it\n",
      "Tokenizer for it already exists\n",
      "Source vocabulary size: 15698\n",
      "Target vocabulary size: 22463\n"
     ]
    }
   ],
   "source": [
    "# Build tokenizers for source and target languages\n",
    "tokenizer_src = build_tokenizer(ds_raw, get_config()[\"src_lang\"])\n",
    "tokenizer_tgt = build_tokenizer(ds_raw, get_config()[\"tgt_lang\"])\n",
    "\n",
    "# Print vocabulary sizes\n",
    "print(f\"Source vocabulary size: {tokenizer_src.get_vocab_size()}\")\n",
    "print(f\"Target vocabulary size: {tokenizer_tgt.get_vocab_size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_mask(size):\n",
    "    \"\"\"Create a causal mask to ensure each position can attend to previous positions\"\"\"\n",
    "    # The causal mask ensures that each token can only attend to previous tokens (for autoregressive decoding)\n",
    "    mask = torch.triu(torch.ones(size, size), diagonal=1).type(torch.bool)\n",
    "    return ~mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BilingualDataset(Dataset):\n",
    "    \"\"\"Custom Dataset for bilingual translation data\"\"\"\n",
    "\n",
    "    def __init__(self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialize dataset and tokenizers\n",
    "        self.ds = ds\n",
    "        self.tokenizer_src = tokenizer_src\n",
    "        self.tokenizer_tgt = tokenizer_tgt\n",
    "        self.src_lang = src_lang\n",
    "        self.tgt_lang = tgt_lang\n",
    "\n",
    "        # Token IDs for special tokens\n",
    "        self.sos_token = torch.tensor(\n",
    "            [tokenizer_tgt.token_to_id(\"[SOS]\")], dtype=torch.int64\n",
    "        )\n",
    "        self.eos_token = torch.tensor(\n",
    "            [tokenizer_tgt.token_to_id(\"[EOS]\")], dtype=torch.int64\n",
    "        )\n",
    "        self.pad_token = torch.tensor(\n",
    "            [tokenizer_tgt.token_to_id(\"[PAD]\")], dtype=torch.int64\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of samples in the dataset\"\"\"\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Return a sample from the dataset at the given index\"\"\"\n",
    "        src_target_pair = self.ds[idx]\n",
    "        src_text = src_target_pair[\"translation\"][self.src_lang]\n",
    "        tgt_text = src_target_pair[\"translation\"][self.tgt_lang]\n",
    "\n",
    "        # Tokenize source and target texts\n",
    "        enc_input_tokens = self.tokenizer_src.encode(src_text).ids\n",
    "        dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n",
    "\n",
    "        # Add <SOS> and <EOS> tokens to the encoder input\n",
    "        encoder_input = torch.cat(\n",
    "            [\n",
    "                self.sos_token,\n",
    "                torch.tensor(enc_input_tokens, dtype=torch.int64),\n",
    "                self.eos_token,\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        # Add <SOS> token to the decoder input\n",
    "        decoder_input = torch.cat(\n",
    "            [self.sos_token, torch.tensor(dec_input_tokens, dtype=torch.int64)], dim=0\n",
    "        )\n",
    "\n",
    "        # Add <EOS> token to the label\n",
    "        label = torch.cat(\n",
    "            [torch.tensor(dec_input_tokens, dtype=torch.int64), self.eos_token], dim=0\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"encoder_input\": encoder_input,  # (seq_len)\n",
    "            \"decoder_input\": decoder_input,  # (seq_len)\n",
    "            \"label\": label,  # (seq_len)\n",
    "            \"src_text\": src_text,\n",
    "            \"tgt_text\": tgt_text,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"Collate function to pad sequences and create masks for batching\"\"\"\n",
    "    encoder_input = [sample[\"encoder_input\"] for sample in batch]\n",
    "    decoder_input = [sample[\"decoder_input\"] for sample in batch]\n",
    "    label = [sample[\"label\"] for sample in batch]\n",
    "    src_texts = [sample[\"src_text\"] for sample in batch]\n",
    "    tgt_texts = [sample[\"tgt_text\"] for sample in batch]\n",
    "\n",
    "    # Pad sequences for batching\n",
    "    encoder_input = torch.nn.utils.rnn.pad_sequence(\n",
    "        encoder_input,\n",
    "        batch_first=True,\n",
    "        padding_value=tokenizer_tgt.token_to_id(\"[PAD]\"),\n",
    "    )\n",
    "    decoder_input = torch.nn.utils.rnn.pad_sequence(\n",
    "        decoder_input,\n",
    "        batch_first=True,\n",
    "        padding_value=tokenizer_tgt.token_to_id(\"[PAD]\"),\n",
    "    )\n",
    "    label = torch.nn.utils.rnn.pad_sequence(\n",
    "        label, batch_first=True, padding_value=tokenizer_tgt.token_to_id(\"[PAD]\")\n",
    "    )\n",
    "\n",
    "    # Create masks\n",
    "    encoder_mask = (\n",
    "        (encoder_input != tokenizer_tgt.token_to_id(\"[PAD]\"))\n",
    "        .unsqueeze(1)\n",
    "        .type(torch.bool)\n",
    "    )\n",
    "    decoder_mask = (decoder_input != tokenizer_tgt.token_to_id(\"[PAD]\")).unsqueeze(\n",
    "        1\n",
    "    ).type(torch.bool) & causal_mask(decoder_input.size(1))\n",
    "\n",
    "    return {\n",
    "        \"encoder_input\": encoder_input,  # (batch_size, seq_len)\n",
    "        \"decoder_input\": decoder_input,  # (batch_size, seq_len)\n",
    "        \"label\": label,  # (batch_size, seq_len)\n",
    "        \"encoder_mask\": encoder_mask,  # (batch_size, 1, seq_len)\n",
    "        \"decoder_mask\": decoder_mask,  # (batch_size, seq_len, seq_len)\n",
    "        \"src_texts\": src_texts,  # List of source texts\n",
    "        \"tgt_texts\": tgt_texts,  # List of target texts\n",
    "    }\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom dataset and dataloader\n",
    "custom_dataset = BilingualDataset(\n",
    "    ds_raw,\n",
    "    tokenizer_src,\n",
    "    tokenizer_tgt,\n",
    "    get_config()[\"src_lang\"],\n",
    "    get_config()[\"tgt_lang\"],\n",
    ")\n",
    "custom_dataloader = DataLoader(custom_dataset, batch_size=10, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_input torch.Size([10, 97])\n",
      "decoder_input torch.Size([10, 77])\n",
      "label torch.Size([10, 77])\n",
      "encoder_mask torch.Size([10, 1, 97])\n",
      "decoder_mask torch.Size([10, 77, 77])\n",
      "src_texts ['Source: Project Gutenberg', 'Jane Eyre', 'Charlotte Bronte', 'CHAPTER I', 'There was no possibility of taking a walk that day.', 'We had been wandering, indeed, in the leafless shrubbery an hour in the morning; but since dinner (Mrs. Reed, when there was no company, dined early) the cold winter wind had brought with it clouds so sombre, and a rain so penetrating, that further out-door exercise was now out of the question.', 'I was glad of it: I never liked long walks, especially on chilly afternoons: dreadful to me was the coming home in the raw twilight, with nipped fingers and toes, and a heart saddened by the chidings of Bessie, the nurse, and humbled by the consciousness of my physical inferiority to Eliza, John, and Georgiana Reed.', 'The said Eliza, John, and Georgiana were now clustered round their mama in the drawing-room: she lay reclined on a sofa by the fireside, and with her darlings about her (for the time neither quarrelling nor crying) looked perfectly happy.', 'Me, she had dispensed from joining the group; saying, \"She regretted to be under the necessity of keeping me at a distance; but that until she heard from Bessie, and could discover by her own observation, that I was endeavouring in good earnest to acquire a more sociable and childlike disposition, a more attractive and sprightly manner--something lighter, franker, more natural, as it were--she really must exclude me from privileges intended only for contented, happy, little children.\"', '\"What does Bessie say I have done?\" I asked.']\n",
      "tgt_texts ['Source: www.liberliber.it/Audiobook available here', 'Jane Eyre', 'Charlotte Brontë', 'PARTE PRIMA', 'I. In quel giorno era impossibile passeggiare.', \"La mattina avevamo errato per un'ora nel boschetto spogliato di foglie, ma dopo pranzo (quando non vi erano invitati, la signora Reed desinava presto), il vento gelato d'inverno aveva portato seco nubi così scure e una pioggia così penetrante, che non si poteva pensare a nessuna escursione.\", 'Ne ero contenta. Non mi sono mai piaciute le lunghe passeggiate, sopra tutto col freddo, ed era cosa penosa per me di tornar di notte con le mani e i piedi gelati, col cuore amareggiato dalle sgridate di Bessie, la bambinaia, e con lo spirito abbattuto dalla coscienza della mia inferiorità fisica di fronte a Eliza, a John e a Georgiana Reed.', 'Eliza, John e Georgiana erano aggruppati in salotto attorno alla loro mamma; questa, sdraiata sul sofà accanto al fuoco e circondata dai suoi bambini, che in quel momento non questionavano fra loro né piangevano, pareva perfettamente felice.', 'Ella mi aveva proibito di unirmi al loro gruppo, dicendo che deplorava la necessità in cui trovavasi di tenermi così lontana, ma che fino al momento in cui Bessie non guarentirebbe che mi studiavo di acquistare un carattere più socievole e più infantile, maniere più cortesi e qualcosa di più radioso, di più aperto, di più sincero, non poteva concedermi gli stessi privilegi che ai bambini allegri e soddisfatti.', '— Che cosa vi ha detto Bessie di nuovo sul conto mio? — domandai.']\n",
      "\n",
      "\n",
      "encoder_input torch.Size([10, 85])\n",
      "decoder_input torch.Size([10, 85])\n",
      "label torch.Size([10, 85])\n",
      "encoder_mask torch.Size([10, 1, 85])\n",
      "decoder_mask torch.Size([10, 85, 85])\n",
      "src_texts ['\"Jane, I don\\'t like cavillers or questioners; besides, there is something truly forbidding in a child taking up her elders in that manner.', 'Be seated somewhere; and until you can speak pleasantly, remain silent.\"', 'A breakfast-room adjoined the drawing-room, I slipped in there.', 'It contained a bookcase: I soon possessed myself of a volume, taking care that it should be one stored with pictures.', 'I mounted into the window- seat: gathering up my feet, I sat cross-legged, like a Turk; and, having drawn the red moreen curtain nearly close, I was shrined in double retirement.', 'Folds of scarlet drapery shut in my view to the right hand; to the left were the clear panes of glass, protecting, but not separating me from the drear November day.', 'At intervals, while turning over the leaves of my book, I studied the aspect of that winter afternoon. Afar, it offered a pale blank of mist and cloud; near a scene of wet lawn and storm-beat shrub, with ceaseless rain sweeping away wildly before a long and lamentable blast.', \"I returned to my book--Bewick's History of British Birds: the letterpress thereof I cared little for, generally speaking; and yet there were certain introductory pages that, child as I was, I could not pass quite as a blank.\", 'They were those which treat of the haunts of sea-fowl; of \"the solitary rocks and promontories\" by them only inhabited; of the coast of Norway, studded with isles from its southern extremity, the Lindeness, or Naze, to the North Cape-- \"Where the Northern Ocean, in vast whirls, Boils round the naked, melancholy isles Of farthest Thule; and the Atlantic surge Pours in among the stormy Hebrides.\"', 'Nor could I pass unnoticed the suggestion of the bleak shores of Lapland, Siberia, Spitzbergen, Nova Zembla, Iceland, Greenland, with \"the vast sweep of the Arctic Zone, and those forlorn regions of dreary space,--that reservoir of frost and snow, where firm fields of ice, the accumulation of centuries of winters, glazed in Alpine heights above heights, surround the pole, and concentre the multiplied rigours of extreme cold.\"']\n",
      "tgt_texts ['— Jane, non mi piace di essere interrogata. Sta male, del resto, che una bimba tratti così i suoi superiori.', 'Sedetevi in qualche posto e state buona fino a quando non saprete parlare ragionevolmente.', 'Una piccola sala da pranzo metteva nel salotto, andai in quella pian piano.', \"Vi era una biblioteca e io m'impossessai di un libro, cercando che fosse ornato d'incisioni.\", 'Mi collocai allora nel vano di una finestra, sedendomi sui piedi come i turchi, e tirando la tenda di damasco rosso, mi trovai rinchiusa in un doppio ritiro.', 'Le larghe pieghe della cortina scarlatta mi nascondevano tutto ciò che era alla mia destra: alla mia sinistra una invetriata mi proteggeva, ma non mi separava da una triste giornata di novembre.', \"Di tanto in tanto, sfogliando il libro, gettavo un'occhiata al difuori e studiavo l'aspetto di quella serata d'inverno; in lontananza si scorgeva una pallida striscia di nebbia con nuvole, più vicino alberi bagnati, piante sradicate dal temporale e, infine, una pioggia incessante, che lunghe e lamentevoli ventate respingevano sibilando.\", \"Tornavo allora al mio libro; era La storia degli uccelli dell'Inghilterra, scritta da Berwich. In generale non mi occupavo del testo, nondimeno c'erano delle pagine d'introduzione che non potevo lasciar passare inosservate, malgrado la mia gioventù.\", 'Esse parlavano di quei rifugi degli uccelli marini, di quei promontori, di quelle rocce deserte abitate da essi soli, di quelle coste della Norvegia sparse d\\'isole dalla più meridionale punta al capo più nordico, là dove \"l\\'Oceano Polare mugge in vasti turbini attorno all\\'isola arida e malinconica di Tule, là ove il mare Atlantico si precipita in mezzo alle Ebridi tempestose.\"', \"Non potevo neppure saltare la descrizione di quei pallidi paesaggi della Siberia, dello Spitzberg, della Nuova-Zembla, dell'Islanda, della verde Finlandia! Ero assorta nel pensiero di quella solitudine della zona artica, di quelle immense regioni abbandonate, di quei serbatoi di ghiaccio, ove i campi di neve accumulati durante gli inverni di molti secoli, ammucchiano montagne su montagne per circondare il polo e vi concentrano tutti i rigori del freddo più intenso.\"]\n",
      "\n",
      "\n",
      "encoder_input torch.Size([10, 127])\n",
      "decoder_input torch.Size([10, 124])\n",
      "label torch.Size([10, 124])\n",
      "encoder_mask torch.Size([10, 1, 127])\n",
      "decoder_mask torch.Size([10, 124, 124])\n",
      "src_texts [\"Of these death-white realms I formed an idea of my own: shadowy, like all the half-comprehended notions that float dim through children's brains, but strangely impressive.\", 'The words in these introductory pages connected themselves with the succeeding vignettes, and gave significance to the rock standing up alone in a sea of billow and spray; to the broken boat stranded on a desolate coast; to the cold and ghastly moon glancing through bars of cloud at a wreck just sinking.', 'I cannot tell what sentiment haunted the quite solitary churchyard, with its inscribed headstone; its gate, its two trees, its low horizon, girdled by a broken wall, and its newly-risen crescent, attesting the hour of eventide.', 'The two ships becalmed on a torpid sea, I believed to be marine phantoms.', \"The fiend pinning down the thief's pack behind him, I passed over quickly: it was an object of terror. So was the black horned thing seated aloof on a rock, surveying a distant crowd surrounding a gallows.\", \"Each picture told a story; mysterious often to my undeveloped understanding and imperfect feelings, yet ever profoundly interesting: as interesting as the tales Bessie sometimes narrated on winter evenings, when she chanced to be in good humour; and when, having brought her ironing-table to the nursery hearth, she allowed us to sit about it, and while she got up Mrs. Reed's lace frills, and crimped her nightcap borders, fed our eager attention with passages of love and adventure taken from old fairy tales and other ballads; or (as at a later period I discovered) from the pages of Pamela, and Henry, Earl of Moreland.\", 'With Bewick on my knee, I was then happy: happy at least in my way.', 'I feared nothing but interruption, and that came too soon.', 'The breakfast- room door opened.', '\"Boh! Madam Mope!\" cried the voice of John Reed; then he paused: he found the room apparently empty.']\n",
      "tgt_texts [\"Mi ero formata un'idea tutta mia di quei regni pallidi come la morte, idea vaga, come sono tutte le cose capite per metà, che fluttuano nella testa dei bimbi; ma quella che mi figuravo produceva in me uno strano effetto.\", \"In quella introduzione il testo, accordandosi con le figure, dava un significato allo scoglio isolato in mezzo a un mare di onde e di spuma, alla nave gettata su una costa desolata, alla fredda e fantastica luna, che, spingendo i suoi raggi luminosi attraverso un cumulo di nubi, illuminava appunto un'altra scena di naufragio.\", \"Io non potrei dire quale sentimento animasse il tranquillo e solitario cimitero, con le sue lapidi, le sue cancellate, i due alberi e l'orizzonte limitato dal muro rotto e la luna crescente che indicava l'ora della sera.\", 'Le due navi, in quel mare immobili, mi parevano due fantasmi marini.', 'Sfogliai sollecitamente la figura che rappresenta il mortale nemico, inchiodando il fardello sulla schiena del ladro; era per me un soggetto di terrore, come quella creatura con le corna, seduta sullo scoglio, che spiava la lontana turba che circondava la forca.', 'Ogni incisione mi narrava una storia, spesso misteriosa per la mia intelligenza poco sviluppata e per il mio incompleto sentimento, ma sempre interessantissima; così interessante come i racconti che ci faceva Bessie nelle serate invernali quando era di buon umore e quando, dopo aver portato la tavola da stirare nella stanza dei bambini, ci permetteva di sedersi vicino a lei. Allora, pieghettando le sciarpe di trina della signora Reed e le cuffie da notte, ci riscaldava la fantasia con narrazioni di amore e di avventure, tolte dai vecchi racconti di fate e dalle antiche ballate, o, come mi accorsi più tardi, da Pamela e da Enrico, conte di Mareland.', 'Così, avendo Borwick sulle ginocchia, ero felice, felice a modo mio.', 'Temevo soltanto una interruzione, che non tardò.', 'La porta della stanza da pranzo fu vivamente aperta.', '— Oh! signora scontrosa, — gridò John Reed.']\n",
      "\n",
      "\n",
      "encoder_input torch.Size([10, 63])\n",
      "decoder_input torch.Size([10, 55])\n",
      "label torch.Size([10, 55])\n",
      "encoder_mask torch.Size([10, 1, 63])\n",
      "decoder_mask torch.Size([10, 55, 55])\n",
      "src_texts ['\"Where the dickens is she!\" he continued. \"Lizzy!', 'Georgy! (calling to his sisters) Joan is not here: tell mama she is run out into the rain--bad animal!\"', '\"It is well I drew the curtain,\" thought I; and I wished fervently he might not discover my hiding-place: nor would John Reed have found it out himself; he was not quick either of vision or conception; but Eliza just put her head in at the door, and said at once--', '\"She is in the window-seat, to be sure, Jack.\"', 'And I came out immediately, for I trembled at the idea of being dragged forth by the said Jack.', '\"What do you want?\" I asked, with awkward diffidence.', '\"Say, \\'What do you want, Master Reed?\\'\" was the answer.', '\"I want you to come here;\" and seating himself in an arm-chair, he intimated by a gesture that I was to approach and stand before him.', 'John Reed was a schoolboy of fourteen years old; four years older than I, for I was but ten: large and stout for his age, with a dingy and unwholesome skin; thick lineaments in a spacious visage, heavy limbs and large extremities.', 'He gorged himself habitually at table, which made him bilious, and gave him a dim and bleared eye and flabby cheeks.']\n",
      "tgt_texts [\"Poi tacque, perché gli parve che la stanza fosse deserta. — Per bacco, dov'è?\", 'Liszy, Giorgy, — continuò egli volgendosi alle sorelle, — dite alla mamma che la cattiva bestia è andata a correre in giardino con questa pioggia!', \"— Ho fatto bene a tirare la tenda, — pensavo fra me; e mi auguravo sinceramente che non scoprissero il mio nascondiglio. John non lo avrebbe mai trovato da sè stesso: non aveva lo sguardo pronto; ma Eliza, avendo sporto la testa dall'uscio, esclamò:\", '— Ella è certamente nel vano della finestra!', 'Uscii subito, perché mi sgomentavo al pensiero di esser condotta fuori dal mio nascondiglio da John.', '— Che cosa volete? — gli domandai con timidezza rispettosa.', '— Dite: Che cosa volete, signor Reed?', 'Mi rispose. — Voglio che veniate qui! — e collocandosi nella poltrona, mi fece cenno di accostarmi e di star ritta dinanzi a lui.', 'Era alto e forte per la sua età, ma aveva una carnagione scura e malsana. I lineamenti del volto grossolani, le membra pesanti e le estremità molto sviluppate.', 'Soleva mangiare avidamente, e ciò avevagli prodotta quella tinta biliosa, quello sguardo turbato e quelle guancie flosce.']\n",
      "\n",
      "\n",
      "encoder_input torch.Size([10, 90])\n",
      "decoder_input torch.Size([10, 92])\n",
      "label torch.Size([10, 92])\n",
      "encoder_mask torch.Size([10, 1, 90])\n",
      "decoder_mask torch.Size([10, 92, 92])\n",
      "src_texts ['He ought now to have been at school; but his mama had taken him home for a month or two, \"on account of his delicate health.\"', \"Mr. Miles, the master, affirmed that he would do very well if he had fewer cakes and sweetmeats sent him from home; but the mother's heart turned from an opinion so harsh, and inclined rather to the more refined idea that John's sallowness was owing to over-application and, perhaps, to pining after home.\", 'John had not much affection for his mother and sisters, and an antipathy to me.', 'He bullied and punished me; not two or three times in the week, nor once or twice in the day, but continually: every nerve I had feared him, and every morsel of flesh in my bones shrank when he came near.', 'There were moments when I was bewildered by the terror he inspired, because I had no appeal whatever against either his menaces or his inflictions; the servants did not like to offend their young master by taking my part against him, and Mrs. Reed was blind and deaf on the subject: she never saw him strike or heard him abuse me, though he did both now and then in her very presence, more frequently, however, behind her back.', 'Habitually obedient to John, I came up to his chair: he spent some three minutes in thrusting out his tongue at me as far as he could without damaging the roots: I knew he would soon strike, and while dreading the blow, I mused on the disgusting and ugly appearance of him who would presently deal it.', 'I wonder if he read that notion in my face; for, all at once, without speaking, he struck suddenly and strongly.', 'I tottered, and on regaining my equilibrium retired back a step or two from his chair.', '\"That is for your impudence in answering mama awhile since,\" said he, \"and for your sneaking way of getting behind curtains, and for the look you had in your eyes two minutes since, you rat!\"', \"Accustomed to John Reed's abuse, I never had an idea of replying to it; my care was how to endure the blow which would certainly follow the insult.\"]\n",
      "tgt_texts ['In quel tempo avrebbe dovuto trovarsi in collegio, ma sua madre avevalo tolto per un mese o due col pretesto della sua delicata salute.', \"Il signor Miles, direttore del collegio, assicurava che sarebbe stato benissimo se da casa gli avessero mandate meno dolci e meno ghiottonerie, ma il cuore della madre si era ribellato contro questa severità e aveva preferito di accoglier l'idea più gentile che il malessere di John dipendesse dal soverchio studio e dal dolore di esser separato dai suoi.\", 'John non voleva molto bene né alla madre né alle sorelle.', 'Io poi gli ero antipatica; mi maltrattava e mi puniva, non due o tre volte la settimana, non due o tre volte al giorno, ma sempre; ognuno dei miei nervi aveva paura di lui, ogni brano della mia carne e delle mie ossa fremeva allorché egli si accostava a me.', \"Vi erano momenti in cui divenivo selvaggia per il terrore che mi ispirava, perché non sapevo a chi ricorrere contro le sue minaccie e le sue punizioni. I servi non avrebbero voluto prendere le mie difese per non offendere il loro giovine padrone, e la signora Reed su quell'argomento era cieca e sorda, ella fingeva di non accorgersi quando mi picchiava o m'insultava, benché egli ciò facesse spesso in presenza di lei, ma più spesso quando non c'era.\", 'Essendo assuefatta ad ubbidire a John, mi accostai alla seggiola sua. Egli stette tre minuti a mostrarmi la lingua, allungandola quanto più poteva, sapevo che stava per picchiarmi e spiavo sulla sua brutta faccia il momento in cui la collera avrebbegli fatto allungare la mano.', \"Credo che s'accorgesse del mio pensiero, perché a un tratto si alzò senza dir parola, e mi colpì duramente.\", 'Barcollai e poi rimettendomi in equilibrio, mi allontanai di un passo o due dalla sua sedia.', \"— Questo è per l'impudenza con cui avete risposto alla mamma, — mi disse, — e per esservi nascosta dietro la tenda e per lo sguardo che avevate negli occhi poco fa, talpa!\", \"— Assuefatta com'ero agli insulti di John, non mi venne neppur l'idea di rispondergli; ponevo ogni cura invece nel sopportare coraggiosamente il colpo, che avrebbe tenuto dietro all'insulto.\"]\n",
      "\n",
      "\n",
      "encoder_input torch.Size([10, 72])\n",
      "decoder_input torch.Size([10, 65])\n",
      "label torch.Size([10, 65])\n",
      "encoder_mask torch.Size([10, 1, 72])\n",
      "decoder_mask torch.Size([10, 65, 65])\n",
      "src_texts ['\"What were you doing behind the curtain?\" he asked.', '\"I was reading.\"', '\"Show the book.\"', 'I returned to the window and fetched it thence.', '\"You have no business to take our books; you are a dependent, mama says; you have no money; your father left you none; you ought to beg, and not to live here with gentlemen\\'s children like us, and eat the same meals we do, and wear clothes at our mama\\'s expense.', \"Now, I'll teach you to rummage my bookshelves: for they _are_ mine; all the house belongs to me, or will do in a few years.\", 'Go and stand by the door, out of the way of the mirror and the windows.\"', 'I did so, not at first aware what was his intention; but when I saw him lift and poise the book and stand in act to hurl it, I instinctively started aside with a cry of alarm: not soon enough, however; the volume was flung, it hit me, and I fell, striking my head against the door and cutting it.', 'The cut bled, the pain was sharp: my terror had passed its climax; other feelings succeeded.', '\"Wicked and cruel boy!\" I said. \"You are like a murderer--you are like a slave-driver--you are like the Roman emperors!\"']\n",
      "tgt_texts ['— Che cosa facevate dietro la tenda? — mi domandò.', '— Leggevo.', '— Fatemi vedere il libro.', '— Mi diressi verso la finestra per prenderlo.', \"— Non c'è bisogno che prendiate i nostri libri; dipendete da noi, dice la mamma; non avete quattrini, vostro padre non vi lasciò nulla; dovreste andare ad accattare invece di star qui con noi, che siamo figli di signori, di mangiare i medesimi cibi che mangiamo e di esser vestita alle spese della mamma.\", \"Ora v'insegnerò a frugar nella mia biblioteca, perché questi libri sono miei, tutto mi appartiene in casa, o mi apparterrà fra pochi anni.\", 'Andate vicino alla porta, lontano dallo specchio e dalla finestra.', \"Ubbidii senza sapere che intenzione avesse; ma quando vidi che alzava il libro e far atto di gettarmelo contro, mi tirai istintivamente da parte, mandando un grido d'allarme. Non fui però abbastanza pronta; il volume volò per aria e mi colpì nella testa; io caddi e battendo nello spigolo della porta mi ferii.\", 'La ferita sanguinava ed io provai un gran dolore: ma il terrore era svanito per dar luogo ad altri sentimenti.', '— Perfido e crudele ragazzo! — dissi, — siete simile a un assassino, a un guardiano di schiavi, a un imperatore romano!']\n",
      "\n",
      "\n",
      "encoder_input torch.Size([10, 43])\n",
      "decoder_input torch.Size([10, 36])\n",
      "label torch.Size([10, 36])\n",
      "encoder_mask torch.Size([10, 1, 43])\n",
      "decoder_mask torch.Size([10, 36, 36])\n",
      "src_texts [\"I had read Goldsmith's History of Rome, and had formed my opinion of Nero, Caligula, &c. Also I had drawn parallels in silence, which I never thought thus to have declared aloud.\", '\"What! what!\" he cried.', '\"Did she say that to me?', 'Did you hear her, Eliza and Georgiana?', 'Won\\'t I tell mama? but first--\" He ran headlong at me: I felt him grasp my hair and my shoulder: he had closed with a desperate thing.', 'I really saw in him a tyrant, a murderer.', 'I felt a drop or two of blood from my head trickle down my neck, and was sensible of somewhat pungent suffering: these sensations for the time predominated over fear, and I received him in frantic sort.', 'I don\\'t very well know what I did with my hands, but he called me \"Rat!', 'Rat!\" and bellowed out aloud.', 'Aid was near him: Eliza and Georgiana had run for Mrs. Reed, who was gone upstairs: she now came upon the scene, followed by Bessie and her maid Abbot.']\n",
      "tgt_texts ['Avevo appunto letto la storia di Roma di Goldsmith e mi ero fatta un concetto di Nerone, di Caligola, che non credevo di dover esporre mai a voce alta.', '— Come!', 'Come! — esclamò. — Dice a me forse?', \"L'avete sentita, Eliza, Georgiana?\", 'Vado a dirlo a mamma, ma prima.... Egli si slanciò contro di me, e mi sentii afferrare per i capelli e per le spalle con disperato furore.', 'Io vedevo realmente in lui un assassino, un tiranno.', \"Sentii scendermi dalla testa e cadere sul collo una o due gocce di sangue e provai un'acuta sofferenza; queste sensazioni per un momento dominarono la paura e mi resero furente.\", 'Non so dire quello che io facessi con le mani, ma John mi chiamava: \"Talpa!', 'Talpa!\" e continuava a insultarmi.', 'Eliza e Georgiana erano corse a chiamar la mamma, che era salita al piano superiore. La signora Reed entrò durante quella scena, seguita da Bessie e da Abbot, la cameriera.']\n",
      "\n",
      "\n",
      "encoder_input torch.Size([10, 64])\n",
      "decoder_input torch.Size([10, 50])\n",
      "label torch.Size([10, 50])\n",
      "encoder_mask torch.Size([10, 1, 64])\n",
      "decoder_mask torch.Size([10, 50, 50])\n",
      "src_texts ['We were parted: I heard the words-- \"Dear! dear!', 'What a fury to fly at Master John!\"', '\"Did ever anybody see such a picture of passion!\"', 'Then Mrs. Reed subjoined--', '\"Take her away to the red-room, and lock her in there.\"', 'Four hands were immediately laid upon me, and I was borne upstairs.', 'CHAPTER II', 'I resisted all the way: a new thing for me, and a circumstance which greatly strengthened the bad opinion Bessie and Miss Abbot were disposed to entertain of me.', \"The fact is, I was a trifle beside myself; or rather _out_ of myself, as the French would say: I was conscious that a moment's mutiny had already rendered me liable to strange penalties, and, like any other rebel slave, I felt resolved, in my desperation, to go all lengths.\", '\"Hold her arms, Miss Abbot: she\\'s like a mad cat.\"']\n",
      "tgt_texts ['— Dio mio, che orrore!', 'Percuotere il signorino John!', '— Avete mai visto una rabbiosa come questa?', 'Allora la signora Reed soggiunse:', '— Portatela nella camera rossa e chiudetevela dentro.', 'Quattro mani mi afferrarono e io fui trascinata su per le scale.', 'II.', 'Opposi resistenza per tutto il percorso, così che accrebbi grandemente la cattiva opinione che Bessie e Abbot avevano di me.', 'È un fatto che non ero più io, o meglio ero fuori di me, come direbbero i francesi. Sapevo che quella ribellione momentanea mi avrebbe valso delle strane punizioni, e, pari a ogni schiavo ribelle, ero spinta agli estremi dalla disperazione stessa.', '— Reggetele le mani, signorina Abbot; è come un gatto infuriato. — Vergogna!']\n",
      "\n",
      "\n",
      "encoder_input torch.Size([10, 46])\n",
      "decoder_input torch.Size([10, 35])\n",
      "label torch.Size([10, 35])\n",
      "encoder_mask torch.Size([10, 1, 46])\n",
      "decoder_mask torch.Size([10, 35, 35])\n",
      "src_texts ['\"For shame! for shame!\" cried the lady\\'s-maid.', '\"What shocking conduct, Miss Eyre, to strike a young gentleman, your benefactress\\'s son!', 'Your young master.\"', '\"Master!', 'How is he my master? Am I a servant?\"', '\"No; you are less than a servant, for you do nothing for your keep.', 'There, sit down, and think over your wickedness.\"', 'They had got me by this time into the apartment indicated by Mrs. Reed, and had thrust me upon a stool: my impulse was to rise from it like a spring; their two pair of hands arrested me instantly.', '\"If you don\\'t sit still, you must be tied down,\" said Bessie.', '\"Miss Abbot, lend me your garters; she would break mine directly.\"']\n",
      "tgt_texts ['Vergogna! — esclamò la cameriera. — Che gatto arrabbiato!', 'Che scandalosa condotta, signorina Eyre!', 'Percuotere il signorino, il figlio della vostra benefattrice, il vostro padroncino!', '— Chi è il mio padrone?', 'Sono forse una serva?', '— No, siete meno che una serva, perché non vi guadagnate il pane.', 'Sedetevi qui e pensate alla vostra perfidia.', \"Intanto mi avevano condotta nell'appartamento indicato dalla signora Reed e mi avevano gettata su una sedia. Io mi sentii spinta ad alzarmi di botto; quattro mani mi trattennero subito.\", '— Se non state ferma costì a sedere, vi legheremo, — disse Bessie.', '— Signorina Abbot, prestatemi le vostre legaccie delle calze, perché presto avrò rotto le mie.']\n",
      "\n",
      "\n",
      "encoder_input torch.Size([10, 55])\n",
      "decoder_input torch.Size([10, 58])\n",
      "label torch.Size([10, 58])\n",
      "encoder_mask torch.Size([10, 1, 55])\n",
      "decoder_mask torch.Size([10, 58, 58])\n",
      "src_texts ['Miss Abbot turned to divest a stout leg of the necessary ligature.', 'This preparation for bonds, and the additional ignominy it inferred, took a little of the excitement out of me.', '\"Don\\'t take them off,\" I cried; \"I will not stir.\"', 'In guarantee whereof, I attached myself to my seat by my hands.', '\"Mind you don\\'t,\" said Bessie; and when she had ascertained that I was really subsiding, she loosened her hold of me; then she and Miss Abbot stood with folded arms, looking darkly and doubtfully on my face, as incredulous of my sanity.', '\"She never did so before,\" at last said Bessie, turning to the Abigail.', '\"But it was always in her,\" was the reply. \"I\\'ve told Missis often my opinion about the child, and Missis agreed with me. She\\'s an underhand little thing: I never saw a girl of her age with so much cover.\"', 'Bessie answered not; but ere long, addressing me, she said--\"You ought to be aware, Miss, that you are under obligations to Mrs. Reed: she keeps you: if she were to turn you off, you would have to go to the poorhouse.\"', 'I had nothing to say to these words: they were not new to me: my very first recollections of existence included hints of the same kind.', 'This reproach of my dependence had become a vague sing-song in my ear: very painful and crushing, but only half intelligible.']\n",
      "tgt_texts ['La signorina Abbot si affrettò a sciogliersi le calze.', 'Questo preparativo di legatura e la vergogna che per me ne derivava, calmarono la mia agitazione.', '— Non vi sciogliete le calze, non mi muoverò.', 'E per dare una prova di ciò che asserivo, mi avviticchiai alla sedia.', 'Quando fu sicura che avevo veramente intenzione di obbedirla, mi lasciò andare. Allora lei e Abbot incrociarono le braccia e mi guardarono severamente, come se avessero dubitato dello stato della mia mente.', '— Non era mai giunta a tanto, — disse Bessie alla fine, volgendosi verso Abigail Abbot.', '— Ma però si vedeva che sarebbe giunta a questo, — rispose Abbot. — Ho spesso palesato alla signora la mia opinione su questa bambina, e la signora ha convenuto che avevo ragione; è una creatura subdola; non ho mai veduto una bimba della sua età che sapesse finger così bene.', 'Bessie non rispose, ma poco dopo, rivolgendosi a me, disse: — Non sapete, signorina, che dovete tutto alla signora Reed? Vi tiene presso di sé, ma se vi mandasse via, dovreste andare in un ricovero di mendicità.', 'Non avevo nulla da rispondere a quelle parole, che non sonavano nuove al mio orecchio; i più antichi ricordi della mia esistenza si riferivano a parole simili.', 'Il rimprovero per il mio stato di dipendenza era divenuto per i miei orecchi un sono vago, penoso e opprimente, ma a metà inintelligibile.']\n",
      "\n",
      "\n",
      "encoder_input torch.Size([10, 103])\n",
      "decoder_input torch.Size([10, 67])\n",
      "label torch.Size([10, 67])\n",
      "encoder_mask torch.Size([10, 1, 103])\n",
      "decoder_mask torch.Size([10, 67, 67])\n",
      "src_texts ['Miss Abbot joined in-- \"And you ought not to think yourself on an equality with the Misses Reed and Master Reed, because Missis kindly allows you to be brought up with them.', 'They will have a great deal of money, and you will have none: it is your place to be humble, and to try to make yourself agreeable to them.\"', '\"What we tell you is for your good,\" added Bessie, in no harsh voice, \"you should try to be useful and pleasant, then, perhaps, you would have a home here; but if you become passionate and rude, Missis will send you away, I am sure.\"', '\"Besides,\" said Miss Abbot, \"God will punish her: He might strike her dead in the midst of her tantrums, and then where would she go?', \"Come, Bessie, we will leave her: I wouldn't have her heart for anything.\", 'Say your prayers, Miss Eyre, when you are by yourself; for if you don\\'t repent, something bad might be permitted to come down the chimney and fetch you away.\"', 'They went, shutting the door, and locking it behind them.', 'The red-room was a square chamber, very seldom slept in, I might say never, indeed, unless when a chance influx of visitors at Gateshead Hall rendered it necessary to turn to account all the accommodation it contained: yet it was one of the largest and stateliest chambers in the mansion.', 'A bed supported on massive pillars of mahogany, hung with curtains of deep red damask, stood out like a tabernacle in the centre; the two large windows, with their blinds always drawn down, were half shrouded in festoons and falls of similar drapery; the carpet was red; the table at the foot of the bed was covered with a crimson cloth; the walls were a soft fawn colour with a blush of pink in it; the wardrobe, the toilet-table, the chairs were of darkly polished old mahogany.', 'Out of these deep surrounding shades rose high, and glared white, the piled- up mattresses and pillows of the bed, spread with a snowy Marseilles counterpane.']\n",
      "tgt_texts ['La signorina Abbot soggiunse: — Spero che non vi crederete eguale alle signorine e al signor Reed, perché la signora è così buona da farvi educare insieme con loro.', 'Essi avranno molto danaro e voi non ne avrete punto; dovreste cercare di studiare di esser umile e di rendervi gradita a loro.', '— Quello che vi diciamo, è per il vostro bene, — aggiunse Bessie con voce che non era aspra; — dovreste cercare di rendervi utile e di farvi piacevole e allora forse potreste rimaner qui; ma se divenite violenta e brutale, la signora vi manderà via, ne son certa.', '— Inoltre, — continuò Abbot, — Iddio la punirà. Potrebbe colpirla con la morte mentre è in peccato, e allora dove andrà?', 'Venite, Bessie, lasciamola. Non vorrei davvero avere un cuore come il suo.', 'Dite le vostre preghiere, signorina Eyre; se non vi pentite, Iddio potrà concedere a qualche spirito malvagio di scendere dalla cappa del camino, e di portarvi via.', 'Le due donne se ne andarono sbatacchiando la porta e poi la chiusero a chiave.', \"La camera rossa era una camera riservata, dove raramente qualcuno dormiva. Non l'aveva mai veduta abitata altro che quando vi era molta affluenza di ospiti nella villa di Gateshead e occorreva trar partito da ogni stanza; era una delle camere più grandi e più eleganti della casa.\", \"Le due grandi finestre, con le persiane chiuse, erano ornate di drappeggiamenti della stessa stoffa. Il tappeto era rosso, la tavola, collocata a piè del letto, era coperta con un panno rosso; i muri erano coperti di carta giallastra a rose; l'armadio, la toilette, le seggiole, erano di vecchio mogano ben lustro.\", \"In mezzo a questo cupo arredamento, s'inalzava sul letto e si staccava in bianco, un mucchio di materasse abballinate e di guanciali, nascosti da una coperta di Marsiglia.\"]\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the dataloader to check shapes and functionality\n",
    "for idx, (\n",
    "    encoder_input,\n",
    "    decoder_input,\n",
    "    label,\n",
    "    encoder_mask,\n",
    "    decoder_mask,\n",
    "    src_texts,\n",
    "    tgt_texts,\n",
    ") in enumerate(custom_dataloader):\n",
    "    print(\"encoder_input\", encoder_input.shape)  # (batch_size, enc_seq_len)\n",
    "    print(\"decoder_input\", decoder_input.shape)  # (batch_size, dec_seq_len)\n",
    "    print(\"label\", label.shape)  # (batch_size, dec_seq_len)\n",
    "    print(\"encoder_mask\", encoder_mask.shape)  # (batch_size, 1, enc_seq_len)\n",
    "    # (batch_size, dec_seq_len, dec_seq_len)\n",
    "    print(\"decoder_mask\", decoder_mask.shape)\n",
    "    print(\"src_texts\", src_texts)  # List of source texts\n",
    "    print(\"tgt_texts\", tgt_texts)  # List of target texts\n",
    "\n",
    "    if idx == 10:\n",
    "        break\n",
    "\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detailed Steps and Reasoning\n",
    "\n",
    "1. **Dataset Loading**:\n",
    "   - The dataset is loaded using the Hugging Face `datasets` library, which simplifies accessing and managing datasets. The configuration specifies the data source and language pairs. This step ensures we have access to the raw data needed for training.\n",
    "\n",
    "2. **Tokenizer Building**:\n",
    "   - **Tokenizer Initialization**: A `WordLevel` tokenizer is initialized with an unknown token (`[UNK]`).\n",
    "   - **Whitespace Tokenization**: Tokenization is performed using whitespace, which is simple and effective for many languages.\n",
    "   - **Normalization**: Text is converted to lowercase to maintain consistency and reduce the vocabulary size.\n",
    "   - **Training**: The tokenizer is trained on the dataset, ensuring it learns the vocabulary specific to the source and target languages. Special tokens are included to handle unknown words, padding, start of sequence, and end of sequence tokens.\n",
    "\n",
    "3. **Bilingual Dataset Class**:\n",
    "   - This class handles the preparation of bilingual translation data. It tokenizes the input text and adds special tokens required for the transformer model.\n",
    "   - **Special Tokens**: `[SOS]` (start of sequence) and `[EOS]` (end of sequence) tokens are added to the inputs and labels to guide the model during training.\n",
    "\n",
    "4. **Causal Mask**:\n",
    "   - The causal mask ensures that each position in the decoder can only attend to previous positions. This is essential for autoregressive decoding, where the model generates one token at a time.\n",
    "\n",
    "5. **Collate Function**:\n",
    "   - The `collate_fn` function handles batching of data. It pads sequences to the same length within a batch and creates masks to ignore padding tokens during model training.\n",
    "   - **Padding**: Sequences are padded to the maximum length within the batch using the `[PAD]` token.\n",
    "   - **Masks**: Encoder and decoder masks are created to ignore padding tokens and enforce causal attention in the decoder.\n",
    "\n",
    "6. **DataLoader**:\n",
    "\n",
    "\n",
    "   - A `DataLoader` is created to handle batching and shuffling of data during training. The `batch_size` is set to 10, but this can be adjusted based on available memory and model requirements.\n",
    "   - **Iteration and Checking**: The dataloader is iterated over to check the shapes and functionality of the batched data. This ensures that the data is correctly prepared for model training.\n",
    "\n",
    "By following these steps, the code is structured to efficiently prepare data for training a transformer model, ensuring proper tokenization, masking, and batching. This refined code is optimized for readability, maintainability, and functionality, adhering to best practices for transformer model training in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
