{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "IPython.notebook.set_autosave_interval(300000)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 300 seconds\n"
     ]
    }
   ],
   "source": [
    "%autosave 300\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/batch/tasks/shared/LS_root/mounts/clusters/insights-model-run2/code/Users/soutrik.chowdhury/EraV2_Transformers\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\n",
    "    \"/mnt/batch/tasks/shared/LS_root/mounts/clusters/insights-model-run2/code/Users/soutrik.chowdhury/EraV2_Transformers\"\n",
    ")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import AdamW\n",
    "from contextlib import nullcontext\n",
    "from typing import Tuple\n",
    "import inspect\n",
    "from notebooks.gpt2_models.dummy_model import GPT, GPTConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "DATA_DIR = os.path.join(os.getcwd(), \"data/tinyshakespeare\")\n",
    "ENCODER = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "MAX_LR = 6e-4\n",
    "MIN_LR = MAX_LR * 0.3\n",
    "WARMUP_STEPS = 10\n",
    "WEIGHT_DECAY = 0.1\n",
    "NUM_EPOCHS = 2\n",
    "MIN_VAL_ITER = 10\n",
    "\n",
    "IDEAL_BATCH_SIZE = 524288  # Batch size for the model as per GPT-3 paper\n",
    "PERMIT_BATCH = 8  # No of batches as permitted by GPU\n",
    "PERMIT_CONTEXT = 1024  # context length / sequence length\n",
    "GRAD_ACCUMULATION_STEPS = IDEAL_BATCH_SIZE // (PERMIT_BATCH * PERMIT_CONTEXT)\n",
    "\n",
    "GRAD_CLIP = 0.0\n",
    "COMPILE = True if GPTConfig().flash_attention else False\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE_TYPE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DTYPE = (\n",
    "    \"bfloat16\"\n",
    "    if torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "    else \"float16\"\n",
    ")\n",
    "ptdtype = {\n",
    "    \"float32\": torch.float32,\n",
    "    \"bfloat16\": torch.bfloat16,\n",
    "    \"float16\": torch.float16,\n",
    "}[DTYPE]\n",
    "\n",
    "\n",
    "# Set the random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_train_val_token_array(data_dir, filename, encoder, split_ratio=0.9):\n",
    "    \"\"\"Read the file, encode it, split it into train and val, and save the token array as binary files\"\"\"\n",
    "    file_path = os.path.join(data_dir, filename)\n",
    "    print(f\"Reading file from {file_path}\")\n",
    "\n",
    "    # Read the text data from the file\n",
    "    with open(file_path, \"r\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # Split the text into train and validation parts\n",
    "    train_len = int(split_ratio * len(text))\n",
    "    train_str, val_str = text[:train_len], text[train_len:]\n",
    "\n",
    "    print(f\"Train length: {len(train_str)}\")\n",
    "    print(f\"Val length: {len(val_str)}\")\n",
    "\n",
    "    # Encode the train and validation text to tokens\n",
    "    train_tokens, val_tokens = encoder.encode(train_str), encoder.encode(val_str)\n",
    "\n",
    "    print(f\"Train tokens: {len(train_tokens)}\")\n",
    "    print(f\"Val tokens: {len(val_tokens)}\")\n",
    "\n",
    "    # Save the tokens as binary files\n",
    "    np.array(train_tokens, dtype=np.uint16).tofile(os.path.join(data_dir, \"train.bin\"))\n",
    "    np.array(val_tokens, dtype=np.uint16).tofile(os.path.join(data_dir, \"val.bin\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_token_array(data_dir, filename):\n",
    "    \"\"\"Load the token array from binary file\"\"\"\n",
    "    token_data = np.memmap(\n",
    "        os.path.join(data_dir, f\"{filename}.bin\"), dtype=np.uint16, mode=\"r\"\n",
    "    )\n",
    "    return torch.from_numpy(token_data.astype(np.int64))\n",
    "\n",
    "\n",
    "class DataloaderLite:\n",
    "    def __init__(self, B: int, T: int, data_dir: str, filename: str):\n",
    "        self.B = B  # batch size\n",
    "        self.T = T  # seq length\n",
    "        self.data_dir = data_dir\n",
    "        self.filename = filename\n",
    "        assert filename in [\"train\", \"val\"], \"Only 'train' and 'val' files are allowed\"\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        self.tokens = load_token_array(self.data_dir, self.filename)\n",
    "        print(f\"Total tokens in the file: {len(self.tokens)}\")\n",
    "        self.current_position = 0\n",
    "        self.num_iterations = len(self.tokens) // (self.B * self.T)\n",
    "        print(\n",
    "            f\"1 epoch will have {self.num_iterations} iterations given Batch size={self.B} and Context length={self.T}\"\n",
    "        )\n",
    "\n",
    "    def next_batch(self) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        B, T = self.B, self.T\n",
    "        # print(\n",
    "        #     f\"Current position: {self.current_position} and total tokens: {len(self.tokens)}\"\n",
    "        # )\n",
    "        if self.current_position + B * T + 1 > len(self.tokens):\n",
    "            # print(\n",
    "            #     \"Resetting the position for the next batch as current iter exhausts the data file\"\n",
    "            # )\n",
    "            self.reset()\n",
    "\n",
    "        end_position = self.current_position + B * T + 1\n",
    "        current_batch = (\n",
    "            self.tokens[self.current_position : end_position].clone().detach()\n",
    "        )  # clone and detach to avoid memory leak\n",
    "\n",
    "        # reshaping the tensor to (B, T) shape and creating input and target tensors\n",
    "        x = current_batch[:-1].view(B, T)  # input tensor of shape (B, T)\n",
    "        y = current_batch[1:].view(B, T)  # target tensor of shape (B, T)\n",
    "\n",
    "        # updating the position for the next batch\n",
    "        self.current_position += B * T\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_schedule_lr(\n",
    "    it, warmup_iters=500, learning_rate=0.1, lr_decay_iters=1000, min_lr=0.01\n",
    "):\n",
    "    \"\"\"Get the learning rate schedule for training after cosine annealing\"\"\"\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * (it + 1) / warmup_iters\n",
    "    if it > lr_decay_iters:\n",
    "        return min_lr\n",
    "\n",
    "    # Cosine annealing learning rate schedule\n",
    "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
    "    return min_lr + coeff * (learning_rate - min_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_optimizers(model, weight_decay, learning_rate, device_type):\n",
    "    \"\"\"Create the optimizer and scheduler for training\"\"\"\n",
    "\n",
    "    # start with all of the candidate parameters (that require grad)\n",
    "    param_dict = {pn: p for pn, p in model.named_parameters()}\n",
    "    param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "\n",
    "    # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
    "    # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
    "    decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "    nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "    optim_groups = [\n",
    "        {\"params\": decay_params, \"weight_decay\": weight_decay},\n",
    "        {\"params\": nodecay_params, \"weight_decay\": 0.0},\n",
    "    ]\n",
    "\n",
    "    num_decay_params = sum(p.numel() for p in decay_params)\n",
    "    num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "\n",
    "    print(\n",
    "        f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\"\n",
    "    )\n",
    "    print(\n",
    "        f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\"\n",
    "    )\n",
    "    # Create AdamW optimizer and use the fused version if it is available\n",
    "    fused_available = \"fused\" in inspect.signature(torch.optim.AdamW).parameters\n",
    "    use_fused = fused_available and device_type == \"cuda\"\n",
    "\n",
    "    optimizer = AdamW(\n",
    "        optim_groups, lr=learning_rate, betas=(0.9, 0.95), eps=1e-8, fused=use_fused\n",
    "    )\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, dataloader, device, ctx, min_val_iter):\n",
    "    \"\"\"Validate the model\"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    dataloader.reset()\n",
    "    val_loss_accum = 0.0\n",
    "    with torch.no_grad():\n",
    "        iter_range = min(min_val_iter, dataloader.num_iterations)\n",
    "        for _ in range(iter_range):\n",
    "            x, y = dataloader.next_batch()\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            with ctx:\n",
    "                _, loss = model(x, y)\n",
    "            val_loss_accum += loss.detach() / iter_range\n",
    "    print(f\"Validation loss: {val_loss_accum.item():.4f}\")\n",
    "    return val_loss_accum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model,\n",
    "    optimizer,\n",
    "    train_dataloader,\n",
    "    grad_accumulation_steps,\n",
    "    ctx,\n",
    "    scaler,\n",
    "    device,\n",
    "    grad_clip,\n",
    "    iteration,\n",
    "    total_iterations,\n",
    "    warmup_steps,\n",
    "    max_lr,\n",
    "    min_lr,\n",
    "):\n",
    "    \"\"\"Train the model\"\"\"\n",
    "    model.train()  # Set the model to training mode\n",
    "    optimizer.zero_grad()  # Zero out the gradients\n",
    "    loss_accum = 0.0\n",
    "\n",
    "    for _ in range(grad_accumulation_steps):\n",
    "        x, y = train_dataloader.next_batch()\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        with ctx:\n",
    "            _, loss = model(x, y)\n",
    "\n",
    "        loss /= grad_accumulation_steps  # Normalize loss\n",
    "        loss_accum += loss.detach()\n",
    "        scaler.scale(loss).backward()  # Backpropagation\n",
    "\n",
    "    if grad_clip != 0.0:\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "\n",
    "    lr = get_schedule_lr(\n",
    "        iteration, warmup_steps, max_lr, int(total_iterations * 0.8), min_lr\n",
    "    )\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = lr\n",
    "\n",
    "    scaler.step(optimizer)  # Update the weights\n",
    "    scaler.update()  # Update the scale for next iteration\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return loss_accum, lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_train_eval():\n",
    "    # Prepare the training and validation data\n",
    "    prep_train_val_token_array(DATA_DIR, \"input.txt\", ENCODER)\n",
    "\n",
    "    train_dataloader = DataloaderLite(PERMIT_BATCH, PERMIT_CONTEXT, DATA_DIR, \"train\")\n",
    "    val_dataloader = DataloaderLite(PERMIT_BATCH, PERMIT_CONTEXT, DATA_DIR, \"val\")\n",
    "\n",
    "    ctx = (\n",
    "        nullcontext()\n",
    "        if DEVICE_TYPE == \"cpu\"\n",
    "        else torch.amp.autocast(device_type=DEVICE_TYPE, dtype=ptdtype)\n",
    "    )\n",
    "\n",
    "    if ctx != nullcontext():\n",
    "        scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
    "\n",
    "    # Clear GPU memory\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    # Initialize the model\n",
    "    transformer_model = GPT(\n",
    "        GPTConfig(vocab_size=50304, block_size=1024, n_layer=12, n_head=12, n_embd=768)\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    if COMPILE:\n",
    "        transformer_model = torch.compile(transformer_model)\n",
    "\n",
    "    # Calculate the total number of iterations\n",
    "    total_iterations = NUM_EPOCHS * train_dataloader.num_iterations\n",
    "    print(f\"Total iterations: {total_iterations}\")\n",
    "    optimizer = configure_optimizers(\n",
    "        transformer_model, WEIGHT_DECAY, MAX_LR, DEVICE_TYPE\n",
    "    )\n",
    "\n",
    "    for iteration in range(total_iterations):\n",
    "        start_time = time.time()\n",
    "\n",
    "        if iteration % train_dataloader.num_iterations == 0:\n",
    "            print(f\"Epoch: {iteration // train_dataloader.num_iterations}\")\n",
    "            print(\"Model Evaluation.....\")\n",
    "            val_loss_accum = validate_model(\n",
    "                transformer_model, val_dataloader, DEVICE, ctx, MIN_VAL_ITER\n",
    "            )\n",
    "\n",
    "        print(\"Model Training.....\")\n",
    "        train_loss_accum, lr = train_model(\n",
    "            transformer_model,\n",
    "            optimizer,\n",
    "            train_dataloader,\n",
    "            GRAD_ACCUMULATION_STEPS,\n",
    "            ctx,\n",
    "            scaler,\n",
    "            DEVICE,\n",
    "            GRAD_CLIP,\n",
    "            iteration,\n",
    "            total_iterations,\n",
    "            WARMUP_STEPS,\n",
    "            MAX_LR,\n",
    "            MIN_LR,\n",
    "        )\n",
    "\n",
    "        end_time = time.time()\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "        time_elapsed = (end_time - start_time) * 1000\n",
    "        token_persec = (\n",
    "            PERMIT_BATCH * PERMIT_CONTEXT * GRAD_ACCUMULATION_STEPS\n",
    "        ) / time_elapsed\n",
    "        print(\n",
    "            f\"Iter: {iteration} | Train loss: {train_loss_accum.item():.4f} | Validation loss: {val_loss_accum.item():.4f} | LR: {lr:.4e} | Time: {time_elapsed:.2f}ms | Tokens/ms: {token_persec:.2f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file from /mnt/batch/tasks/shared/LS_root/mounts/clusters/insights-model-run2/code/Users/soutrik.chowdhury/EraV2_Transformers/data/tinyshakespeare/input.txt\n",
      "Train length: 1003854\n",
      "Val length: 111540\n",
      "Train tokens: 301966\n",
      "Val tokens: 36059\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Total tokens in the file: 36059\n",
      "1 epoch will have 4 iterations given Batch size=8 and Context length=1024\n",
      "Flash attention: True\n",
      "Flash attention: True\n",
      "Flash attention: True\n",
      "Flash attention: True\n",
      "Flash attention: True\n",
      "Flash attention: True\n",
      "Flash attention: True\n",
      "Flash attention: True\n",
      "Flash attention: True\n",
      "Flash attention: True\n",
      "Flash attention: True\n",
      "Flash attention: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/torch_env/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total iterations: 72\n",
      "num decayed parameter tensors: 50, with 124,354,560 parameters\n",
      "num non-decayed parameter tensors: 98, with 121,344 parameters\n",
      "Epoch: 0\n",
      "Model Evaluation.....\n",
      "Total tokens in the file: 36059\n",
      "1 epoch will have 4 iterations given Batch size=8 and Context length=1024\n",
      "Validation loss: 10.9303\n",
      "Model Training.....\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Iter: 0 | Train loss: 10.9230 | Validation loss: 10.9303 | LR: 6.0000e-05 | Time: 52345.26ms | Tokens/ms: 10.02\n",
      "Model Training.....\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Iter: 1 | Train loss: 9.6116 | Validation loss: 10.9303 | LR: 1.2000e-04 | Time: 9079.20ms | Tokens/ms: 57.75\n",
      "Model Training.....\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Iter: 2 | Train loss: 9.1785 | Validation loss: 10.9303 | LR: 1.8000e-04 | Time: 9145.25ms | Tokens/ms: 57.33\n",
      "Model Training.....\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Iter: 3 | Train loss: 9.5007 | Validation loss: 10.9303 | LR: 2.4000e-04 | Time: 9230.49ms | Tokens/ms: 56.80\n",
      "Model Training.....\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Iter: 4 | Train loss: 9.0295 | Validation loss: 10.9303 | LR: 3.0000e-04 | Time: 9139.14ms | Tokens/ms: 57.37\n",
      "Model Training.....\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Iter: 5 | Train loss: 8.6296 | Validation loss: 10.9303 | LR: 3.6000e-04 | Time: 9200.62ms | Tokens/ms: 56.98\n",
      "Model Training.....\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Iter: 6 | Train loss: 8.2872 | Validation loss: 10.9303 | LR: 4.2000e-04 | Time: 9079.47ms | Tokens/ms: 57.74\n",
      "Model Training.....\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Iter: 7 | Train loss: 8.0286 | Validation loss: 10.9303 | LR: 4.8000e-04 | Time: 9198.96ms | Tokens/ms: 56.99\n",
      "Model Training.....\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Iter: 8 | Train loss: 7.7694 | Validation loss: 10.9303 | LR: 5.4000e-04 | Time: 9082.57ms | Tokens/ms: 57.72\n",
      "Model Training.....\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Iter: 9 | Train loss: 7.4097 | Validation loss: 10.9303 | LR: 6.0000e-04 | Time: 9178.89ms | Tokens/ms: 57.12\n",
      "Model Training.....\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Iter: 10 | Train loss: 7.1236 | Validation loss: 10.9303 | LR: 6.0000e-04 | Time: 9086.25ms | Tokens/ms: 57.70\n",
      "Model Training.....\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Iter: 11 | Train loss: 6.9201 | Validation loss: 10.9303 | LR: 5.9953e-04 | Time: 9125.75ms | Tokens/ms: 57.45\n",
      "Model Training.....\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Iter: 12 | Train loss: 6.7692 | Validation loss: 10.9303 | LR: 5.9813e-04 | Time: 9091.53ms | Tokens/ms: 57.67\n",
      "Model Training.....\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Iter: 13 | Train loss: 6.6414 | Validation loss: 10.9303 | LR: 5.9579e-04 | Time: 9075.71ms | Tokens/ms: 57.77\n",
      "Model Training.....\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Iter: 14 | Train loss: 6.5409 | Validation loss: 10.9303 | LR: 5.9254e-04 | Time: 9100.07ms | Tokens/ms: 57.61\n",
      "Model Training.....\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Iter: 15 | Train loss: 6.4765 | Validation loss: 10.9303 | LR: 5.8838e-04 | Time: 9080.90ms | Tokens/ms: 57.74\n",
      "Model Training.....\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Iter: 16 | Train loss: 6.4303 | Validation loss: 10.9303 | LR: 5.8334e-04 | Time: 9081.95ms | Tokens/ms: 57.73\n",
      "Model Training.....\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Iter: 17 | Train loss: 6.4272 | Validation loss: 10.9303 | LR: 5.7743e-04 | Time: 9093.19ms | Tokens/ms: 57.66\n",
      "Model Training.....\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Iter: 18 | Train loss: 6.4230 | Validation loss: 10.9303 | LR: 5.7068e-04 | Time: 9212.35ms | Tokens/ms: 56.91\n",
      "Model Training.....\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Iter: 19 | Train loss: 6.4253 | Validation loss: 10.9303 | LR: 5.6313e-04 | Time: 9087.30ms | Tokens/ms: 57.69\n",
      "Model Training.....\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Iter: 20 | Train loss: 6.4159 | Validation loss: 10.9303 | LR: 5.5481e-04 | Time: 9126.34ms | Tokens/ms: 57.45\n",
      "Model Training.....\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Iter: 21 | Train loss: 6.4193 | Validation loss: 10.9303 | LR: 5.4575e-04 | Time: 9083.69ms | Tokens/ms: 57.72\n",
      "Model Training.....\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Iter: 22 | Train loss: 6.4070 | Validation loss: 10.9303 | LR: 5.3599e-04 | Time: 9097.63ms | Tokens/ms: 57.63\n",
      "Model Training.....\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Iter: 23 | Train loss: 6.3800 | Validation loss: 10.9303 | LR: 5.2558e-04 | Time: 9092.60ms | Tokens/ms: 57.66\n",
      "Model Training.....\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Iter: 24 | Train loss: 6.3593 | Validation loss: 10.9303 | LR: 5.1457e-04 | Time: 9087.08ms | Tokens/ms: 57.70\n",
      "Model Training.....\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Iter: 25 | Train loss: 6.3317 | Validation loss: 10.9303 | LR: 5.0300e-04 | Time: 9242.34ms | Tokens/ms: 56.73\n",
      "Model Training.....\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Iter: 26 | Train loss: 6.3221 | Validation loss: 10.9303 | LR: 4.9092e-04 | Time: 9183.98ms | Tokens/ms: 57.09\n",
      "Model Training.....\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Iter: 27 | Train loss: 6.2913 | Validation loss: 10.9303 | LR: 4.7840e-04 | Time: 9160.08ms | Tokens/ms: 57.24\n",
      "Model Training.....\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Iter: 28 | Train loss: 6.2675 | Validation loss: 10.9303 | LR: 4.6548e-04 | Time: 9095.94ms | Tokens/ms: 57.64\n",
      "Model Training.....\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Iter: 29 | Train loss: 6.2403 | Validation loss: 10.9303 | LR: 4.5222e-04 | Time: 9155.72ms | Tokens/ms: 57.26\n",
      "Model Training.....\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Iter: 30 | Train loss: 6.2386 | Validation loss: 10.9303 | LR: 4.3868e-04 | Time: 9102.52ms | Tokens/ms: 57.60\n",
      "Model Training.....\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Iter: 31 | Train loss: 6.2260 | Validation loss: 10.9303 | LR: 4.2493e-04 | Time: 9123.87ms | Tokens/ms: 57.46\n",
      "Model Training.....\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Iter: 32 | Train loss: 6.2088 | Validation loss: 10.9303 | LR: 4.1102e-04 | Time: 9075.17ms | Tokens/ms: 57.77\n",
      "Model Training.....\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Iter: 33 | Train loss: 6.2067 | Validation loss: 10.9303 | LR: 3.9702e-04 | Time: 9109.17ms | Tokens/ms: 57.56\n",
      "Model Training.....\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Iter: 34 | Train loss: 6.1930 | Validation loss: 10.9303 | LR: 3.8298e-04 | Time: 9085.67ms | Tokens/ms: 57.70\n",
      "Model Training.....\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Iter: 35 | Train loss: 6.1952 | Validation loss: 10.9303 | LR: 3.6898e-04 | Time: 9081.32ms | Tokens/ms: 57.73\n",
      "Epoch: 1\n",
      "Model Evaluation.....\n",
      "Total tokens in the file: 36059\n",
      "1 epoch will have 4 iterations given Batch size=8 and Context length=1024\n",
      "Validation loss: 6.4581\n",
      "Model Training.....\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Iter: 36 | Train loss: 6.1779 | Validation loss: 6.4581 | LR: 3.5507e-04 | Time: 9417.18ms | Tokens/ms: 55.67\n",
      "Model Training.....\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Iter: 37 | Train loss: 6.1743 | Validation loss: 6.4581 | LR: 3.4132e-04 | Time: 9098.73ms | Tokens/ms: 57.62\n",
      "Model Training.....\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Iter: 38 | Train loss: 6.1561 | Validation loss: 6.4581 | LR: 3.2778e-04 | Time: 9100.65ms | Tokens/ms: 57.61\n",
      "Model Training.....\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Iter: 39 | Train loss: 6.1583 | Validation loss: 6.4581 | LR: 3.1452e-04 | Time: 9153.02ms | Tokens/ms: 57.28\n",
      "Model Training.....\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Iter: 40 | Train loss: 6.1428 | Validation loss: 6.4581 | LR: 3.0160e-04 | Time: 9116.48ms | Tokens/ms: 57.51\n",
      "Model Training.....\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Iter: 41 | Train loss: 6.1242 | Validation loss: 6.4581 | LR: 2.8908e-04 | Time: 9110.28ms | Tokens/ms: 57.55\n",
      "Model Training.....\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Iter: 42 | Train loss: 6.1229 | Validation loss: 6.4581 | LR: 2.7700e-04 | Time: 9124.42ms | Tokens/ms: 57.46\n",
      "Model Training.....\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Iter: 43 | Train loss: 6.1036 | Validation loss: 6.4581 | LR: 2.6543e-04 | Time: 9090.62ms | Tokens/ms: 57.67\n",
      "Model Training.....\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Iter: 44 | Train loss: 6.1005 | Validation loss: 6.4581 | LR: 2.5442e-04 | Time: 9104.21ms | Tokens/ms: 57.59\n",
      "Model Training.....\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Iter: 45 | Train loss: 6.0806 | Validation loss: 6.4581 | LR: 2.4401e-04 | Time: 9161.56ms | Tokens/ms: 57.23\n",
      "Model Training.....\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Iter: 46 | Train loss: 6.0757 | Validation loss: 6.4581 | LR: 2.3425e-04 | Time: 9103.36ms | Tokens/ms: 57.59\n",
      "Model Training.....\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Iter: 47 | Train loss: 6.0560 | Validation loss: 6.4581 | LR: 2.2519e-04 | Time: 9127.37ms | Tokens/ms: 57.44\n",
      "Model Training.....\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Iter: 48 | Train loss: 6.0599 | Validation loss: 6.4581 | LR: 2.1687e-04 | Time: 9100.96ms | Tokens/ms: 57.61\n",
      "Model Training.....\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Iter: 49 | Train loss: 6.0483 | Validation loss: 6.4581 | LR: 2.0932e-04 | Time: 9116.49ms | Tokens/ms: 57.51\n",
      "Model Training.....\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Iter: 50 | Train loss: 6.0304 | Validation loss: 6.4581 | LR: 2.0257e-04 | Time: 9125.34ms | Tokens/ms: 57.45\n",
      "Model Training.....\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Iter: 51 | Train loss: 6.0279 | Validation loss: 6.4581 | LR: 1.9666e-04 | Time: 9100.63ms | Tokens/ms: 57.61\n",
      "Model Training.....\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Iter: 52 | Train loss: 6.0146 | Validation loss: 6.4581 | LR: 1.9162e-04 | Time: 9114.24ms | Tokens/ms: 57.52\n",
      "Model Training.....\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Iter: 53 | Train loss: 6.0130 | Validation loss: 6.4581 | LR: 1.8746e-04 | Time: 9115.52ms | Tokens/ms: 57.52\n",
      "Model Training.....\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Iter: 54 | Train loss: 5.9972 | Validation loss: 6.4581 | LR: 1.8421e-04 | Time: 9156.98ms | Tokens/ms: 57.26\n",
      "Model Training.....\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Iter: 55 | Train loss: 5.9952 | Validation loss: 6.4581 | LR: 1.8187e-04 | Time: 9285.89ms | Tokens/ms: 56.46\n",
      "Model Training.....\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Iter: 56 | Train loss: 5.9768 | Validation loss: 6.4581 | LR: 1.8047e-04 | Time: 9111.34ms | Tokens/ms: 57.54\n",
      "Model Training.....\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Iter: 57 | Train loss: 5.9851 | Validation loss: 6.4581 | LR: 1.8000e-04 | Time: 9221.98ms | Tokens/ms: 56.85\n",
      "Model Training.....\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Iter: 58 | Train loss: 5.9729 | Validation loss: 6.4581 | LR: 1.8000e-04 | Time: 9118.50ms | Tokens/ms: 57.50\n",
      "Model Training.....\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Iter: 59 | Train loss: 5.9570 | Validation loss: 6.4581 | LR: 1.8000e-04 | Time: 9223.66ms | Tokens/ms: 56.84\n",
      "Model Training.....\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Iter: 60 | Train loss: 5.9587 | Validation loss: 6.4581 | LR: 1.8000e-04 | Time: 9087.43ms | Tokens/ms: 57.69\n",
      "Model Training.....\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Iter: 61 | Train loss: 5.9427 | Validation loss: 6.4581 | LR: 1.8000e-04 | Time: 9235.97ms | Tokens/ms: 56.77\n",
      "Model Training.....\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Iter: 62 | Train loss: 5.9468 | Validation loss: 6.4581 | LR: 1.8000e-04 | Time: 9156.27ms | Tokens/ms: 57.26\n",
      "Model Training.....\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Iter: 63 | Train loss: 5.9318 | Validation loss: 6.4581 | LR: 1.8000e-04 | Time: 9169.94ms | Tokens/ms: 57.17\n",
      "Model Training.....\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Iter: 64 | Train loss: 5.9272 | Validation loss: 6.4581 | LR: 1.8000e-04 | Time: 9111.15ms | Tokens/ms: 57.54\n",
      "Model Training.....\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Iter: 65 | Train loss: 5.9101 | Validation loss: 6.4581 | LR: 1.8000e-04 | Time: 9150.32ms | Tokens/ms: 57.30\n",
      "Model Training.....\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Iter: 66 | Train loss: 5.9164 | Validation loss: 6.4581 | LR: 1.8000e-04 | Time: 9147.31ms | Tokens/ms: 57.32\n",
      "Model Training.....\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Iter: 67 | Train loss: 5.9048 | Validation loss: 6.4581 | LR: 1.8000e-04 | Time: 9167.43ms | Tokens/ms: 57.19\n",
      "Model Training.....\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Iter: 68 | Train loss: 5.8886 | Validation loss: 6.4581 | LR: 1.8000e-04 | Time: 9104.23ms | Tokens/ms: 57.59\n",
      "Model Training.....\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Iter: 69 | Train loss: 5.8895 | Validation loss: 6.4581 | LR: 1.8000e-04 | Time: 9142.06ms | Tokens/ms: 57.35\n",
      "Model Training.....\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Iter: 70 | Train loss: 5.8755 | Validation loss: 6.4581 | LR: 1.8000e-04 | Time: 9139.57ms | Tokens/ms: 57.36\n",
      "Model Training.....\n",
      "Total tokens in the file: 301966\n",
      "1 epoch will have 36 iterations given Batch size=8 and Context length=1024\n",
      "Iter: 71 | Train loss: 5.8824 | Validation loss: 6.4581 | LR: 1.8000e-04 | Time: 9097.54ms | Tokens/ms: 57.63\n"
     ]
    }
   ],
   "source": [
    "main_train_eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
