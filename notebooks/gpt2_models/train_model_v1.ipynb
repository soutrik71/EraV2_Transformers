{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "IPython.notebook.set_autosave_interval(300000)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 300 seconds\n"
     ]
    }
   ],
   "source": [
    "%autosave 300\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/batch/tasks/shared/LS_root/mounts/clusters/insights-model-run2/code/Users/soutrik.chowdhury/EraV2_Transformers\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\n",
    "    \"/mnt/batch/tasks/shared/LS_root/mounts/clusters/insights-model-run2/code/Users/soutrik.chowdhury/EraV2_Transformers\"\n",
    ")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "from notebooks.gpt2_models.dummy_model import GPT, GPTConfig\n",
    "from torch.nn import functional as F\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Init using Pretrained weights for GPT-2 from HF\n",
    "Initiate the model with pretrained weights from HF for Gpt2 and try creating 5 predicted sequences using same instruction prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_return_sequences = 5 # Number of sentences to generate\n",
    "max_length =30 # Maximum length of the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights from pretrained gpt: gpt2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT(\n",
      "  (transformer): ModuleDict(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): CausalSelfAttention(\n",
      "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (gelu): GELU(approximate='tanh')\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "pretrained_model = GPT.from_pretrained(\"gpt2\")\n",
    "print(pretrained_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do we have - First and embedding layer with vocab size of 50257 and embedding size of 768. Post which we have pos embedding of 1024 block size .Then we have 12 transformer decoder layers with 12 heads each. Each decoder layer is build up with LayerNorm -> MultiHeadAttention -> LayerNorm -> FeedForward -> LayerNorm. The output of the last decoder layer is passed through a linear layer to get the logits for the next token prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic settings\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "device_type = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): GELU(approximate='tanh')\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_model.to(device)\n",
    "pretrained_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14])\n"
     ]
    }
   ],
   "source": [
    "dummy_str = \"Hello I am a language Model, I am here to help you.\"\n",
    "encoder = tiktoken.get_encoding(encoding_name=\"gpt2\")\n",
    "tokens = encoder.encode(dummy_str)\n",
    "tokens_vec = torch.tensor(tokens, dtype=torch.long)\n",
    "print(tokens_vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 14])\n"
     ]
    }
   ],
   "source": [
    "# creating a batch of repeated tokens\n",
    "tokens_batch = tokens_vec.unsqueeze(0).repeat(num_return_sequences, 1)\n",
    "tokens_batch = tokens_batch.to(device)\n",
    "print(tokens_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_prediction(xgen, model):\n",
    "\n",
    "    xgen = xgen.clone()\n",
    "\n",
    "    while xgen.size(1) < max_length:\n",
    "\n",
    "        # forward the model to get the logits\n",
    "        with torch.no_grad():\n",
    "            logits, loss = model(xgen)  # (B, T, vocab_size)\n",
    "            # take the logits at the last position\n",
    "            logits = logits[:, -1, :]  # (B, vocab_size)\n",
    "            # get the probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # do top-k sampling of 50 (huggingface pipeline default)\n",
    "            # topk_probs here becomes (5, 50), topk_indices is (5, 50)\n",
    "            topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
    "            # select a token from the top-k probabilities\n",
    "            # note: multinomial does not demand the input to sum to 1\n",
    "            ix = torch.multinomial(topk_probs, 1)  # (B, 1)\n",
    "            # gather the corresponding indices\n",
    "            xcol = torch.gather(topk_indices, -1, ix)  # (B, 1)\n",
    "            # append to the sequence\n",
    "            xgen = torch.cat((xgen, xcol), dim=1)\n",
    "\n",
    "    return xgen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_batch = batch_prediction(tokens_batch, pretrained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 30])\n"
     ]
    }
   ],
   "source": [
    "print(prediction_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>Hello I am a language Model, I am here to help you. I am very happy with where this world would be if you could give me something\n",
      "\n",
      "\n",
      ">>Hello I am a language Model, I am here to help you.\n",
      "\n",
      "Thank you,\n",
      "\n",
      "Emma<|endoftext|>Liz Peltz:\n",
      "\n",
      "\n",
      ">>Hello I am a language Model, I am here to help you.\n",
      "\n",
      "But as you say, my job now is to get things done.\n",
      "\n",
      "\n",
      ">>Hello I am a language Model, I am here to help you.\n",
      "\n",
      "I am a language model, I am here to help you. You\n",
      "\n",
      "\n",
      ">>Hello I am a language Model, I am here to help you. If you have no idea what a data model is:\n",
      "\n",
      "So. what\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(prediction_batch.size(0)):\n",
    "    x = prediction_batch[i, :max_length].tolist()\n",
    "    decoded_str = encoder.decode(x)\n",
    "    print(f\">>{decoded_str}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Model Training of the model with toy data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets define the data loader for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = tiktoken.get_encoding(encoding_name=\"gpt2\")\n",
    "filename = \"./data/tinyshakespeare/input.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tokens(filename, encoder):\n",
    "    with open(filename, \"r\") as file:\n",
    "        lines = file.read()\n",
    "\n",
    "    tokens = encoder.encode(lines)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataloaderLiter:\n",
    "    def __init__(self, B, T, encoder, filename):\n",
    "        self.B = B  # batch size\n",
    "        self.T = T  # seq length\n",
    "        tokens = read_tokens(filename, encoder)\n",
    "        self.tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "\n",
    "        print(f\"Total tokens in the file: {len(self.tokens)}\")\n",
    "        print(f\"1 epoch will have {len(self.tokens) // (B * T)} iterations\")\n",
    "\n",
    "        self.current_position = 0\n",
    "        self.num_iterations = len(self.tokens) // (B * T)\n",
    "\n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "        current_batch = self.tokens[\n",
    "            self.current_position : self.current_position + (B * T) + 1\n",
    "        ]  # +1 for the target\n",
    "        x = current_batch[:-1].view(B, T)  # input tensor of shape (B, T)\n",
    "        y = current_batch[1:].view(B, T)  # target tensor of shape (B, T)\n",
    "\n",
    "        # updating the position for next batch\n",
    "        self.current_position += B * T\n",
    "\n",
    "        # we reset the position if the next batch is OOB\n",
    "        if self.current_position + (B * T) + 1 > len(self.tokens):\n",
    "            self.current_position = 0\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens in the file: 338025\n",
      "1 epoch will have 41 iterations\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataloaderLiter(B=8, T=1024, encoder=encoder, filename=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total iterations: 82\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 2\n",
    "initial_iterations = 100\n",
    "total_iteration = num_epochs * train_loader.num_iterations\n",
    "print(f\"Total iterations: {total_iteration}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): GELU(approximate='tanh')\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_model = GPT(GPTConfig())\n",
    "transformer_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(transformer_model.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for iter: 0, loss: 11.010125160217285, time: 1204.90ms, tokens/mssec: 6.80\n",
      "for iter: 1, loss: 9.599913597106934, time: 219.55ms, tokens/mssec: 37.31\n",
      "for iter: 2, loss: 8.829021453857422, time: 249.11ms, tokens/mssec: 32.88\n",
      "for iter: 3, loss: 8.613097190856934, time: 256.11ms, tokens/mssec: 31.99\n",
      "for iter: 4, loss: 8.430353164672852, time: 253.04ms, tokens/mssec: 32.37\n",
      "for iter: 5, loss: 8.372970581054688, time: 234.27ms, tokens/mssec: 34.97\n",
      "for iter: 6, loss: 8.321300506591797, time: 249.31ms, tokens/mssec: 32.86\n",
      "for iter: 7, loss: 7.983493804931641, time: 253.12ms, tokens/mssec: 32.36\n",
      "for iter: 8, loss: 7.712906837463379, time: 250.26ms, tokens/mssec: 32.73\n",
      "for iter: 9, loss: 7.669942378997803, time: 245.65ms, tokens/mssec: 33.35\n",
      "for iter: 10, loss: 7.633736610412598, time: 257.81ms, tokens/mssec: 31.77\n",
      "for iter: 11, loss: 7.45471715927124, time: 246.34ms, tokens/mssec: 33.25\n",
      "for iter: 12, loss: 7.39841890335083, time: 251.71ms, tokens/mssec: 32.54\n",
      "for iter: 13, loss: 7.170974254608154, time: 251.09ms, tokens/mssec: 32.63\n",
      "for iter: 14, loss: 7.07072639465332, time: 245.31ms, tokens/mssec: 33.39\n",
      "for iter: 15, loss: 6.871374607086182, time: 253.65ms, tokens/mssec: 32.30\n",
      "for iter: 16, loss: 6.743826866149902, time: 253.09ms, tokens/mssec: 32.37\n",
      "for iter: 17, loss: 6.7445549964904785, time: 250.86ms, tokens/mssec: 32.66\n",
      "for iter: 18, loss: 6.6323723793029785, time: 250.42ms, tokens/mssec: 32.71\n",
      "for iter: 19, loss: 6.477555751800537, time: 225.78ms, tokens/mssec: 36.28\n",
      "for iter: 20, loss: 6.512205123901367, time: 253.73ms, tokens/mssec: 32.29\n",
      "for iter: 21, loss: 6.381094932556152, time: 256.45ms, tokens/mssec: 31.94\n",
      "for iter: 22, loss: 6.476471424102783, time: 258.75ms, tokens/mssec: 31.66\n",
      "for iter: 23, loss: 6.343664169311523, time: 254.78ms, tokens/mssec: 32.15\n",
      "for iter: 24, loss: 6.3337931632995605, time: 237.76ms, tokens/mssec: 34.45\n",
      "for iter: 25, loss: 6.424341678619385, time: 239.92ms, tokens/mssec: 34.14\n",
      "for iter: 26, loss: 6.601984977722168, time: 249.53ms, tokens/mssec: 32.83\n",
      "for iter: 27, loss: 6.512826919555664, time: 248.75ms, tokens/mssec: 32.93\n",
      "for iter: 28, loss: 6.739025115966797, time: 215.80ms, tokens/mssec: 37.96\n",
      "for iter: 29, loss: 6.515888214111328, time: 258.03ms, tokens/mssec: 31.75\n",
      "for iter: 30, loss: 6.522669792175293, time: 249.23ms, tokens/mssec: 32.87\n",
      "for iter: 31, loss: 6.53373908996582, time: 254.42ms, tokens/mssec: 32.20\n",
      "for iter: 32, loss: 6.397052764892578, time: 202.64ms, tokens/mssec: 40.43\n",
      "for iter: 33, loss: 6.716008186340332, time: 256.67ms, tokens/mssec: 31.92\n",
      "for iter: 34, loss: 6.581930160522461, time: 204.57ms, tokens/mssec: 40.05\n",
      "for iter: 35, loss: 6.4634108543396, time: 254.92ms, tokens/mssec: 32.14\n",
      "for iter: 36, loss: 6.50404691696167, time: 254.49ms, tokens/mssec: 32.19\n",
      "for iter: 37, loss: 6.506907939910889, time: 248.10ms, tokens/mssec: 33.02\n",
      "for iter: 38, loss: 6.277915954589844, time: 259.07ms, tokens/mssec: 31.62\n",
      "for iter: 39, loss: 6.310752868652344, time: 251.73ms, tokens/mssec: 32.54\n",
      "for iter: 40, loss: 6.5713725090026855, time: 253.80ms, tokens/mssec: 32.28\n",
      "for iter: 41, loss: 6.314132213592529, time: 262.38ms, tokens/mssec: 31.22\n",
      "for iter: 42, loss: 6.383302688598633, time: 215.42ms, tokens/mssec: 38.03\n",
      "for iter: 43, loss: 6.081002712249756, time: 261.82ms, tokens/mssec: 31.29\n",
      "for iter: 44, loss: 6.016303062438965, time: 237.26ms, tokens/mssec: 34.53\n",
      "for iter: 45, loss: 6.099287986755371, time: 252.53ms, tokens/mssec: 32.44\n",
      "for iter: 46, loss: 6.235737323760986, time: 210.28ms, tokens/mssec: 38.96\n",
      "for iter: 47, loss: 6.198183059692383, time: 198.91ms, tokens/mssec: 41.19\n",
      "for iter: 48, loss: 6.094192981719971, time: 258.82ms, tokens/mssec: 31.65\n",
      "for iter: 49, loss: 5.942729473114014, time: 203.90ms, tokens/mssec: 40.18\n",
      "for iter: 50, loss: 6.14497709274292, time: 205.42ms, tokens/mssec: 39.88\n",
      "for iter: 51, loss: 6.213353633880615, time: 252.21ms, tokens/mssec: 32.48\n",
      "for iter: 52, loss: 6.318689346313477, time: 207.60ms, tokens/mssec: 39.46\n",
      "for iter: 53, loss: 6.411952972412109, time: 212.31ms, tokens/mssec: 38.59\n",
      "for iter: 54, loss: 6.29691743850708, time: 260.23ms, tokens/mssec: 31.48\n",
      "for iter: 55, loss: 6.289147853851318, time: 217.32ms, tokens/mssec: 37.70\n",
      "for iter: 56, loss: 6.111241340637207, time: 253.28ms, tokens/mssec: 32.34\n",
      "for iter: 57, loss: 6.116686820983887, time: 254.78ms, tokens/mssec: 32.15\n",
      "for iter: 58, loss: 6.207864284515381, time: 198.40ms, tokens/mssec: 41.29\n",
      "for iter: 59, loss: 6.143858432769775, time: 201.23ms, tokens/mssec: 40.71\n",
      "for iter: 60, loss: 6.022165775299072, time: 255.10ms, tokens/mssec: 32.11\n",
      "for iter: 61, loss: 6.125793933868408, time: 196.85ms, tokens/mssec: 41.62\n",
      "for iter: 62, loss: 5.973659515380859, time: 195.84ms, tokens/mssec: 41.83\n",
      "for iter: 63, loss: 6.149219036102295, time: 250.77ms, tokens/mssec: 32.67\n",
      "for iter: 64, loss: 5.958234786987305, time: 205.16ms, tokens/mssec: 39.93\n",
      "for iter: 65, loss: 5.949652194976807, time: 204.15ms, tokens/mssec: 40.13\n",
      "for iter: 66, loss: 6.033618450164795, time: 253.37ms, tokens/mssec: 32.33\n",
      "for iter: 67, loss: 6.253592491149902, time: 208.32ms, tokens/mssec: 39.32\n",
      "for iter: 68, loss: 6.157671928405762, time: 201.92ms, tokens/mssec: 40.57\n",
      "for iter: 69, loss: 6.418794631958008, time: 254.17ms, tokens/mssec: 32.23\n",
      "for iter: 70, loss: 6.1715407371521, time: 201.78ms, tokens/mssec: 40.60\n",
      "for iter: 71, loss: 6.169620513916016, time: 212.03ms, tokens/mssec: 38.64\n",
      "for iter: 72, loss: 6.10941743850708, time: 252.30ms, tokens/mssec: 32.47\n",
      "for iter: 73, loss: 6.00929069519043, time: 206.92ms, tokens/mssec: 39.59\n",
      "for iter: 74, loss: 6.23443603515625, time: 206.19ms, tokens/mssec: 39.73\n",
      "for iter: 75, loss: 6.0996198654174805, time: 260.98ms, tokens/mssec: 31.39\n",
      "for iter: 76, loss: 6.066870212554932, time: 205.49ms, tokens/mssec: 39.87\n",
      "for iter: 77, loss: 6.054555416107178, time: 227.76ms, tokens/mssec: 35.97\n",
      "for iter: 78, loss: 6.076549053192139, time: 257.26ms, tokens/mssec: 31.84\n",
      "for iter: 79, loss: 5.888930797576904, time: 254.51ms, tokens/mssec: 32.19\n",
      "for iter: 80, loss: 5.874754905700684, time: 204.93ms, tokens/mssec: 39.97\n",
      "for iter: 81, loss: 6.24181604385376, time: 253.36ms, tokens/mssec: 32.33\n",
      "for iter: 82, loss: 6.097992420196533, time: 209.83ms, tokens/mssec: 39.04\n",
      "for iter: 83, loss: 6.178536415100098, time: 209.51ms, tokens/mssec: 39.10\n",
      "for iter: 84, loss: 5.919396877288818, time: 252.22ms, tokens/mssec: 32.48\n",
      "for iter: 85, loss: 5.861344337463379, time: 203.72ms, tokens/mssec: 40.21\n",
      "for iter: 86, loss: 5.927563190460205, time: 201.48ms, tokens/mssec: 40.66\n",
      "for iter: 87, loss: 6.040862083435059, time: 253.69ms, tokens/mssec: 32.29\n",
      "for iter: 88, loss: 5.969811916351318, time: 203.90ms, tokens/mssec: 40.18\n",
      "for iter: 89, loss: 5.864760875701904, time: 206.55ms, tokens/mssec: 39.66\n",
      "for iter: 90, loss: 5.7286272048950195, time: 258.11ms, tokens/mssec: 31.74\n",
      "for iter: 91, loss: 5.949522972106934, time: 252.48ms, tokens/mssec: 32.45\n",
      "for iter: 92, loss: 6.0199294090271, time: 248.98ms, tokens/mssec: 32.90\n",
      "for iter: 93, loss: 6.147654056549072, time: 254.05ms, tokens/mssec: 32.25\n",
      "for iter: 94, loss: 6.231391429901123, time: 203.35ms, tokens/mssec: 40.29\n",
      "for iter: 95, loss: 6.125607013702393, time: 201.24ms, tokens/mssec: 40.71\n",
      "for iter: 96, loss: 6.136750221252441, time: 256.70ms, tokens/mssec: 31.91\n",
      "for iter: 97, loss: 5.959352016448975, time: 204.27ms, tokens/mssec: 40.10\n",
      "for iter: 98, loss: 5.9357099533081055, time: 206.91ms, tokens/mssec: 39.59\n",
      "for iter: 99, loss: 6.030149459838867, time: 264.47ms, tokens/mssec: 30.97\n"
     ]
    }
   ],
   "source": [
    "transformer_model.train()\n",
    "for iter in range(initial_iterations):\n",
    "    start_time = time.time()\n",
    "    x, y = train_loader.next_batch()\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    logits, loss = transformer_model(x, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    end_time = time.time()\n",
    "    torch.cuda.empty_cache()  # clear the cache\n",
    "    torch.cuda.synchronize()  # wait for the computation to be done\n",
    "    time_elapsed = (end_time - start_time) * 1000\n",
    "    token_persec = x.numel() / time_elapsed\n",
    "    print(\n",
    "        f\"for iter: {iter}, loss: {loss.item()}, time: {time_elapsed:.2f}ms, tokens/mssec: {token_persec:.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision settings - Application of tf float32 to increase throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "torch.backends.cudnn.benchmark = True # set to true for faster training\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): GELU(approximate='tanh')\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_model = GPT(GPTConfig())\n",
    "transformer_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(transformer_model.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for iter: 0, loss: 10.890664100646973, time: 42.46ms, tokens/mssec: 192.94\n",
      "for iter: 1, loss: 9.615824699401855, time: 240.92ms, tokens/mssec: 34.00\n",
      "for iter: 2, loss: 9.006220817565918, time: 268.63ms, tokens/mssec: 30.50\n",
      "for iter: 3, loss: 8.814001083374023, time: 273.68ms, tokens/mssec: 29.93\n",
      "for iter: 4, loss: 8.714908599853516, time: 271.72ms, tokens/mssec: 30.15\n",
      "for iter: 5, loss: 8.374526977539062, time: 274.42ms, tokens/mssec: 29.85\n",
      "for iter: 6, loss: 8.152328491210938, time: 278.57ms, tokens/mssec: 29.41\n",
      "for iter: 7, loss: 7.994481563568115, time: 266.88ms, tokens/mssec: 30.70\n",
      "for iter: 8, loss: 7.850434303283691, time: 257.98ms, tokens/mssec: 31.75\n",
      "for iter: 9, loss: 7.57218074798584, time: 270.49ms, tokens/mssec: 30.29\n",
      "for iter: 10, loss: 7.56642484664917, time: 272.42ms, tokens/mssec: 30.07\n",
      "for iter: 11, loss: 7.270780086517334, time: 261.13ms, tokens/mssec: 31.37\n",
      "for iter: 12, loss: 7.126434803009033, time: 281.27ms, tokens/mssec: 29.13\n",
      "for iter: 13, loss: 6.978614807128906, time: 271.54ms, tokens/mssec: 30.17\n",
      "for iter: 14, loss: 6.790109634399414, time: 267.77ms, tokens/mssec: 30.59\n",
      "for iter: 15, loss: 6.916836261749268, time: 269.86ms, tokens/mssec: 30.36\n",
      "for iter: 16, loss: 6.751651287078857, time: 270.30ms, tokens/mssec: 30.31\n",
      "for iter: 17, loss: 6.6469831466674805, time: 276.06ms, tokens/mssec: 29.68\n",
      "for iter: 18, loss: 6.65116024017334, time: 270.55ms, tokens/mssec: 30.28\n",
      "for iter: 19, loss: 6.61794900894165, time: 271.57ms, tokens/mssec: 30.17\n",
      "for iter: 20, loss: 6.393957138061523, time: 274.93ms, tokens/mssec: 29.80\n",
      "for iter: 21, loss: 6.386856555938721, time: 272.56ms, tokens/mssec: 30.06\n",
      "for iter: 22, loss: 6.661072731018066, time: 277.56ms, tokens/mssec: 29.51\n",
      "for iter: 23, loss: 6.589475631713867, time: 266.13ms, tokens/mssec: 30.78\n",
      "for iter: 24, loss: 6.641298294067383, time: 271.09ms, tokens/mssec: 30.22\n",
      "for iter: 25, loss: 6.406744480133057, time: 275.25ms, tokens/mssec: 29.76\n",
      "for iter: 26, loss: 6.328742504119873, time: 273.59ms, tokens/mssec: 29.94\n",
      "for iter: 27, loss: 6.3797383308410645, time: 276.86ms, tokens/mssec: 29.59\n",
      "for iter: 28, loss: 6.476198673248291, time: 273.42ms, tokens/mssec: 29.96\n",
      "for iter: 29, loss: 6.452919006347656, time: 266.05ms, tokens/mssec: 30.79\n",
      "for iter: 30, loss: 6.337620735168457, time: 262.76ms, tokens/mssec: 31.18\n",
      "for iter: 31, loss: 6.219374656677246, time: 270.15ms, tokens/mssec: 30.32\n",
      "for iter: 32, loss: 6.451269626617432, time: 268.30ms, tokens/mssec: 30.53\n",
      "for iter: 33, loss: 6.536831378936768, time: 271.74ms, tokens/mssec: 30.15\n",
      "for iter: 34, loss: 6.6028900146484375, time: 273.02ms, tokens/mssec: 30.00\n",
      "for iter: 35, loss: 6.6975250244140625, time: 272.81ms, tokens/mssec: 30.03\n",
      "for iter: 36, loss: 6.554645538330078, time: 271.79ms, tokens/mssec: 30.14\n",
      "for iter: 37, loss: 6.55347204208374, time: 273.56ms, tokens/mssec: 29.95\n",
      "for iter: 38, loss: 6.384043216705322, time: 275.59ms, tokens/mssec: 29.72\n",
      "for iter: 39, loss: 6.280202388763428, time: 275.65ms, tokens/mssec: 29.72\n",
      "for iter: 40, loss: 6.316856384277344, time: 275.08ms, tokens/mssec: 29.78\n",
      "for iter: 41, loss: 6.187473297119141, time: 252.72ms, tokens/mssec: 32.41\n",
      "for iter: 42, loss: 6.067376613616943, time: 271.89ms, tokens/mssec: 30.13\n",
      "for iter: 43, loss: 6.176492214202881, time: 278.78ms, tokens/mssec: 29.39\n",
      "for iter: 44, loss: 6.063877105712891, time: 262.66ms, tokens/mssec: 31.19\n",
      "for iter: 45, loss: 6.248235702514648, time: 256.24ms, tokens/mssec: 31.97\n",
      "for iter: 46, loss: 6.0544753074646, time: 269.93ms, tokens/mssec: 30.35\n",
      "for iter: 47, loss: 6.041767597198486, time: 269.13ms, tokens/mssec: 30.44\n",
      "for iter: 48, loss: 6.128332614898682, time: 262.87ms, tokens/mssec: 31.16\n",
      "for iter: 49, loss: 6.285793781280518, time: 278.39ms, tokens/mssec: 29.43\n",
      "for iter: 50, loss: 6.175589084625244, time: 270.43ms, tokens/mssec: 30.29\n",
      "for iter: 51, loss: 6.447669982910156, time: 267.67ms, tokens/mssec: 30.61\n",
      "for iter: 52, loss: 6.183827877044678, time: 279.40ms, tokens/mssec: 29.32\n",
      "for iter: 53, loss: 6.175693035125732, time: 278.98ms, tokens/mssec: 29.36\n",
      "for iter: 54, loss: 6.106761932373047, time: 281.24ms, tokens/mssec: 29.13\n",
      "for iter: 55, loss: 5.984302520751953, time: 283.58ms, tokens/mssec: 28.89\n",
      "for iter: 56, loss: 6.215015888214111, time: 271.79ms, tokens/mssec: 30.14\n",
      "for iter: 57, loss: 6.090268611907959, time: 278.85ms, tokens/mssec: 29.38\n",
      "for iter: 58, loss: 6.069613456726074, time: 274.32ms, tokens/mssec: 29.86\n",
      "for iter: 59, loss: 6.0697174072265625, time: 273.08ms, tokens/mssec: 30.00\n",
      "for iter: 60, loss: 6.093522071838379, time: 275.63ms, tokens/mssec: 29.72\n",
      "for iter: 61, loss: 5.899548530578613, time: 281.92ms, tokens/mssec: 29.06\n",
      "for iter: 62, loss: 5.8802008628845215, time: 273.59ms, tokens/mssec: 29.94\n",
      "for iter: 63, loss: 6.244692325592041, time: 278.50ms, tokens/mssec: 29.41\n",
      "for iter: 64, loss: 6.201231479644775, time: 276.40ms, tokens/mssec: 29.64\n",
      "for iter: 65, loss: 6.278341770172119, time: 286.79ms, tokens/mssec: 28.56\n",
      "for iter: 66, loss: 6.015580177307129, time: 274.82ms, tokens/mssec: 29.81\n",
      "for iter: 67, loss: 5.9800705909729, time: 273.22ms, tokens/mssec: 29.98\n",
      "for iter: 68, loss: 6.017223834991455, time: 273.63ms, tokens/mssec: 29.94\n",
      "for iter: 69, loss: 6.118692398071289, time: 277.27ms, tokens/mssec: 29.55\n",
      "for iter: 70, loss: 6.066275119781494, time: 274.85ms, tokens/mssec: 29.80\n",
      "for iter: 71, loss: 5.954668045043945, time: 282.63ms, tokens/mssec: 28.98\n",
      "for iter: 72, loss: 5.80509090423584, time: 275.32ms, tokens/mssec: 29.75\n",
      "for iter: 73, loss: 6.030314922332764, time: 270.97ms, tokens/mssec: 30.23\n",
      "for iter: 74, loss: 6.094367980957031, time: 253.19ms, tokens/mssec: 32.35\n",
      "for iter: 75, loss: 6.218217372894287, time: 276.43ms, tokens/mssec: 29.63\n",
      "for iter: 76, loss: 6.291283130645752, time: 278.57ms, tokens/mssec: 29.41\n",
      "for iter: 77, loss: 6.190968036651611, time: 271.92ms, tokens/mssec: 30.13\n",
      "for iter: 78, loss: 6.194146633148193, time: 252.34ms, tokens/mssec: 32.46\n",
      "for iter: 79, loss: 6.0057783126831055, time: 270.60ms, tokens/mssec: 30.27\n",
      "for iter: 80, loss: 5.9829230308532715, time: 275.05ms, tokens/mssec: 29.78\n",
      "for iter: 81, loss: 6.05584716796875, time: 275.51ms, tokens/mssec: 29.73\n",
      "for iter: 82, loss: 5.970654010772705, time: 249.83ms, tokens/mssec: 32.79\n",
      "for iter: 83, loss: 5.843723773956299, time: 275.95ms, tokens/mssec: 29.69\n",
      "for iter: 84, loss: 5.963657855987549, time: 274.81ms, tokens/mssec: 29.81\n",
      "for iter: 85, loss: 5.862631320953369, time: 270.30ms, tokens/mssec: 30.31\n",
      "for iter: 86, loss: 6.04770565032959, time: 292.36ms, tokens/mssec: 28.02\n",
      "for iter: 87, loss: 5.829980850219727, time: 272.82ms, tokens/mssec: 30.03\n",
      "for iter: 88, loss: 5.821634292602539, time: 280.07ms, tokens/mssec: 29.25\n",
      "for iter: 89, loss: 5.930551052093506, time: 284.49ms, tokens/mssec: 28.80\n",
      "for iter: 90, loss: 6.077146053314209, time: 280.21ms, tokens/mssec: 29.23\n",
      "for iter: 91, loss: 5.985466480255127, time: 271.16ms, tokens/mssec: 30.21\n",
      "for iter: 92, loss: 6.267089366912842, time: 269.36ms, tokens/mssec: 30.41\n",
      "for iter: 93, loss: 5.991279602050781, time: 278.41ms, tokens/mssec: 29.42\n",
      "for iter: 94, loss: 6.003833293914795, time: 274.78ms, tokens/mssec: 29.81\n",
      "for iter: 95, loss: 5.904575347900391, time: 285.70ms, tokens/mssec: 28.67\n",
      "for iter: 96, loss: 5.816195487976074, time: 270.94ms, tokens/mssec: 30.24\n",
      "for iter: 97, loss: 6.041373252868652, time: 274.10ms, tokens/mssec: 29.89\n",
      "for iter: 98, loss: 5.910916328430176, time: 297.28ms, tokens/mssec: 27.56\n",
      "for iter: 99, loss: 5.906538486480713, time: 272.13ms, tokens/mssec: 30.10\n"
     ]
    }
   ],
   "source": [
    "transformer_model.train()\n",
    "for iter in range(initial_iterations):\n",
    "    start_time = time.time()\n",
    "    x, y = train_loader.next_batch()\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    logits, loss = transformer_model(x, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    end_time = time.time()\n",
    "    torch.cuda.empty_cache()  # clear the cache\n",
    "    torch.cuda.synchronize()  # wait for the computation to be done\n",
    "    time_elapsed = (end_time - start_time) * 1000\n",
    "    token_persec = x.numel() / time_elapsed\n",
    "    print(\n",
    "        f\"for iter: {iter}, loss: {loss.item()}, time: {time_elapsed:.2f}ms, tokens/mssec: {token_persec:.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
