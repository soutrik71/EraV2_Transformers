{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will try figuring out all the RAW possible ways to train and optimize training a decode model as taught by Andrej Karpathy\n",
    "Key Steps Explained:\n",
    "- Basic Settings: Set the learning rate, device, random seeds, and batch sizes.\n",
    "- Learning Rate Schedule Function: Define a function to adjust the learning rate using warmup and cosine decay.\n",
    "- Optimizer Configuration: Create an optimizer and parameter groups for weight decay.\n",
    "- Data Loading: Read and tokenize data from a file, and set up a data loader.\n",
    "- Mixed Precision Training: Use autocast and GradScaler to speed up training and reduce memory usage.\n",
    "- Model Initialization: Initialize the GPT model and compile it if supported.\n",
    "- Training Loop: Iterate over the data, accumulate gradients, adjust learning rates, and update weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "IPython.notebook.set_autosave_interval(300000)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 300 seconds\n"
     ]
    }
   ],
   "source": [
    "%autosave 300\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/batch/tasks/shared/LS_root/mounts/clusters/insights-model-run2/code/Users/soutrik.chowdhury/EraV2_Transformers\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\n",
    "    \"/mnt/batch/tasks/shared/LS_root/mounts/clusters/insights-model-run2/code/Users/soutrik.chowdhury/EraV2_Transformers\"\n",
    ")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "from notebooks.gpt2_models.dummy_model import GPT, GPTConfig\n",
    "from torch.nn import functional as F\n",
    "import time\n",
    "from contextlib import nullcontext\n",
    "import inspect\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Init using Pretrained weights for GPT-2 from HF\n",
    "Initiate the model with pretrained weights from HF for Gpt2 and try creating 5 predicted sequences using same instruction prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_return_sequences = 5  # Number of sentences to generate\n",
    "max_length = 30  # Maximum length of the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights from pretrained gpt: gpt2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT(\n",
      "  (transformer): ModuleDict(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): CausalSelfAttention(\n",
      "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (gelu): GELU(approximate='tanh')\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "pretrained_model = GPT.from_pretrained(\"gpt2\")\n",
    "print(pretrained_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do we have - First and embedding layer with vocab size of 50257 and embedding size of 768. Post which we have pos embedding of 1024 block size .Then we have 12 transformer decoder layers with 12 heads each. Each decoder layer is build up with LayerNorm -> MultiHeadAttention -> LayerNorm -> FeedForward -> LayerNorm. The output of the last decoder layer is passed through a linear layer to get the logits for the next token prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic settings\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "device_type = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): GELU(approximate='tanh')\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_model.to(device)\n",
    "pretrained_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14])\n"
     ]
    }
   ],
   "source": [
    "dummy_str = \"Hello I am a language Model, I am here to help you.\"\n",
    "encoder = tiktoken.get_encoding(encoding_name=\"gpt2\")\n",
    "tokens = encoder.encode(dummy_str)\n",
    "tokens_vec = torch.tensor(tokens, dtype=torch.long)\n",
    "print(tokens_vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 14])\n"
     ]
    }
   ],
   "source": [
    "# creating a batch of repeated tokens\n",
    "tokens_batch = tokens_vec.unsqueeze(0).repeat(num_return_sequences, 1)\n",
    "tokens_batch = tokens_batch.to(device)\n",
    "print(tokens_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_prediction(xgen, model):\n",
    "\n",
    "    xgen = xgen.clone()\n",
    "\n",
    "    while xgen.size(1) < max_length:\n",
    "\n",
    "        # forward the model to get the logits\n",
    "        with torch.no_grad():\n",
    "            logits, loss = model(xgen)  # (B, T, vocab_size)\n",
    "            # take the logits at the last position\n",
    "            logits = logits[:, -1, :]  # (B, vocab_size)\n",
    "            # get the probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # do top-k sampling of 50 (huggingface pipeline default)\n",
    "            # topk_probs here becomes (5, 50), topk_indices is (5, 50)\n",
    "            topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
    "            # select a token from the top-k probabilities\n",
    "            # note: multinomial does not demand the input to sum to 1\n",
    "            ix = torch.multinomial(topk_probs, 1)  # (B, 1)\n",
    "            # gather the corresponding indices\n",
    "            xcol = torch.gather(topk_indices, -1, ix)  # (B, 1)\n",
    "            # append to the sequence\n",
    "            xgen = torch.cat((xgen, xcol), dim=1)\n",
    "\n",
    "    return xgen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_batch = batch_prediction(tokens_batch, pretrained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 30])\n"
     ]
    }
   ],
   "source": [
    "print(prediction_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>Hello I am a language Model, I am here to help you. I am very happy with where this world would be if you could give me something\n",
      "\n",
      "\n",
      ">>Hello I am a language Model, I am here to help you.\n",
      "\n",
      "Thank you,\n",
      "\n",
      "Emma<|endoftext|>Liz Peltz:\n",
      "\n",
      "\n",
      ">>Hello I am a language Model, I am here to help you.\n",
      "\n",
      "But as you say, my job now is to get things done.\n",
      "\n",
      "\n",
      ">>Hello I am a language Model, I am here to help you.\n",
      "\n",
      "I am a language model, I am here to help you. You\n",
      "\n",
      "\n",
      ">>Hello I am a language Model, I am here to help you. If you have no idea what a data model is:\n",
      "\n",
      "So. what\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(prediction_batch.size(0)):\n",
    "    x = prediction_batch[i, :max_length].tolist()\n",
    "    decoded_str = encoder.decode(x)\n",
    "    print(f\">>{decoded_str}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Model Training of the model with toy data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets define the data loader for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = tiktoken.get_encoding(encoding_name=\"gpt2\")\n",
    "filename = \"./data/tinyshakespeare/input.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tokens(filename, encoder):\n",
    "    with open(filename, \"r\") as file:\n",
    "        lines = file.read()\n",
    "\n",
    "    tokens = encoder.encode(lines)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataloaderLiter:\n",
    "    def __init__(self, B, T, encoder, filename):\n",
    "        self.B = B  # batch size\n",
    "        self.T = T  # seq length\n",
    "        tokens = read_tokens(filename, encoder)\n",
    "        self.tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "\n",
    "        print(f\"Total tokens in the file: {len(self.tokens)}\")\n",
    "        print(\n",
    "            f\"1 epoch will have {len(self.tokens) // (B * T)} iterations given Batch size={B} and Context length={T}\"\n",
    "        )\n",
    "\n",
    "        self.current_position = 0\n",
    "        self.num_iterations = len(self.tokens) // (B * T)\n",
    "\n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "        current_batch = self.tokens[\n",
    "            self.current_position : self.current_position + (B * T) + 1\n",
    "        ]  # +1 for the target\n",
    "        x = current_batch[:-1].view(B, T)  # input tensor of shape (B, T)\n",
    "        y = current_batch[1:].view(B, T)  # target tensor of shape (B, T)\n",
    "\n",
    "        # updating the position for next batch\n",
    "        self.current_position += B * T\n",
    "\n",
    "        # we reset the position if the next batch is OOB\n",
    "        if self.current_position + (B * T) + 1 > len(self.tokens):\n",
    "            self.current_position = 0\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First full training loop with the dummy model and the tinyshakespeare dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens in the file: 338025\n",
      "1 epoch will have 41 iterations given Batch size=8 and Context length=1024\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataloaderLiter(B=8, T=1024, encoder=encoder, filename=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total iterations: 82\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 2\n",
    "initial_iterations = 100\n",
    "total_iteration = num_epochs * train_loader.num_iterations\n",
    "print(f\"Total iterations: {total_iteration}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): GELU(approximate='tanh')\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_model = GPT(GPTConfig())\n",
    "transformer_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(transformer_model.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for iter: 0, loss: 11.010125160217285, time: 801.24ms, tokens/mssec: 10.22\n",
      "for iter: 1, loss: 9.59991455078125, time: 183.30ms, tokens/mssec: 44.69\n",
      "for iter: 2, loss: 8.829021453857422, time: 253.47ms, tokens/mssec: 32.32\n",
      "for iter: 3, loss: 8.613096237182617, time: 256.11ms, tokens/mssec: 31.99\n",
      "for iter: 4, loss: 8.430353164672852, time: 246.82ms, tokens/mssec: 33.19\n",
      "for iter: 5, loss: 8.372970581054688, time: 253.79ms, tokens/mssec: 32.28\n",
      "for iter: 6, loss: 8.321300506591797, time: 248.37ms, tokens/mssec: 32.98\n",
      "for iter: 7, loss: 7.983494281768799, time: 257.68ms, tokens/mssec: 31.79\n",
      "for iter: 8, loss: 7.7129058837890625, time: 218.96ms, tokens/mssec: 37.41\n",
      "for iter: 9, loss: 7.6699419021606445, time: 207.05ms, tokens/mssec: 39.57\n",
      "for iter: 10, loss: 7.6337361335754395, time: 248.13ms, tokens/mssec: 33.02\n",
      "for iter: 11, loss: 7.454716682434082, time: 210.50ms, tokens/mssec: 38.92\n",
      "for iter: 12, loss: 7.3984174728393555, time: 244.49ms, tokens/mssec: 33.51\n",
      "for iter: 13, loss: 7.17097282409668, time: 237.23ms, tokens/mssec: 34.53\n",
      "for iter: 14, loss: 7.070725440979004, time: 259.85ms, tokens/mssec: 31.53\n",
      "for iter: 15, loss: 6.8713765144348145, time: 258.05ms, tokens/mssec: 31.75\n",
      "for iter: 16, loss: 6.743827819824219, time: 206.35ms, tokens/mssec: 39.70\n",
      "for iter: 17, loss: 6.744556427001953, time: 251.31ms, tokens/mssec: 32.60\n",
      "for iter: 18, loss: 6.632373332977295, time: 200.35ms, tokens/mssec: 40.89\n",
      "for iter: 19, loss: 6.477558612823486, time: 199.01ms, tokens/mssec: 41.16\n",
      "for iter: 20, loss: 6.512205600738525, time: 260.41ms, tokens/mssec: 31.46\n",
      "for iter: 21, loss: 6.3810954093933105, time: 203.68ms, tokens/mssec: 40.22\n",
      "for iter: 22, loss: 6.476472854614258, time: 209.62ms, tokens/mssec: 39.08\n",
      "for iter: 23, loss: 6.343663215637207, time: 258.23ms, tokens/mssec: 31.72\n",
      "for iter: 24, loss: 6.333795070648193, time: 203.46ms, tokens/mssec: 40.26\n",
      "for iter: 25, loss: 6.424341201782227, time: 204.84ms, tokens/mssec: 39.99\n",
      "for iter: 26, loss: 6.601986885070801, time: 242.57ms, tokens/mssec: 33.77\n",
      "for iter: 27, loss: 6.512825965881348, time: 250.49ms, tokens/mssec: 32.70\n",
      "for iter: 28, loss: 6.73902702331543, time: 252.86ms, tokens/mssec: 32.40\n",
      "for iter: 29, loss: 6.515888690948486, time: 253.04ms, tokens/mssec: 32.37\n",
      "for iter: 30, loss: 6.522668838500977, time: 210.62ms, tokens/mssec: 38.89\n",
      "for iter: 31, loss: 6.5337395668029785, time: 198.88ms, tokens/mssec: 41.19\n",
      "for iter: 32, loss: 6.397054672241211, time: 250.05ms, tokens/mssec: 32.76\n",
      "for iter: 33, loss: 6.716010093688965, time: 202.41ms, tokens/mssec: 40.47\n",
      "for iter: 34, loss: 6.581932544708252, time: 212.49ms, tokens/mssec: 38.55\n",
      "for iter: 35, loss: 6.463414192199707, time: 251.47ms, tokens/mssec: 32.58\n",
      "for iter: 36, loss: 6.504049777984619, time: 208.15ms, tokens/mssec: 39.36\n",
      "for iter: 37, loss: 6.506911277770996, time: 203.53ms, tokens/mssec: 40.25\n",
      "for iter: 38, loss: 6.277918338775635, time: 254.56ms, tokens/mssec: 32.18\n",
      "for iter: 39, loss: 6.310758590698242, time: 207.75ms, tokens/mssec: 39.43\n",
      "for iter: 40, loss: 6.571374416351318, time: 199.36ms, tokens/mssec: 41.09\n",
      "for iter: 41, loss: 6.314127445220947, time: 251.61ms, tokens/mssec: 32.56\n",
      "for iter: 42, loss: 6.383302688598633, time: 203.15ms, tokens/mssec: 40.32\n",
      "for iter: 43, loss: 6.0810065269470215, time: 202.56ms, tokens/mssec: 40.44\n",
      "for iter: 44, loss: 6.016306400299072, time: 258.94ms, tokens/mssec: 31.64\n",
      "for iter: 45, loss: 6.099288463592529, time: 200.29ms, tokens/mssec: 40.90\n",
      "for iter: 46, loss: 6.2357354164123535, time: 199.35ms, tokens/mssec: 41.09\n",
      "for iter: 47, loss: 6.198180198669434, time: 251.24ms, tokens/mssec: 32.61\n",
      "for iter: 48, loss: 6.094194412231445, time: 193.44ms, tokens/mssec: 42.35\n",
      "for iter: 49, loss: 5.942734241485596, time: 213.12ms, tokens/mssec: 38.44\n",
      "for iter: 50, loss: 6.144978046417236, time: 255.01ms, tokens/mssec: 32.12\n",
      "for iter: 51, loss: 6.213354110717773, time: 209.89ms, tokens/mssec: 39.03\n",
      "for iter: 52, loss: 6.318689346313477, time: 206.44ms, tokens/mssec: 39.68\n",
      "for iter: 53, loss: 6.411952972412109, time: 252.48ms, tokens/mssec: 32.45\n",
      "for iter: 54, loss: 6.296917915344238, time: 208.64ms, tokens/mssec: 39.26\n",
      "for iter: 55, loss: 6.289135456085205, time: 207.32ms, tokens/mssec: 39.51\n",
      "for iter: 56, loss: 6.111236572265625, time: 262.31ms, tokens/mssec: 31.23\n",
      "for iter: 57, loss: 6.116687774658203, time: 254.69ms, tokens/mssec: 32.16\n",
      "for iter: 58, loss: 6.20786714553833, time: 256.26ms, tokens/mssec: 31.97\n",
      "for iter: 59, loss: 6.14385986328125, time: 253.20ms, tokens/mssec: 32.35\n",
      "for iter: 60, loss: 6.022165298461914, time: 255.34ms, tokens/mssec: 32.08\n",
      "for iter: 61, loss: 6.125797748565674, time: 243.63ms, tokens/mssec: 33.62\n",
      "for iter: 62, loss: 5.973664283752441, time: 258.49ms, tokens/mssec: 31.69\n",
      "for iter: 63, loss: 6.149227142333984, time: 252.48ms, tokens/mssec: 32.45\n",
      "for iter: 64, loss: 5.958241939544678, time: 202.56ms, tokens/mssec: 40.44\n",
      "for iter: 65, loss: 5.949655532836914, time: 262.73ms, tokens/mssec: 31.18\n",
      "for iter: 66, loss: 6.033611297607422, time: 213.89ms, tokens/mssec: 38.30\n",
      "for iter: 67, loss: 6.253593921661377, time: 210.85ms, tokens/mssec: 38.85\n",
      "for iter: 68, loss: 6.157674312591553, time: 254.95ms, tokens/mssec: 32.13\n",
      "for iter: 69, loss: 6.418787479400635, time: 207.83ms, tokens/mssec: 39.42\n",
      "for iter: 70, loss: 6.171533584594727, time: 202.38ms, tokens/mssec: 40.48\n",
      "for iter: 71, loss: 6.169618129730225, time: 254.10ms, tokens/mssec: 32.24\n",
      "for iter: 72, loss: 6.109414100646973, time: 209.59ms, tokens/mssec: 39.09\n",
      "for iter: 73, loss: 6.009283065795898, time: 205.08ms, tokens/mssec: 39.94\n",
      "for iter: 74, loss: 6.234431743621826, time: 229.46ms, tokens/mssec: 35.70\n",
      "for iter: 75, loss: 6.099612712860107, time: 210.89ms, tokens/mssec: 38.85\n",
      "for iter: 76, loss: 6.066872596740723, time: 252.68ms, tokens/mssec: 32.42\n",
      "for iter: 77, loss: 6.054553985595703, time: 257.48ms, tokens/mssec: 31.82\n",
      "for iter: 78, loss: 6.0765380859375, time: 262.52ms, tokens/mssec: 31.21\n",
      "for iter: 79, loss: 5.888916492462158, time: 253.95ms, tokens/mssec: 32.26\n",
      "for iter: 80, loss: 5.874738693237305, time: 262.17ms, tokens/mssec: 31.25\n",
      "for iter: 81, loss: 6.241803169250488, time: 252.11ms, tokens/mssec: 32.49\n",
      "for iter: 82, loss: 6.097970008850098, time: 259.32ms, tokens/mssec: 31.59\n",
      "for iter: 83, loss: 6.178531646728516, time: 258.50ms, tokens/mssec: 31.69\n",
      "for iter: 84, loss: 5.919369220733643, time: 253.29ms, tokens/mssec: 32.34\n",
      "for iter: 85, loss: 5.861310958862305, time: 256.88ms, tokens/mssec: 31.89\n",
      "for iter: 86, loss: 5.927563667297363, time: 257.00ms, tokens/mssec: 31.88\n",
      "for iter: 87, loss: 6.040827751159668, time: 254.29ms, tokens/mssec: 32.22\n",
      "for iter: 88, loss: 5.969776630401611, time: 258.67ms, tokens/mssec: 31.67\n",
      "for iter: 89, loss: 5.864748001098633, time: 254.19ms, tokens/mssec: 32.23\n",
      "for iter: 90, loss: 5.728613376617432, time: 240.43ms, tokens/mssec: 34.07\n",
      "for iter: 91, loss: 5.949514389038086, time: 236.06ms, tokens/mssec: 34.70\n",
      "for iter: 92, loss: 6.019901752471924, time: 253.43ms, tokens/mssec: 32.32\n",
      "for iter: 93, loss: 6.147653102874756, time: 260.03ms, tokens/mssec: 31.50\n",
      "for iter: 94, loss: 6.231372833251953, time: 252.59ms, tokens/mssec: 32.43\n",
      "for iter: 95, loss: 6.125584125518799, time: 257.54ms, tokens/mssec: 31.81\n",
      "for iter: 96, loss: 6.136728763580322, time: 255.41ms, tokens/mssec: 32.07\n",
      "for iter: 97, loss: 5.959315299987793, time: 262.01ms, tokens/mssec: 31.27\n",
      "for iter: 98, loss: 5.93570613861084, time: 255.80ms, tokens/mssec: 32.03\n",
      "for iter: 99, loss: 6.030129909515381, time: 257.93ms, tokens/mssec: 31.76\n"
     ]
    }
   ],
   "source": [
    "transformer_model.train()\n",
    "for iter in range(initial_iterations):\n",
    "    start_time = time.time()\n",
    "    x, y = train_loader.next_batch()\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    logits, loss = transformer_model(x, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    end_time = time.time()\n",
    "    torch.cuda.empty_cache()  # clear the cache\n",
    "    torch.cuda.synchronize()  # wait for the computation to be done\n",
    "    time_elapsed = (end_time - start_time) * 1000\n",
    "    token_persec = x.numel() / time_elapsed\n",
    "    print(\n",
    "        f\"for iter: {iter}, loss: {loss.item()}, time: {time_elapsed:.2f}ms, tokens/mssec: {token_persec:.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Optimization and  Faster Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision settings - Application of tf float32 to increase throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cuda.matmul.allow_tf32 = True  # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True  # allow tf32 on cudnn\n",
    "torch.backends.cudnn.benchmark = True  # set to true for faster training\n",
    "torch.set_float32_matmul_precision(\"high\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): GELU(approximate='tanh')\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_model = GPT(GPTConfig())\n",
    "transformer_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(transformer_model.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for iter: 0, loss: 10.890664100646973, time: 42.46ms, tokens/mssec: 192.94\n",
      "for iter: 1, loss: 9.615824699401855, time: 240.92ms, tokens/mssec: 34.00\n",
      "for iter: 2, loss: 9.006220817565918, time: 268.63ms, tokens/mssec: 30.50\n",
      "for iter: 3, loss: 8.814001083374023, time: 273.68ms, tokens/mssec: 29.93\n",
      "for iter: 4, loss: 8.714908599853516, time: 271.72ms, tokens/mssec: 30.15\n",
      "for iter: 5, loss: 8.374526977539062, time: 274.42ms, tokens/mssec: 29.85\n",
      "for iter: 6, loss: 8.152328491210938, time: 278.57ms, tokens/mssec: 29.41\n",
      "for iter: 7, loss: 7.994481563568115, time: 266.88ms, tokens/mssec: 30.70\n",
      "for iter: 8, loss: 7.850434303283691, time: 257.98ms, tokens/mssec: 31.75\n",
      "for iter: 9, loss: 7.57218074798584, time: 270.49ms, tokens/mssec: 30.29\n",
      "for iter: 10, loss: 7.56642484664917, time: 272.42ms, tokens/mssec: 30.07\n",
      "for iter: 11, loss: 7.270780086517334, time: 261.13ms, tokens/mssec: 31.37\n",
      "for iter: 12, loss: 7.126434803009033, time: 281.27ms, tokens/mssec: 29.13\n",
      "for iter: 13, loss: 6.978614807128906, time: 271.54ms, tokens/mssec: 30.17\n",
      "for iter: 14, loss: 6.790109634399414, time: 267.77ms, tokens/mssec: 30.59\n",
      "for iter: 15, loss: 6.916836261749268, time: 269.86ms, tokens/mssec: 30.36\n",
      "for iter: 16, loss: 6.751651287078857, time: 270.30ms, tokens/mssec: 30.31\n",
      "for iter: 17, loss: 6.6469831466674805, time: 276.06ms, tokens/mssec: 29.68\n",
      "for iter: 18, loss: 6.65116024017334, time: 270.55ms, tokens/mssec: 30.28\n",
      "for iter: 19, loss: 6.61794900894165, time: 271.57ms, tokens/mssec: 30.17\n",
      "for iter: 20, loss: 6.393957138061523, time: 274.93ms, tokens/mssec: 29.80\n",
      "for iter: 21, loss: 6.386856555938721, time: 272.56ms, tokens/mssec: 30.06\n",
      "for iter: 22, loss: 6.661072731018066, time: 277.56ms, tokens/mssec: 29.51\n",
      "for iter: 23, loss: 6.589475631713867, time: 266.13ms, tokens/mssec: 30.78\n",
      "for iter: 24, loss: 6.641298294067383, time: 271.09ms, tokens/mssec: 30.22\n",
      "for iter: 25, loss: 6.406744480133057, time: 275.25ms, tokens/mssec: 29.76\n",
      "for iter: 26, loss: 6.328742504119873, time: 273.59ms, tokens/mssec: 29.94\n",
      "for iter: 27, loss: 6.3797383308410645, time: 276.86ms, tokens/mssec: 29.59\n",
      "for iter: 28, loss: 6.476198673248291, time: 273.42ms, tokens/mssec: 29.96\n",
      "for iter: 29, loss: 6.452919006347656, time: 266.05ms, tokens/mssec: 30.79\n",
      "for iter: 30, loss: 6.337620735168457, time: 262.76ms, tokens/mssec: 31.18\n",
      "for iter: 31, loss: 6.219374656677246, time: 270.15ms, tokens/mssec: 30.32\n",
      "for iter: 32, loss: 6.451269626617432, time: 268.30ms, tokens/mssec: 30.53\n",
      "for iter: 33, loss: 6.536831378936768, time: 271.74ms, tokens/mssec: 30.15\n",
      "for iter: 34, loss: 6.6028900146484375, time: 273.02ms, tokens/mssec: 30.00\n",
      "for iter: 35, loss: 6.6975250244140625, time: 272.81ms, tokens/mssec: 30.03\n",
      "for iter: 36, loss: 6.554645538330078, time: 271.79ms, tokens/mssec: 30.14\n",
      "for iter: 37, loss: 6.55347204208374, time: 273.56ms, tokens/mssec: 29.95\n",
      "for iter: 38, loss: 6.384043216705322, time: 275.59ms, tokens/mssec: 29.72\n",
      "for iter: 39, loss: 6.280202388763428, time: 275.65ms, tokens/mssec: 29.72\n",
      "for iter: 40, loss: 6.316856384277344, time: 275.08ms, tokens/mssec: 29.78\n",
      "for iter: 41, loss: 6.187473297119141, time: 252.72ms, tokens/mssec: 32.41\n",
      "for iter: 42, loss: 6.067376613616943, time: 271.89ms, tokens/mssec: 30.13\n",
      "for iter: 43, loss: 6.176492214202881, time: 278.78ms, tokens/mssec: 29.39\n",
      "for iter: 44, loss: 6.063877105712891, time: 262.66ms, tokens/mssec: 31.19\n",
      "for iter: 45, loss: 6.248235702514648, time: 256.24ms, tokens/mssec: 31.97\n",
      "for iter: 46, loss: 6.0544753074646, time: 269.93ms, tokens/mssec: 30.35\n",
      "for iter: 47, loss: 6.041767597198486, time: 269.13ms, tokens/mssec: 30.44\n",
      "for iter: 48, loss: 6.128332614898682, time: 262.87ms, tokens/mssec: 31.16\n",
      "for iter: 49, loss: 6.285793781280518, time: 278.39ms, tokens/mssec: 29.43\n",
      "for iter: 50, loss: 6.175589084625244, time: 270.43ms, tokens/mssec: 30.29\n",
      "for iter: 51, loss: 6.447669982910156, time: 267.67ms, tokens/mssec: 30.61\n",
      "for iter: 52, loss: 6.183827877044678, time: 279.40ms, tokens/mssec: 29.32\n",
      "for iter: 53, loss: 6.175693035125732, time: 278.98ms, tokens/mssec: 29.36\n",
      "for iter: 54, loss: 6.106761932373047, time: 281.24ms, tokens/mssec: 29.13\n",
      "for iter: 55, loss: 5.984302520751953, time: 283.58ms, tokens/mssec: 28.89\n",
      "for iter: 56, loss: 6.215015888214111, time: 271.79ms, tokens/mssec: 30.14\n",
      "for iter: 57, loss: 6.090268611907959, time: 278.85ms, tokens/mssec: 29.38\n",
      "for iter: 58, loss: 6.069613456726074, time: 274.32ms, tokens/mssec: 29.86\n",
      "for iter: 59, loss: 6.0697174072265625, time: 273.08ms, tokens/mssec: 30.00\n",
      "for iter: 60, loss: 6.093522071838379, time: 275.63ms, tokens/mssec: 29.72\n",
      "for iter: 61, loss: 5.899548530578613, time: 281.92ms, tokens/mssec: 29.06\n",
      "for iter: 62, loss: 5.8802008628845215, time: 273.59ms, tokens/mssec: 29.94\n",
      "for iter: 63, loss: 6.244692325592041, time: 278.50ms, tokens/mssec: 29.41\n",
      "for iter: 64, loss: 6.201231479644775, time: 276.40ms, tokens/mssec: 29.64\n",
      "for iter: 65, loss: 6.278341770172119, time: 286.79ms, tokens/mssec: 28.56\n",
      "for iter: 66, loss: 6.015580177307129, time: 274.82ms, tokens/mssec: 29.81\n",
      "for iter: 67, loss: 5.9800705909729, time: 273.22ms, tokens/mssec: 29.98\n",
      "for iter: 68, loss: 6.017223834991455, time: 273.63ms, tokens/mssec: 29.94\n",
      "for iter: 69, loss: 6.118692398071289, time: 277.27ms, tokens/mssec: 29.55\n",
      "for iter: 70, loss: 6.066275119781494, time: 274.85ms, tokens/mssec: 29.80\n",
      "for iter: 71, loss: 5.954668045043945, time: 282.63ms, tokens/mssec: 28.98\n",
      "for iter: 72, loss: 5.80509090423584, time: 275.32ms, tokens/mssec: 29.75\n",
      "for iter: 73, loss: 6.030314922332764, time: 270.97ms, tokens/mssec: 30.23\n",
      "for iter: 74, loss: 6.094367980957031, time: 253.19ms, tokens/mssec: 32.35\n",
      "for iter: 75, loss: 6.218217372894287, time: 276.43ms, tokens/mssec: 29.63\n",
      "for iter: 76, loss: 6.291283130645752, time: 278.57ms, tokens/mssec: 29.41\n",
      "for iter: 77, loss: 6.190968036651611, time: 271.92ms, tokens/mssec: 30.13\n",
      "for iter: 78, loss: 6.194146633148193, time: 252.34ms, tokens/mssec: 32.46\n",
      "for iter: 79, loss: 6.0057783126831055, time: 270.60ms, tokens/mssec: 30.27\n",
      "for iter: 80, loss: 5.9829230308532715, time: 275.05ms, tokens/mssec: 29.78\n",
      "for iter: 81, loss: 6.05584716796875, time: 275.51ms, tokens/mssec: 29.73\n",
      "for iter: 82, loss: 5.970654010772705, time: 249.83ms, tokens/mssec: 32.79\n",
      "for iter: 83, loss: 5.843723773956299, time: 275.95ms, tokens/mssec: 29.69\n",
      "for iter: 84, loss: 5.963657855987549, time: 274.81ms, tokens/mssec: 29.81\n",
      "for iter: 85, loss: 5.862631320953369, time: 270.30ms, tokens/mssec: 30.31\n",
      "for iter: 86, loss: 6.04770565032959, time: 292.36ms, tokens/mssec: 28.02\n",
      "for iter: 87, loss: 5.829980850219727, time: 272.82ms, tokens/mssec: 30.03\n",
      "for iter: 88, loss: 5.821634292602539, time: 280.07ms, tokens/mssec: 29.25\n",
      "for iter: 89, loss: 5.930551052093506, time: 284.49ms, tokens/mssec: 28.80\n",
      "for iter: 90, loss: 6.077146053314209, time: 280.21ms, tokens/mssec: 29.23\n",
      "for iter: 91, loss: 5.985466480255127, time: 271.16ms, tokens/mssec: 30.21\n",
      "for iter: 92, loss: 6.267089366912842, time: 269.36ms, tokens/mssec: 30.41\n",
      "for iter: 93, loss: 5.991279602050781, time: 278.41ms, tokens/mssec: 29.42\n",
      "for iter: 94, loss: 6.003833293914795, time: 274.78ms, tokens/mssec: 29.81\n",
      "for iter: 95, loss: 5.904575347900391, time: 285.70ms, tokens/mssec: 28.67\n",
      "for iter: 96, loss: 5.816195487976074, time: 270.94ms, tokens/mssec: 30.24\n",
      "for iter: 97, loss: 6.041373252868652, time: 274.10ms, tokens/mssec: 29.89\n",
      "for iter: 98, loss: 5.910916328430176, time: 297.28ms, tokens/mssec: 27.56\n",
      "for iter: 99, loss: 5.906538486480713, time: 272.13ms, tokens/mssec: 30.10\n"
     ]
    }
   ],
   "source": [
    "transformer_model.train()\n",
    "for iter in range(initial_iterations):\n",
    "    start_time = time.time()\n",
    "    x, y = train_loader.next_batch()\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    logits, loss = transformer_model(x, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    end_time = time.time()\n",
    "    torch.cuda.empty_cache()  # clear the cache\n",
    "    torch.cuda.synchronize()  # wait for the computation to be done\n",
    "    time_elapsed = (end_time - start_time) * 1000\n",
    "    token_persec = x.numel() / time_elapsed\n",
    "    print(\n",
    "        f\"for iter: {iter}, loss: {loss.item()}, time: {time_elapsed:.2f}ms, tokens/mssec: {token_persec:.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision settings - Application of autocast and mixed precision training aslong with gradient scaling\n",
    "* bfloat16 is only available on A series GPUs so we use float16 for mixed precision training along with gradient scaling\n",
    "* always use autocast to reduce the precision of the model to float16 along with scaling the gradients to avoid underflow and overflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using float16 for automatic mixed precision training\n"
     ]
    }
   ],
   "source": [
    "dtype = (\n",
    "    \"bfloat16\"\n",
    "    if torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "    else \"float16\"\n",
    ")\n",
    "print(f\"Using {dtype} for automatic mixed precision training\")\n",
    "\n",
    "# note: float16 data type will automatically use a GradScaler\n",
    "ptdtype = {\n",
    "    \"float32\": torch.float32,\n",
    "    \"bfloat16\": torch.bfloat16,\n",
    "    \"float16\": torch.float16,\n",
    "}[dtype]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# context manager for automatic mixed precision training\n",
    "ctx = (\n",
    "    nullcontext()\n",
    "    if device_type == \"cpu\"\n",
    "    else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    ")\n",
    "\n",
    "if ctx != nullcontext():\n",
    "    # GradScaler for automatic mixed precision training\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): GELU(approximate='tanh')\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_model = GPT(GPTConfig())\n",
    "transformer_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(transformer_model.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for iter: 0, loss: 10.965310096740723, time: 390.68ms, tokens/mssec: 20.97\n",
      "for iter: 1, loss: 9.53087329864502, time: 251.98ms, tokens/mssec: 32.51\n",
      "for iter: 2, loss: 9.536520004272461, time: 295.73ms, tokens/mssec: 27.70\n",
      "for iter: 3, loss: 8.736123085021973, time: 306.79ms, tokens/mssec: 26.70\n",
      "for iter: 4, loss: 8.643856048583984, time: 298.67ms, tokens/mssec: 27.43\n",
      "for iter: 5, loss: 8.479509353637695, time: 290.01ms, tokens/mssec: 28.25\n",
      "for iter: 6, loss: 8.368569374084473, time: 304.90ms, tokens/mssec: 26.87\n",
      "for iter: 7, loss: 7.94486665725708, time: 297.54ms, tokens/mssec: 27.53\n",
      "for iter: 8, loss: 7.701452255249023, time: 271.95ms, tokens/mssec: 30.12\n",
      "for iter: 9, loss: 7.574922561645508, time: 301.71ms, tokens/mssec: 27.15\n",
      "for iter: 10, loss: 7.519839286804199, time: 297.82ms, tokens/mssec: 27.51\n",
      "for iter: 11, loss: 7.5592041015625, time: 304.23ms, tokens/mssec: 26.93\n",
      "for iter: 12, loss: 7.290929317474365, time: 293.78ms, tokens/mssec: 27.88\n",
      "for iter: 13, loss: 7.059926986694336, time: 294.52ms, tokens/mssec: 27.82\n",
      "for iter: 14, loss: 7.122575759887695, time: 295.93ms, tokens/mssec: 27.68\n",
      "for iter: 15, loss: 7.211103439331055, time: 302.29ms, tokens/mssec: 27.10\n",
      "for iter: 16, loss: 7.074745178222656, time: 292.99ms, tokens/mssec: 27.96\n",
      "for iter: 17, loss: 7.094254493713379, time: 271.33ms, tokens/mssec: 30.19\n",
      "for iter: 18, loss: 6.933478355407715, time: 297.18ms, tokens/mssec: 27.57\n",
      "for iter: 19, loss: 6.8880815505981445, time: 295.30ms, tokens/mssec: 27.74\n",
      "for iter: 20, loss: 6.752786636352539, time: 298.49ms, tokens/mssec: 27.45\n",
      "for iter: 21, loss: 6.6658935546875, time: 300.48ms, tokens/mssec: 27.26\n",
      "for iter: 22, loss: 6.715666770935059, time: 291.40ms, tokens/mssec: 28.11\n",
      "for iter: 23, loss: 6.601866245269775, time: 303.58ms, tokens/mssec: 26.98\n",
      "for iter: 24, loss: 6.515629768371582, time: 296.91ms, tokens/mssec: 27.59\n",
      "for iter: 25, loss: 6.597643852233887, time: 300.71ms, tokens/mssec: 27.24\n",
      "for iter: 26, loss: 6.550115585327148, time: 294.18ms, tokens/mssec: 27.85\n",
      "for iter: 27, loss: 6.625463008880615, time: 295.11ms, tokens/mssec: 27.76\n",
      "for iter: 28, loss: 6.532286167144775, time: 297.30ms, tokens/mssec: 27.56\n",
      "for iter: 29, loss: 6.526129245758057, time: 298.00ms, tokens/mssec: 27.49\n",
      "for iter: 30, loss: 6.594934463500977, time: 298.29ms, tokens/mssec: 27.46\n",
      "for iter: 31, loss: 6.661135196685791, time: 302.17ms, tokens/mssec: 27.11\n",
      "for iter: 32, loss: 6.526878833770752, time: 299.76ms, tokens/mssec: 27.33\n",
      "for iter: 33, loss: 6.766528606414795, time: 304.58ms, tokens/mssec: 26.90\n",
      "for iter: 34, loss: 6.588387489318848, time: 294.69ms, tokens/mssec: 27.80\n",
      "for iter: 35, loss: 6.543622016906738, time: 298.79ms, tokens/mssec: 27.42\n",
      "for iter: 36, loss: 6.483476638793945, time: 288.98ms, tokens/mssec: 28.35\n",
      "for iter: 37, loss: 6.3539719581604, time: 301.72ms, tokens/mssec: 27.15\n",
      "for iter: 38, loss: 6.598686218261719, time: 293.55ms, tokens/mssec: 27.91\n",
      "for iter: 39, loss: 6.468799114227295, time: 300.40ms, tokens/mssec: 27.27\n",
      "for iter: 40, loss: 6.380755424499512, time: 294.23ms, tokens/mssec: 27.84\n",
      "for iter: 41, loss: 6.24086856842041, time: 309.93ms, tokens/mssec: 26.43\n",
      "for iter: 42, loss: 6.259163856506348, time: 296.29ms, tokens/mssec: 27.65\n",
      "for iter: 43, loss: 6.067290782928467, time: 297.45ms, tokens/mssec: 27.54\n",
      "for iter: 44, loss: 6.07089376449585, time: 297.68ms, tokens/mssec: 27.52\n",
      "for iter: 45, loss: 6.427216053009033, time: 309.34ms, tokens/mssec: 26.48\n",
      "for iter: 46, loss: 6.385709285736084, time: 309.52ms, tokens/mssec: 26.47\n",
      "for iter: 47, loss: 6.435861587524414, time: 298.44ms, tokens/mssec: 27.45\n",
      "for iter: 48, loss: 6.175338268280029, time: 305.93ms, tokens/mssec: 26.78\n",
      "for iter: 49, loss: 6.111572265625, time: 292.66ms, tokens/mssec: 27.99\n",
      "for iter: 50, loss: 6.201765537261963, time: 301.64ms, tokens/mssec: 27.16\n",
      "for iter: 51, loss: 6.31873893737793, time: 299.21ms, tokens/mssec: 27.38\n",
      "for iter: 52, loss: 6.2831878662109375, time: 294.20ms, tokens/mssec: 27.85\n",
      "for iter: 53, loss: 6.191380977630615, time: 299.55ms, tokens/mssec: 27.35\n",
      "for iter: 54, loss: 6.050745487213135, time: 298.06ms, tokens/mssec: 27.48\n",
      "for iter: 55, loss: 6.254978179931641, time: 271.47ms, tokens/mssec: 30.18\n",
      "for iter: 56, loss: 6.3311052322387695, time: 302.02ms, tokens/mssec: 27.12\n",
      "for iter: 57, loss: 6.407949924468994, time: 305.72ms, tokens/mssec: 26.80\n",
      "for iter: 58, loss: 6.497009754180908, time: 297.26ms, tokens/mssec: 27.56\n",
      "for iter: 59, loss: 6.387905120849609, time: 291.06ms, tokens/mssec: 28.15\n",
      "for iter: 60, loss: 6.38169002532959, time: 307.30ms, tokens/mssec: 26.66\n",
      "for iter: 61, loss: 6.205589771270752, time: 291.11ms, tokens/mssec: 28.14\n",
      "for iter: 62, loss: 6.201298713684082, time: 301.47ms, tokens/mssec: 27.17\n",
      "for iter: 63, loss: 6.2990851402282715, time: 297.91ms, tokens/mssec: 27.50\n",
      "for iter: 64, loss: 6.234250068664551, time: 292.26ms, tokens/mssec: 28.03\n",
      "for iter: 65, loss: 6.112386703491211, time: 298.77ms, tokens/mssec: 27.42\n",
      "for iter: 66, loss: 6.202977657318115, time: 294.24ms, tokens/mssec: 27.84\n",
      "for iter: 67, loss: 6.06229829788208, time: 303.91ms, tokens/mssec: 26.96\n",
      "for iter: 68, loss: 6.227644443511963, time: 297.50ms, tokens/mssec: 27.54\n",
      "for iter: 69, loss: 6.050052642822266, time: 309.79ms, tokens/mssec: 26.44\n",
      "for iter: 70, loss: 6.045604705810547, time: 292.52ms, tokens/mssec: 28.00\n",
      "for iter: 71, loss: 6.12955904006958, time: 299.79ms, tokens/mssec: 27.33\n",
      "for iter: 72, loss: 6.327237129211426, time: 291.64ms, tokens/mssec: 28.09\n",
      "for iter: 73, loss: 6.2025909423828125, time: 292.24ms, tokens/mssec: 28.03\n",
      "for iter: 74, loss: 6.482266902923584, time: 297.94ms, tokens/mssec: 27.50\n",
      "for iter: 75, loss: 6.24071741104126, time: 300.85ms, tokens/mssec: 27.23\n",
      "for iter: 76, loss: 6.211026191711426, time: 297.80ms, tokens/mssec: 27.51\n",
      "for iter: 77, loss: 6.13477087020874, time: 301.96ms, tokens/mssec: 27.13\n",
      "for iter: 78, loss: 6.026021480560303, time: 295.92ms, tokens/mssec: 27.68\n",
      "for iter: 79, loss: 6.247807025909424, time: 297.85ms, tokens/mssec: 27.50\n",
      "for iter: 80, loss: 6.113644599914551, time: 294.84ms, tokens/mssec: 27.78\n",
      "for iter: 81, loss: 6.086416721343994, time: 300.98ms, tokens/mssec: 27.22\n",
      "for iter: 82, loss: 6.0389323234558105, time: 299.89ms, tokens/mssec: 27.32\n",
      "for iter: 83, loss: 6.054714202880859, time: 286.02ms, tokens/mssec: 28.64\n",
      "for iter: 84, loss: 5.883450031280518, time: 295.56ms, tokens/mssec: 27.72\n",
      "for iter: 85, loss: 5.859491348266602, time: 302.63ms, tokens/mssec: 27.07\n",
      "for iter: 86, loss: 6.198083877563477, time: 303.53ms, tokens/mssec: 26.99\n",
      "for iter: 87, loss: 6.155015468597412, time: 295.40ms, tokens/mssec: 27.73\n",
      "for iter: 88, loss: 6.240755558013916, time: 297.84ms, tokens/mssec: 27.50\n",
      "for iter: 89, loss: 5.989536285400391, time: 297.91ms, tokens/mssec: 27.50\n",
      "for iter: 90, loss: 5.921606540679932, time: 304.98ms, tokens/mssec: 26.86\n",
      "for iter: 91, loss: 6.000044345855713, time: 302.27ms, tokens/mssec: 27.10\n",
      "for iter: 92, loss: 6.106997489929199, time: 298.84ms, tokens/mssec: 27.41\n",
      "for iter: 93, loss: 6.033506870269775, time: 299.68ms, tokens/mssec: 27.34\n",
      "for iter: 94, loss: 5.9302520751953125, time: 300.26ms, tokens/mssec: 27.28\n",
      "for iter: 95, loss: 5.79494571685791, time: 302.18ms, tokens/mssec: 27.11\n",
      "for iter: 96, loss: 6.0102925300598145, time: 287.78ms, tokens/mssec: 28.47\n",
      "for iter: 97, loss: 6.082411766052246, time: 302.25ms, tokens/mssec: 27.10\n",
      "for iter: 98, loss: 6.19837760925293, time: 305.21ms, tokens/mssec: 26.84\n",
      "for iter: 99, loss: 6.2864603996276855, time: 301.32ms, tokens/mssec: 27.19\n"
     ]
    }
   ],
   "source": [
    "transformer_model.train()\n",
    "for iter in range(initial_iterations):\n",
    "    start_time = time.time()\n",
    "    x, y = train_loader.next_batch()\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    with ctx:\n",
    "        logits, loss = transformer_model(x, y)\n",
    "\n",
    "    # backward pass with GradScaler\n",
    "    scaler.scale(loss).backward()\n",
    "    # Unscales the gradients and calls optimizer.step()\n",
    "    scaler.step(optimizer)\n",
    "    # Updates the scale for next iteration\n",
    "    scaler.update()\n",
    "    end_time = time.time()\n",
    "    torch.cuda.empty_cache()  # clear the cache\n",
    "    torch.cuda.synchronize()  # wait for the computation to be done\n",
    "    time_elapsed = (end_time - start_time) * 1000\n",
    "    token_persec = x.numel() / time_elapsed\n",
    "    print(\n",
    "        f\"for iter: {iter}, loss: {loss.item()}, time: {time_elapsed:.2f}ms, tokens/mssec: {token_persec:.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient clipping - to prevent exploding gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_clip = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# context manager for automatic mixed precision training\n",
    "ctx = (\n",
    "    nullcontext()\n",
    "    if device_type == \"cpu\"\n",
    "    else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    ")\n",
    "\n",
    "if ctx != nullcontext():\n",
    "    # GradScaler for automatic mixed precision training\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): GELU(approximate='tanh')\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_model = GPT(GPTConfig())\n",
    "transformer_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(transformer_model.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-49-26765549d308>:16: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  norm = torch.nn.utils.clip_grad_norm(transformer_model.parameters(), grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for iter: 0, loss: 10.942142486572266, norm: 28.75 time: 485.05ms, tokens/mssec: 16.89\n",
      "for iter: 1, loss: 9.615848541259766, norm: 5.15 time: 269.93ms, tokens/mssec: 30.35\n",
      "for iter: 2, loss: 9.241558074951172, norm: 7.01 time: 296.41ms, tokens/mssec: 27.64\n",
      "for iter: 3, loss: 9.006820678710938, norm: 6.30 time: 289.35ms, tokens/mssec: 28.31\n",
      "for iter: 4, loss: 8.795352935791016, norm: 2.83 time: 291.84ms, tokens/mssec: 28.07\n",
      "for iter: 5, loss: 8.484068870544434, norm: 3.05 time: 298.64ms, tokens/mssec: 27.43\n",
      "for iter: 6, loss: 8.187207221984863, norm: 2.49 time: 297.81ms, tokens/mssec: 27.51\n",
      "for iter: 7, loss: 7.973771572113037, norm: 1.88 time: 297.58ms, tokens/mssec: 27.53\n",
      "for iter: 8, loss: 7.765438556671143, norm: 1.77 time: 275.73ms, tokens/mssec: 29.71\n",
      "for iter: 9, loss: 7.726118087768555, norm: 1.80 time: 294.66ms, tokens/mssec: 27.80\n",
      "for iter: 10, loss: 7.417959213256836, norm: 1.59 time: 293.73ms, tokens/mssec: 27.89\n",
      "for iter: 11, loss: 7.235445976257324, norm: 1.46 time: 292.80ms, tokens/mssec: 27.98\n",
      "for iter: 12, loss: 7.116228103637695, norm: 1.64 time: 281.97ms, tokens/mssec: 29.05\n",
      "for iter: 13, loss: 7.047834396362305, norm: 1.51 time: 298.88ms, tokens/mssec: 27.41\n",
      "for iter: 14, loss: 6.830768585205078, norm: 1.37 time: 300.79ms, tokens/mssec: 27.24\n",
      "for iter: 15, loss: 6.932760238647461, norm: 1.02 time: 293.20ms, tokens/mssec: 27.94\n",
      "for iter: 16, loss: 6.688301086425781, norm: 1.59 time: 293.31ms, tokens/mssec: 27.93\n",
      "for iter: 17, loss: 6.610691547393799, norm: 1.23 time: 294.87ms, tokens/mssec: 27.78\n",
      "for iter: 18, loss: 6.569214820861816, norm: 1.22 time: 298.64ms, tokens/mssec: 27.43\n",
      "for iter: 19, loss: 6.404293060302734, norm: 1.29 time: 298.33ms, tokens/mssec: 27.46\n",
      "for iter: 20, loss: 6.65540075302124, norm: 1.10 time: 298.34ms, tokens/mssec: 27.46\n",
      "for iter: 21, loss: 6.515199184417725, norm: 1.20 time: 296.90ms, tokens/mssec: 27.59\n",
      "for iter: 22, loss: 6.4356465339660645, norm: 1.36 time: 293.84ms, tokens/mssec: 27.88\n",
      "for iter: 23, loss: 6.449216842651367, norm: 1.14 time: 288.87ms, tokens/mssec: 28.36\n",
      "for iter: 24, loss: 6.4618000984191895, norm: 1.34 time: 291.47ms, tokens/mssec: 28.11\n",
      "for iter: 25, loss: 6.2293243408203125, norm: 1.02 time: 293.39ms, tokens/mssec: 27.92\n",
      "for iter: 26, loss: 6.231747627258301, norm: 1.53 time: 293.61ms, tokens/mssec: 27.90\n",
      "for iter: 27, loss: 6.579743385314941, norm: 1.47 time: 293.20ms, tokens/mssec: 27.94\n",
      "for iter: 28, loss: 6.588550567626953, norm: 1.27 time: 295.69ms, tokens/mssec: 27.70\n",
      "for iter: 29, loss: 6.616212844848633, norm: 1.38 time: 296.93ms, tokens/mssec: 27.59\n",
      "for iter: 30, loss: 6.337082386016846, norm: 1.45 time: 298.82ms, tokens/mssec: 27.41\n",
      "for iter: 31, loss: 6.203090190887451, norm: 1.01 time: 295.90ms, tokens/mssec: 27.69\n",
      "for iter: 32, loss: 6.273715019226074, norm: 1.00 time: 301.18ms, tokens/mssec: 27.20\n",
      "for iter: 33, loss: 6.347442626953125, norm: 1.10 time: 297.27ms, tokens/mssec: 27.56\n",
      "for iter: 34, loss: 6.37505578994751, norm: 1.61 time: 294.35ms, tokens/mssec: 27.83\n",
      "for iter: 35, loss: 6.231704235076904, norm: 1.09 time: 291.69ms, tokens/mssec: 28.09\n",
      "for iter: 36, loss: 6.072432041168213, norm: 0.96 time: 297.41ms, tokens/mssec: 27.54\n",
      "for iter: 37, loss: 6.301667213439941, norm: 1.10 time: 294.09ms, tokens/mssec: 27.86\n",
      "for iter: 38, loss: 6.349908828735352, norm: 1.11 time: 289.70ms, tokens/mssec: 28.28\n",
      "for iter: 39, loss: 6.414323329925537, norm: 0.83 time: 300.43ms, tokens/mssec: 27.27\n",
      "for iter: 40, loss: 6.4564642906188965, norm: 0.97 time: 280.23ms, tokens/mssec: 29.23\n",
      "for iter: 41, loss: 6.313321590423584, norm: 0.66 time: 292.70ms, tokens/mssec: 27.99\n",
      "for iter: 42, loss: 6.276830673217773, norm: 0.67 time: 296.14ms, tokens/mssec: 27.66\n",
      "for iter: 43, loss: 6.082023620605469, norm: 0.87 time: 301.13ms, tokens/mssec: 27.20\n",
      "for iter: 44, loss: 6.087725639343262, norm: 0.79 time: 294.33ms, tokens/mssec: 27.83\n",
      "for iter: 45, loss: 6.161050796508789, norm: 0.83 time: 297.83ms, tokens/mssec: 27.51\n",
      "for iter: 46, loss: 6.078580379486084, norm: 0.92 time: 293.52ms, tokens/mssec: 27.91\n",
      "for iter: 47, loss: 5.938689231872559, norm: 1.02 time: 297.52ms, tokens/mssec: 27.53\n",
      "for iter: 48, loss: 6.036243915557861, norm: 0.78 time: 292.66ms, tokens/mssec: 27.99\n",
      "for iter: 49, loss: 5.930271625518799, norm: 1.03 time: 295.28ms, tokens/mssec: 27.74\n",
      "for iter: 50, loss: 6.106130599975586, norm: 1.08 time: 269.39ms, tokens/mssec: 30.41\n",
      "for iter: 51, loss: 5.905233860015869, norm: 0.78 time: 294.11ms, tokens/mssec: 27.85\n",
      "for iter: 52, loss: 5.909204006195068, norm: 0.85 time: 299.41ms, tokens/mssec: 27.36\n",
      "for iter: 53, loss: 5.997730731964111, norm: 0.81 time: 290.04ms, tokens/mssec: 28.24\n",
      "for iter: 54, loss: 6.196479320526123, norm: 1.08 time: 296.82ms, tokens/mssec: 27.60\n",
      "for iter: 55, loss: 6.080021381378174, norm: 1.12 time: 295.54ms, tokens/mssec: 27.72\n",
      "for iter: 56, loss: 6.351482391357422, norm: 1.04 time: 295.60ms, tokens/mssec: 27.71\n",
      "for iter: 57, loss: 6.12116813659668, norm: 1.08 time: 295.85ms, tokens/mssec: 27.69\n",
      "for iter: 58, loss: 6.087806224822998, norm: 0.91 time: 293.23ms, tokens/mssec: 27.94\n",
      "for iter: 59, loss: 6.029722213745117, norm: 1.06 time: 277.11ms, tokens/mssec: 29.56\n",
      "for iter: 60, loss: 5.939840316772461, norm: 1.15 time: 289.92ms, tokens/mssec: 28.26\n",
      "for iter: 61, loss: 6.154660224914551, norm: 1.21 time: 297.52ms, tokens/mssec: 27.53\n",
      "for iter: 62, loss: 6.03342866897583, norm: 1.25 time: 291.53ms, tokens/mssec: 28.10\n",
      "for iter: 63, loss: 6.0171332359313965, norm: 1.12 time: 295.03ms, tokens/mssec: 27.77\n",
      "for iter: 64, loss: 6.000208854675293, norm: 1.10 time: 284.86ms, tokens/mssec: 28.76\n",
      "for iter: 65, loss: 6.020611763000488, norm: 0.94 time: 295.56ms, tokens/mssec: 27.72\n",
      "for iter: 66, loss: 5.821774959564209, norm: 0.97 time: 291.81ms, tokens/mssec: 28.07\n",
      "for iter: 67, loss: 5.799633026123047, norm: 0.94 time: 289.36ms, tokens/mssec: 28.31\n",
      "for iter: 68, loss: 6.169230937957764, norm: 1.21 time: 297.34ms, tokens/mssec: 27.55\n",
      "for iter: 69, loss: 6.116903781890869, norm: 1.02 time: 287.79ms, tokens/mssec: 28.47\n",
      "for iter: 70, loss: 6.201053142547607, norm: 1.19 time: 296.79ms, tokens/mssec: 27.60\n",
      "for iter: 71, loss: 5.940699100494385, norm: 1.35 time: 297.01ms, tokens/mssec: 27.58\n",
      "for iter: 72, loss: 5.8436431884765625, norm: 1.13 time: 304.24ms, tokens/mssec: 26.93\n",
      "for iter: 73, loss: 5.904735088348389, norm: 1.09 time: 289.76ms, tokens/mssec: 28.27\n",
      "for iter: 74, loss: 6.010793209075928, norm: 0.88 time: 301.25ms, tokens/mssec: 27.19\n",
      "for iter: 75, loss: 5.962714672088623, norm: 1.14 time: 297.74ms, tokens/mssec: 27.51\n",
      "for iter: 76, loss: 5.8714399337768555, norm: 1.12 time: 301.95ms, tokens/mssec: 27.13\n",
      "for iter: 77, loss: 5.701624393463135, norm: 1.19 time: 293.19ms, tokens/mssec: 27.94\n",
      "for iter: 78, loss: 5.935553073883057, norm: 0.92 time: 292.35ms, tokens/mssec: 28.02\n",
      "for iter: 79, loss: 5.985249996185303, norm: 1.15 time: 292.88ms, tokens/mssec: 27.97\n",
      "for iter: 80, loss: 6.1068644523620605, norm: 0.79 time: 290.31ms, tokens/mssec: 28.22\n",
      "for iter: 81, loss: 6.185464859008789, norm: 0.84 time: 290.41ms, tokens/mssec: 28.21\n",
      "for iter: 82, loss: 6.070043087005615, norm: 0.83 time: 298.83ms, tokens/mssec: 27.41\n",
      "for iter: 83, loss: 6.057229042053223, norm: 0.81 time: 301.70ms, tokens/mssec: 27.15\n",
      "for iter: 84, loss: 5.880099773406982, norm: 0.90 time: 297.36ms, tokens/mssec: 27.55\n",
      "for iter: 85, loss: 5.87460470199585, norm: 1.00 time: 297.38ms, tokens/mssec: 27.55\n",
      "for iter: 86, loss: 5.958507537841797, norm: 0.82 time: 302.21ms, tokens/mssec: 27.11\n",
      "for iter: 87, loss: 5.862704277038574, norm: 0.85 time: 300.42ms, tokens/mssec: 27.27\n",
      "for iter: 88, loss: 5.764439105987549, norm: 0.85 time: 284.20ms, tokens/mssec: 28.83\n",
      "for iter: 89, loss: 5.856138229370117, norm: 0.81 time: 300.34ms, tokens/mssec: 27.28\n",
      "for iter: 90, loss: 5.730982780456543, norm: 0.77 time: 292.30ms, tokens/mssec: 28.03\n",
      "for iter: 91, loss: 5.928446292877197, norm: 0.85 time: 302.96ms, tokens/mssec: 27.04\n",
      "for iter: 92, loss: 5.72401762008667, norm: 0.81 time: 292.57ms, tokens/mssec: 28.00\n",
      "for iter: 93, loss: 5.715781211853027, norm: 0.85 time: 293.40ms, tokens/mssec: 27.92\n",
      "for iter: 94, loss: 5.809081077575684, norm: 0.78 time: 298.00ms, tokens/mssec: 27.49\n",
      "for iter: 95, loss: 5.972898006439209, norm: 1.03 time: 297.28ms, tokens/mssec: 27.56\n",
      "for iter: 96, loss: 5.855832576751709, norm: 1.09 time: 298.36ms, tokens/mssec: 27.46\n",
      "for iter: 97, loss: 6.134594917297363, norm: 0.88 time: 290.50ms, tokens/mssec: 28.20\n",
      "for iter: 98, loss: 5.887510299682617, norm: 0.98 time: 299.47ms, tokens/mssec: 27.36\n",
      "for iter: 99, loss: 5.876105785369873, norm: 0.93 time: 300.32ms, tokens/mssec: 27.28\n"
     ]
    }
   ],
   "source": [
    "transformer_model.train()\n",
    "for iter in range(initial_iterations):\n",
    "    start_time = time.time()\n",
    "    x, y = train_loader.next_batch()\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    with ctx:\n",
    "        logits, loss = transformer_model(x, y)\n",
    "\n",
    "    # backward pass with GradScaler\n",
    "    scaler.scale(loss).backward()\n",
    "    if grad_clip != 0.0:\n",
    "        # unscales the gradients of optimizer's assigned params in-place\n",
    "        scaler.unscale_(optimizer)\n",
    "        # clip the gradients\n",
    "        norm = torch.nn.utils.clip_grad_norm(transformer_model.parameters(), grad_clip)\n",
    "\n",
    "    # Unscales the gradients and calls optimizer.step()\n",
    "    scaler.step(optimizer)\n",
    "    # Updates the scale for next iteration\n",
    "    scaler.update()\n",
    "    end_time = time.time()\n",
    "    torch.cuda.empty_cache()  # clear the cache\n",
    "    torch.cuda.synchronize()  # wait for the computation to be done\n",
    "    time_elapsed = (end_time - start_time) * 1000\n",
    "    token_persec = x.numel() / time_elapsed\n",
    "    print(\n",
    "        f\"for iter: {iter}, loss: {loss.item()}, norm: {norm:.2f} time: {time_elapsed:.2f}ms, tokens/mssec: {token_persec:.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Optimizations by GPT3 paper -I\n",
    "* Use of 50304 as the vocab size as it is devised to be a multiple of 8\n",
    "* use of specific version of Adam optimizer\n",
    "* use of torch compile for faster training\n",
    "* use of 2048 context size / sequence length for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using float16 for automatic mixed precision training\n"
     ]
    }
   ],
   "source": [
    "dtype = (\n",
    "    \"bfloat16\"\n",
    "    if torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "    else \"float16\"\n",
    ")\n",
    "print(f\"Using {dtype} for automatic mixed precision training\")\n",
    "\n",
    "# note: float16 data type will automatically use a GradScaler\n",
    "ptdtype = {\n",
    "    \"float32\": torch.float32,\n",
    "    \"bfloat16\": torch.bfloat16,\n",
    "    \"float16\": torch.float16,\n",
    "}[dtype]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# permisssion for grad_clip\n",
    "grad_clip = 0.0\n",
    "\n",
    "# model compilation\n",
    "compile = True\n",
    "\n",
    "# context manager for automatic mixed precision training\n",
    "ctx = (\n",
    "    nullcontext()\n",
    "    if device_type == \"cpu\"\n",
    "    else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    ")\n",
    "\n",
    "if ctx != nullcontext():\n",
    "    # GradScaler for automatic mixed precision training\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model = GPT(\n",
    "    GPTConfig(\n",
    "        vocab_size=50304,\n",
    "        block_size=2048,\n",
    "        n_layer=12,\n",
    "        n_head=12,\n",
    "        n_embd=768,\n",
    "    )\n",
    ")\n",
    "transformer_model.to(device)\n",
    "\n",
    "if compile:\n",
    "    transformer_model = torch.compile(transformer_model)\n",
    "\n",
    "# Adam optimizer\n",
    "optimizer = torch.optim.Adam(\n",
    "    transformer_model.parameters(), lr=3e-4, betas=(0.9, 0.95), eps=1e-8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens in the file: 338025\n",
      "1 epoch will have 41 iterations\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataloaderLiter(B=8, T=1024, encoder=encoder, filename=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total iterations: 82\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 2\n",
    "initial_iterations = 100\n",
    "total_iteration = num_epochs * train_loader.num_iterations\n",
    "print(f\"Total iterations: {total_iteration}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for iter: 0, loss: 10.936626434326172, norm: 0.00 time: 43660.35ms, tokens/mssec: 0.19\n",
      "for iter: 1, loss: 9.465581893920898, norm: 0.00 time: 238.07ms, tokens/mssec: 34.41\n",
      "for iter: 2, loss: 8.741138458251953, norm: 0.00 time: 232.24ms, tokens/mssec: 35.27\n",
      "for iter: 3, loss: 8.591870307922363, norm: 0.00 time: 242.22ms, tokens/mssec: 33.82\n",
      "for iter: 4, loss: 8.394872665405273, norm: 0.00 time: 241.84ms, tokens/mssec: 33.87\n",
      "for iter: 5, loss: 8.374696731567383, norm: 0.00 time: 237.87ms, tokens/mssec: 34.44\n",
      "for iter: 6, loss: 8.338766098022461, norm: 0.00 time: 231.76ms, tokens/mssec: 35.35\n",
      "for iter: 7, loss: 8.015644073486328, norm: 0.00 time: 248.27ms, tokens/mssec: 33.00\n",
      "for iter: 8, loss: 7.703030586242676, norm: 0.00 time: 230.76ms, tokens/mssec: 35.50\n",
      "for iter: 9, loss: 7.702876091003418, norm: 0.00 time: 238.97ms, tokens/mssec: 34.28\n",
      "for iter: 10, loss: 7.702516555786133, norm: 0.00 time: 235.65ms, tokens/mssec: 34.76\n",
      "for iter: 11, loss: 7.5014142990112305, norm: 0.00 time: 242.96ms, tokens/mssec: 33.72\n",
      "for iter: 12, loss: 7.467784881591797, norm: 0.00 time: 238.63ms, tokens/mssec: 34.33\n",
      "for iter: 13, loss: 7.241578578948975, norm: 0.00 time: 235.47ms, tokens/mssec: 34.79\n",
      "for iter: 14, loss: 7.125724792480469, norm: 0.00 time: 242.83ms, tokens/mssec: 33.74\n",
      "for iter: 15, loss: 6.939150810241699, norm: 0.00 time: 235.16ms, tokens/mssec: 34.84\n",
      "for iter: 16, loss: 6.811221599578857, norm: 0.00 time: 234.96ms, tokens/mssec: 34.87\n",
      "for iter: 17, loss: 6.796461582183838, norm: 0.00 time: 242.32ms, tokens/mssec: 33.81\n",
      "for iter: 18, loss: 6.67744255065918, norm: 0.00 time: 239.30ms, tokens/mssec: 34.23\n",
      "for iter: 19, loss: 6.512208461761475, norm: 0.00 time: 228.06ms, tokens/mssec: 35.92\n",
      "for iter: 20, loss: 6.562990188598633, norm: 0.00 time: 241.66ms, tokens/mssec: 33.90\n",
      "for iter: 21, loss: 6.426244735717773, norm: 0.00 time: 236.22ms, tokens/mssec: 34.68\n",
      "for iter: 22, loss: 6.496681213378906, norm: 0.00 time: 236.57ms, tokens/mssec: 34.63\n",
      "for iter: 23, loss: 6.3609619140625, norm: 0.00 time: 235.95ms, tokens/mssec: 34.72\n",
      "for iter: 24, loss: 6.351044654846191, norm: 0.00 time: 241.86ms, tokens/mssec: 33.87\n",
      "for iter: 25, loss: 6.424103736877441, norm: 0.00 time: 237.72ms, tokens/mssec: 34.46\n",
      "for iter: 26, loss: 6.591824531555176, norm: 0.00 time: 245.18ms, tokens/mssec: 33.41\n",
      "for iter: 27, loss: 6.480771064758301, norm: 0.00 time: 237.12ms, tokens/mssec: 34.55\n",
      "for iter: 28, loss: 6.7299041748046875, norm: 0.00 time: 238.83ms, tokens/mssec: 34.30\n",
      "for iter: 29, loss: 6.530682563781738, norm: 0.00 time: 236.20ms, tokens/mssec: 34.68\n",
      "for iter: 30, loss: 6.511892318725586, norm: 0.00 time: 242.53ms, tokens/mssec: 33.78\n",
      "for iter: 31, loss: 6.519568920135498, norm: 0.00 time: 232.78ms, tokens/mssec: 35.19\n",
      "for iter: 32, loss: 6.364727973937988, norm: 0.00 time: 245.66ms, tokens/mssec: 33.35\n",
      "for iter: 33, loss: 6.693321228027344, norm: 0.00 time: 241.04ms, tokens/mssec: 33.99\n",
      "for iter: 34, loss: 6.535869121551514, norm: 0.00 time: 240.98ms, tokens/mssec: 33.99\n",
      "for iter: 35, loss: 6.407810211181641, norm: 0.00 time: 235.16ms, tokens/mssec: 34.84\n",
      "for iter: 36, loss: 6.450299263000488, norm: 0.00 time: 241.78ms, tokens/mssec: 33.88\n",
      "for iter: 37, loss: 6.426563739776611, norm: 0.00 time: 240.43ms, tokens/mssec: 34.07\n",
      "for iter: 38, loss: 6.227381229400635, norm: 0.00 time: 241.93ms, tokens/mssec: 33.86\n",
      "for iter: 39, loss: 6.269940376281738, norm: 0.00 time: 242.42ms, tokens/mssec: 33.79\n",
      "for iter: 40, loss: 6.509969711303711, norm: 0.00 time: 240.03ms, tokens/mssec: 34.13\n",
      "for iter: 41, loss: 6.287985801696777, norm: 0.00 time: 237.48ms, tokens/mssec: 34.50\n",
      "for iter: 42, loss: 6.327589988708496, norm: 0.00 time: 248.94ms, tokens/mssec: 32.91\n",
      "for iter: 43, loss: 6.059812545776367, norm: 0.00 time: 241.01ms, tokens/mssec: 33.99\n",
      "for iter: 44, loss: 5.991547107696533, norm: 0.00 time: 247.17ms, tokens/mssec: 33.14\n",
      "for iter: 45, loss: 6.07133150100708, norm: 0.00 time: 238.21ms, tokens/mssec: 34.39\n",
      "for iter: 46, loss: 6.19002628326416, norm: 0.00 time: 241.41ms, tokens/mssec: 33.93\n",
      "for iter: 47, loss: 6.14820671081543, norm: 0.00 time: 238.02ms, tokens/mssec: 34.42\n",
      "for iter: 48, loss: 6.043972015380859, norm: 0.00 time: 238.05ms, tokens/mssec: 34.41\n",
      "for iter: 49, loss: 5.8907341957092285, norm: 0.00 time: 245.56ms, tokens/mssec: 33.36\n",
      "for iter: 50, loss: 6.099894046783447, norm: 0.00 time: 246.11ms, tokens/mssec: 33.29\n",
      "for iter: 51, loss: 6.177635192871094, norm: 0.00 time: 242.45ms, tokens/mssec: 33.79\n",
      "for iter: 52, loss: 6.279587745666504, norm: 0.00 time: 240.52ms, tokens/mssec: 34.06\n",
      "for iter: 53, loss: 6.356354713439941, norm: 0.00 time: 241.87ms, tokens/mssec: 33.87\n",
      "for iter: 54, loss: 6.24679708480835, norm: 0.00 time: 243.08ms, tokens/mssec: 33.70\n",
      "for iter: 55, loss: 6.238012313842773, norm: 0.00 time: 240.42ms, tokens/mssec: 34.07\n",
      "for iter: 56, loss: 6.072169303894043, norm: 0.00 time: 238.23ms, tokens/mssec: 34.39\n",
      "for iter: 57, loss: 6.0782575607299805, norm: 0.00 time: 244.97ms, tokens/mssec: 33.44\n",
      "for iter: 58, loss: 6.152736186981201, norm: 0.00 time: 240.54ms, tokens/mssec: 34.06\n",
      "for iter: 59, loss: 6.081121444702148, norm: 0.00 time: 239.09ms, tokens/mssec: 34.26\n",
      "for iter: 60, loss: 5.959504127502441, norm: 0.00 time: 241.00ms, tokens/mssec: 33.99\n",
      "for iter: 61, loss: 6.0602216720581055, norm: 0.00 time: 240.64ms, tokens/mssec: 34.04\n",
      "for iter: 62, loss: 5.937344074249268, norm: 0.00 time: 240.59ms, tokens/mssec: 34.05\n",
      "for iter: 63, loss: 6.134882926940918, norm: 0.00 time: 239.44ms, tokens/mssec: 34.21\n",
      "for iter: 64, loss: 5.926340103149414, norm: 0.00 time: 248.99ms, tokens/mssec: 32.90\n",
      "for iter: 65, loss: 5.913680076599121, norm: 0.00 time: 249.55ms, tokens/mssec: 32.83\n",
      "for iter: 66, loss: 6.011763572692871, norm: 0.00 time: 243.94ms, tokens/mssec: 33.58\n",
      "for iter: 67, loss: 6.213912010192871, norm: 0.00 time: 240.79ms, tokens/mssec: 34.02\n",
      "for iter: 68, loss: 6.101523399353027, norm: 0.00 time: 243.66ms, tokens/mssec: 33.62\n",
      "for iter: 69, loss: 6.382045745849609, norm: 0.00 time: 242.29ms, tokens/mssec: 33.81\n",
      "for iter: 70, loss: 6.1326446533203125, norm: 0.00 time: 250.17ms, tokens/mssec: 32.75\n",
      "for iter: 71, loss: 6.123112678527832, norm: 0.00 time: 239.81ms, tokens/mssec: 34.16\n",
      "for iter: 72, loss: 6.0982208251953125, norm: 0.00 time: 244.42ms, tokens/mssec: 33.52\n",
      "for iter: 73, loss: 5.9861249923706055, norm: 0.00 time: 238.52ms, tokens/mssec: 34.35\n",
      "for iter: 74, loss: 6.204673767089844, norm: 0.00 time: 241.41ms, tokens/mssec: 33.93\n",
      "for iter: 75, loss: 6.072427272796631, norm: 0.00 time: 242.03ms, tokens/mssec: 33.85\n",
      "for iter: 76, loss: 6.035978317260742, norm: 0.00 time: 242.67ms, tokens/mssec: 33.76\n",
      "for iter: 77, loss: 6.035229682922363, norm: 0.00 time: 236.65ms, tokens/mssec: 34.62\n",
      "for iter: 78, loss: 6.039543151855469, norm: 0.00 time: 242.00ms, tokens/mssec: 33.85\n",
      "for iter: 79, loss: 5.846870422363281, norm: 0.00 time: 230.81ms, tokens/mssec: 35.49\n",
      "for iter: 80, loss: 5.852790355682373, norm: 0.00 time: 237.65ms, tokens/mssec: 34.47\n",
      "for iter: 81, loss: 6.1980204582214355, norm: 0.00 time: 241.52ms, tokens/mssec: 33.92\n"
     ]
    }
   ],
   "source": [
    "transformer_model.train()\n",
    "for iter in range(total_iteration):\n",
    "    start_time = time.time()\n",
    "    x, y = train_loader.next_batch()\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    with ctx:\n",
    "        logits, loss = transformer_model(x, y)\n",
    "\n",
    "    # backward pass with GradScaler\n",
    "    scaler.scale(loss).backward()\n",
    "    norm = 0.0\n",
    "    if grad_clip != 0.0:\n",
    "        # unscales the gradients of optimizer's assigned params in-place\n",
    "        scaler.unscale_(optimizer)\n",
    "        # clip the gradients\n",
    "        norm = torch.nn.utils.clip_grad_norm(transformer_model.parameters(), grad_clip)\n",
    "\n",
    "    # Unscales the gradients and calls optimizer.step()\n",
    "    scaler.step(optimizer)\n",
    "    # Updates the scale for next iteration\n",
    "    scaler.update()\n",
    "    end_time = time.time()\n",
    "    torch.cuda.empty_cache()  # clear the cache\n",
    "    torch.cuda.synchronize()  # wait for the computation to be done\n",
    "    time_elapsed = (end_time - start_time) * 1000\n",
    "    token_persec = x.numel() / time_elapsed\n",
    "    print(\n",
    "        f\"for iter: {iter}, loss: {loss.item()}, norm: {norm:.2f} time: {time_elapsed:.2f}ms, tokens/mssec: {token_persec:.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fail in increasing the context size due to memory issue in the GPU. We will try to use the gradien accumulation to solve this issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Optimizations by GPT3 paper -II\n",
    "* Tweaking the learning rate for optimizer using cosine decay with initial warmup\n",
    "* Application of gradient accumulation to increase the batch size without increasing the memory usage\n",
    "*  Optimized Adam W optimizer with added flexibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Define the get_lr function\n",
    "def get_lr(it, warmup_iters=500, learning_rate=0.1, lr_decay_iters=1500, min_lr=0.01):\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * (it + 1) / warmup_iters\n",
    "    if it > lr_decay_iters:\n",
    "        return min_lr\n",
    "\n",
    "    # Cosine annealing learning rate schedule\n",
    "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
    "    return min_lr + coeff * (learning_rate - min_lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate the learning rate values for 2000 iterations\n",
    "iterations = np.arange(2000)\n",
    "learning_rates = [get_lr(it) for it in iterations]\n",
    "\n",
    "# Plot the learning rate schedule\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(iterations, learning_rates, label=\"Learning Rate\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.title(\"Cosine Annealing Learning Rate Schedule over Iterations\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the graph of the learning rate schedule over 2000 iterations:\n",
    "\n",
    "* Linear Warmup Phase: From iteration 0 to 500, the learning rate increases linearly from 0 to 0.1.\n",
    "* Cosine Decay Phase: From iteration 500 to 1500, the learning rate decreases following a cosine curve down to 0.01.\n",
    "* Constant Minimum Learning Rate: After iteration 1500, the learning rate remains constant at 0.01."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LR scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lr = 6e-4\n",
    "min_lr = max_lr * 0.3\n",
    "warmup_steps = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using float16 for automatic mixed precision training\n"
     ]
    }
   ],
   "source": [
    "# this should be run first\n",
    "dtype = (\n",
    "    \"bfloat16\"\n",
    "    if torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "    else \"float16\"\n",
    ")\n",
    "print(f\"Using {dtype} for automatic mixed precision training\")\n",
    "\n",
    "# note: float16 data type will automatically use a GradScaler\n",
    "ptdtype = {\n",
    "    \"float32\": torch.float32,\n",
    "    \"bfloat16\": torch.bfloat16,\n",
    "    \"float16\": torch.float16,\n",
    "}[dtype]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# permisssion for grad_clip\n",
    "grad_clip = 0.0\n",
    "\n",
    "# model compilation\n",
    "compile = True\n",
    "\n",
    "# context manager for automatic mixed precision training\n",
    "ctx = (\n",
    "    nullcontext()\n",
    "    if device_type == \"cpu\"\n",
    "    else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    ")\n",
    "\n",
    "if ctx != nullcontext():\n",
    "    # GradScaler for automatic mixed precision training\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/torch_env/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    }
   ],
   "source": [
    "transformer_model = GPT(\n",
    "    GPTConfig(\n",
    "        vocab_size=50304,\n",
    "        block_size=2048,\n",
    "        n_layer=12,\n",
    "        n_head=12,\n",
    "        n_embd=768,\n",
    "    )\n",
    ")\n",
    "transformer_model.to(device)\n",
    "\n",
    "if compile:\n",
    "    transformer_model = torch.compile(transformer_model)\n",
    "\n",
    "# change the optimizer to AdamW\n",
    "optimizer = torch.optim.AdamW(\n",
    "    transformer_model.parameters(), lr=3e-4, betas=(0.9, 0.95), eps=1e-8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens in the file: 338025\n",
      "1 epoch will have 41 iterations given Batch size=8 and Context length=1024\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataloaderLiter(B=8, T=1024, encoder=encoder, filename=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total iterations: 82\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 2\n",
    "initial_iterations = 100\n",
    "total_iteration = num_epochs * train_loader.num_iterations\n",
    "print(f\"Total iterations: {total_iteration}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for iter: 0 | loss: 10.953805923461914 | norm: 0.00 | lr 6.0000e-05 | time: 30262.87ms | tokens/mssec: 0.27\n",
      "for iter: 1 | loss: 9.560873031616211 | norm: 0.00 | lr 1.2000e-04 | time: 155.52ms | tokens/mssec: 52.68\n",
      "for iter: 2 | loss: 9.007200241088867 | norm: 0.00 | lr 1.8000e-04 | time: 209.07ms | tokens/mssec: 39.18\n",
      "for iter: 3 | loss: 9.17491340637207 | norm: 0.00 | lr 2.4000e-04 | time: 176.58ms | tokens/mssec: 46.39\n",
      "for iter: 4 | loss: 8.8216552734375 | norm: 0.00 | lr 3.0000e-04 | time: 157.58ms | tokens/mssec: 51.99\n",
      "for iter: 5 | loss: 8.841644287109375 | norm: 0.00 | lr 3.6000e-04 | time: 156.14ms | tokens/mssec: 52.47\n",
      "for iter: 6 | loss: 8.685921669006348 | norm: 0.00 | lr 4.2000e-04 | time: 157.29ms | tokens/mssec: 52.08\n",
      "for iter: 7 | loss: 8.220111846923828 | norm: 0.00 | lr 4.8000e-04 | time: 157.20ms | tokens/mssec: 52.11\n",
      "for iter: 8 | loss: 7.750452995300293 | norm: 0.00 | lr 5.4000e-04 | time: 158.24ms | tokens/mssec: 51.77\n",
      "for iter: 9 | loss: 7.584262847900391 | norm: 0.00 | lr 6.0000e-04 | time: 157.42ms | tokens/mssec: 52.04\n",
      "for iter: 10 | loss: 7.506321907043457 | norm: 0.00 | lr 6.0000e-04 | time: 157.32ms | tokens/mssec: 52.07\n",
      "for iter: 11 | loss: 7.2777557373046875 | norm: 0.00 | lr 5.9966e-04 | time: 157.62ms | tokens/mssec: 51.97\n",
      "for iter: 12 | loss: 7.1533708572387695 | norm: 0.00 | lr 5.9863e-04 | time: 157.04ms | tokens/mssec: 52.16\n",
      "for iter: 13 | loss: 6.921308517456055 | norm: 0.00 | lr 5.9692e-04 | time: 157.90ms | tokens/mssec: 51.88\n",
      "for iter: 14 | loss: 6.830291748046875 | norm: 0.00 | lr 5.9454e-04 | time: 158.68ms | tokens/mssec: 51.62\n",
      "for iter: 15 | loss: 6.683991432189941 | norm: 0.00 | lr 5.9149e-04 | time: 158.48ms | tokens/mssec: 51.69\n",
      "for iter: 16 | loss: 6.795071125030518 | norm: 0.00 | lr 5.8779e-04 | time: 160.06ms | tokens/mssec: 51.18\n",
      "for iter: 17 | loss: 6.839974403381348 | norm: 0.00 | lr 5.8344e-04 | time: 158.06ms | tokens/mssec: 51.83\n",
      "for iter: 18 | loss: 6.747361183166504 | norm: 0.00 | lr 5.7845e-04 | time: 158.16ms | tokens/mssec: 51.79\n",
      "for iter: 19 | loss: 6.596924781799316 | norm: 0.00 | lr 5.7286e-04 | time: 157.31ms | tokens/mssec: 52.08\n",
      "for iter: 20 | loss: 6.712362289428711 | norm: 0.00 | lr 5.6666e-04 | time: 157.65ms | tokens/mssec: 51.96\n",
      "for iter: 21 | loss: 6.552613735198975 | norm: 0.00 | lr 5.5989e-04 | time: 159.93ms | tokens/mssec: 51.22\n",
      "for iter: 22 | loss: 6.657632827758789 | norm: 0.00 | lr 5.5257e-04 | time: 174.27ms | tokens/mssec: 47.01\n",
      "for iter: 23 | loss: 6.5128936767578125 | norm: 0.00 | lr 5.4472e-04 | time: 158.12ms | tokens/mssec: 51.81\n",
      "for iter: 24 | loss: 6.503092288970947 | norm: 0.00 | lr 5.3636e-04 | time: 156.20ms | tokens/mssec: 52.44\n",
      "for iter: 25 | loss: 6.564828395843506 | norm: 0.00 | lr 5.2752e-04 | time: 157.27ms | tokens/mssec: 52.09\n",
      "for iter: 26 | loss: 6.781094074249268 | norm: 0.00 | lr 5.1824e-04 | time: 157.76ms | tokens/mssec: 51.93\n",
      "for iter: 27 | loss: 6.646994590759277 | norm: 0.00 | lr 5.0853e-04 | time: 158.07ms | tokens/mssec: 51.83\n",
      "for iter: 28 | loss: 6.911450386047363 | norm: 0.00 | lr 4.9844e-04 | time: 158.90ms | tokens/mssec: 51.55\n",
      "for iter: 29 | loss: 6.676719665527344 | norm: 0.00 | lr 4.8800e-04 | time: 156.82ms | tokens/mssec: 52.24\n",
      "for iter: 30 | loss: 6.642228126525879 | norm: 0.00 | lr 4.7724e-04 | time: 157.01ms | tokens/mssec: 52.18\n",
      "for iter: 31 | loss: 6.641115188598633 | norm: 0.00 | lr 4.6619e-04 | time: 157.85ms | tokens/mssec: 51.90\n",
      "for iter: 32 | loss: 6.497488021850586 | norm: 0.00 | lr 4.5489e-04 | time: 158.67ms | tokens/mssec: 51.63\n",
      "for iter: 33 | loss: 6.788098335266113 | norm: 0.00 | lr 4.4339e-04 | time: 188.75ms | tokens/mssec: 43.40\n",
      "for iter: 34 | loss: 6.682186126708984 | norm: 0.00 | lr 4.3170e-04 | time: 157.22ms | tokens/mssec: 52.11\n",
      "for iter: 35 | loss: 6.525530815124512 | norm: 0.00 | lr 4.1989e-04 | time: 158.91ms | tokens/mssec: 51.55\n",
      "for iter: 36 | loss: 6.594789505004883 | norm: 0.00 | lr 4.0797e-04 | time: 159.15ms | tokens/mssec: 51.47\n",
      "for iter: 37 | loss: 6.595290184020996 | norm: 0.00 | lr 3.9600e-04 | time: 170.08ms | tokens/mssec: 48.16\n",
      "for iter: 38 | loss: 6.367734432220459 | norm: 0.00 | lr 3.8400e-04 | time: 159.24ms | tokens/mssec: 51.44\n",
      "for iter: 39 | loss: 6.420393943786621 | norm: 0.00 | lr 3.7203e-04 | time: 156.13ms | tokens/mssec: 52.47\n",
      "for iter: 40 | loss: 6.647234916687012 | norm: 0.00 | lr 3.6011e-04 | time: 157.25ms | tokens/mssec: 52.10\n",
      "for iter: 41 | loss: 6.419949531555176 | norm: 0.00 | lr 3.4830e-04 | time: 155.34ms | tokens/mssec: 52.74\n",
      "for iter: 42 | loss: 6.471226692199707 | norm: 0.00 | lr 3.3661e-04 | time: 163.78ms | tokens/mssec: 50.02\n",
      "for iter: 43 | loss: 6.183071136474609 | norm: 0.00 | lr 3.2511e-04 | time: 168.08ms | tokens/mssec: 48.74\n",
      "for iter: 44 | loss: 6.115142822265625 | norm: 0.00 | lr 3.1381e-04 | time: 259.69ms | tokens/mssec: 31.55\n",
      "for iter: 45 | loss: 6.192231178283691 | norm: 0.00 | lr 3.0276e-04 | time: 159.48ms | tokens/mssec: 51.37\n",
      "for iter: 46 | loss: 6.2792463302612305 | norm: 0.00 | lr 2.9200e-04 | time: 161.77ms | tokens/mssec: 50.64\n",
      "for iter: 47 | loss: 6.209145545959473 | norm: 0.00 | lr 2.8156e-04 | time: 157.72ms | tokens/mssec: 51.94\n",
      "for iter: 48 | loss: 6.106249809265137 | norm: 0.00 | lr 2.7147e-04 | time: 158.14ms | tokens/mssec: 51.80\n",
      "for iter: 49 | loss: 5.977999687194824 | norm: 0.00 | lr 2.6176e-04 | time: 169.83ms | tokens/mssec: 48.24\n",
      "for iter: 50 | loss: 6.180410385131836 | norm: 0.00 | lr 2.5248e-04 | time: 171.42ms | tokens/mssec: 47.79\n",
      "for iter: 51 | loss: 6.247197151184082 | norm: 0.00 | lr 2.4364e-04 | time: 157.17ms | tokens/mssec: 52.12\n",
      "for iter: 52 | loss: 6.350337982177734 | norm: 0.00 | lr 2.3528e-04 | time: 159.61ms | tokens/mssec: 51.32\n",
      "for iter: 53 | loss: 6.439146041870117 | norm: 0.00 | lr 2.2743e-04 | time: 158.16ms | tokens/mssec: 51.80\n",
      "for iter: 54 | loss: 6.329429626464844 | norm: 0.00 | lr 2.2011e-04 | time: 176.94ms | tokens/mssec: 46.30\n",
      "for iter: 55 | loss: 6.325358867645264 | norm: 0.00 | lr 2.1334e-04 | time: 175.70ms | tokens/mssec: 46.62\n",
      "for iter: 56 | loss: 6.161197185516357 | norm: 0.00 | lr 2.0714e-04 | time: 157.58ms | tokens/mssec: 51.98\n",
      "for iter: 57 | loss: 6.130453109741211 | norm: 0.00 | lr 2.0155e-04 | time: 157.37ms | tokens/mssec: 52.06\n",
      "for iter: 58 | loss: 6.217911720275879 | norm: 0.00 | lr 1.9656e-04 | time: 158.33ms | tokens/mssec: 51.74\n",
      "for iter: 59 | loss: 6.14848518371582 | norm: 0.00 | lr 1.9221e-04 | time: 156.32ms | tokens/mssec: 52.41\n",
      "for iter: 60 | loss: 6.043618202209473 | norm: 0.00 | lr 1.8851e-04 | time: 158.49ms | tokens/mssec: 51.69\n",
      "for iter: 61 | loss: 6.153234958648682 | norm: 0.00 | lr 1.8546e-04 | time: 175.11ms | tokens/mssec: 46.78\n",
      "for iter: 62 | loss: 6.053190231323242 | norm: 0.00 | lr 1.8308e-04 | time: 159.22ms | tokens/mssec: 51.45\n",
      "for iter: 63 | loss: 6.214770793914795 | norm: 0.00 | lr 1.8137e-04 | time: 157.36ms | tokens/mssec: 52.06\n",
      "for iter: 64 | loss: 6.03493595123291 | norm: 0.00 | lr 1.8034e-04 | time: 157.77ms | tokens/mssec: 51.92\n",
      "for iter: 65 | loss: 6.031651973724365 | norm: 0.00 | lr 1.8000e-04 | time: 163.05ms | tokens/mssec: 50.24\n",
      "for iter: 66 | loss: 6.137974739074707 | norm: 0.00 | lr 1.8000e-04 | time: 159.56ms | tokens/mssec: 51.34\n",
      "for iter: 67 | loss: 6.302643299102783 | norm: 0.00 | lr 1.8000e-04 | time: 179.46ms | tokens/mssec: 45.65\n",
      "for iter: 68 | loss: 6.203575134277344 | norm: 0.00 | lr 1.8000e-04 | time: 168.12ms | tokens/mssec: 48.73\n",
      "for iter: 69 | loss: 6.480434417724609 | norm: 0.00 | lr 1.8000e-04 | time: 157.82ms | tokens/mssec: 51.91\n",
      "for iter: 70 | loss: 6.25969934463501 | norm: 0.00 | lr 1.8000e-04 | time: 158.62ms | tokens/mssec: 51.65\n",
      "for iter: 71 | loss: 6.265704154968262 | norm: 0.00 | lr 1.8000e-04 | time: 157.23ms | tokens/mssec: 52.10\n",
      "for iter: 72 | loss: 6.191371917724609 | norm: 0.00 | lr 1.8000e-04 | time: 157.79ms | tokens/mssec: 51.92\n",
      "for iter: 73 | loss: 6.067281246185303 | norm: 0.00 | lr 1.8000e-04 | time: 191.75ms | tokens/mssec: 42.72\n",
      "for iter: 74 | loss: 6.321318626403809 | norm: 0.00 | lr 1.8000e-04 | time: 157.13ms | tokens/mssec: 52.14\n",
      "for iter: 75 | loss: 6.201733589172363 | norm: 0.00 | lr 1.8000e-04 | time: 161.37ms | tokens/mssec: 50.76\n",
      "for iter: 76 | loss: 6.162078857421875 | norm: 0.00 | lr 1.8000e-04 | time: 158.60ms | tokens/mssec: 51.65\n",
      "for iter: 77 | loss: 6.161685943603516 | norm: 0.00 | lr 1.8000e-04 | time: 158.19ms | tokens/mssec: 51.78\n",
      "for iter: 78 | loss: 6.175293922424316 | norm: 0.00 | lr 1.8000e-04 | time: 158.33ms | tokens/mssec: 51.74\n",
      "for iter: 79 | loss: 5.995906829833984 | norm: 0.00 | lr 1.8000e-04 | time: 157.63ms | tokens/mssec: 51.97\n",
      "for iter: 80 | loss: 6.007894515991211 | norm: 0.00 | lr 1.8000e-04 | time: 157.83ms | tokens/mssec: 51.91\n",
      "for iter: 81 | loss: 6.300731182098389 | norm: 0.00 | lr 1.8000e-04 | time: 158.74ms | tokens/mssec: 51.61\n"
     ]
    }
   ],
   "source": [
    "transformer_model.train()\n",
    "for iter in range(total_iteration):\n",
    "    start_time = time.time()\n",
    "    x, y = train_loader.next_batch()\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    with ctx:\n",
    "        logits, loss = transformer_model(x, y)\n",
    "\n",
    "    # backward pass with GradScaler\n",
    "    scaler.scale(loss).backward()\n",
    "    norm = 0.0\n",
    "    if grad_clip != 0.0:\n",
    "        # unscales the gradients of optimizer's assigned params in-place\n",
    "        scaler.unscale_(optimizer)\n",
    "        # clip the gradients\n",
    "        norm = torch.nn.utils.clip_grad_norm(transformer_model.parameters(), grad_clip)\n",
    "\n",
    "    # get the current learning rate\n",
    "    lr = get_lr(iter, warmup_steps, max_lr, int(total_iteration * 0.8), min_lr)\n",
    "    # update the learning rate\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = lr\n",
    "\n",
    "    # Unscales the gradients and calls optimizer.step()\n",
    "    scaler.step(optimizer)\n",
    "    # Updates the scale for next iteration\n",
    "    scaler.update()\n",
    "    end_time = time.time()\n",
    "    torch.cuda.empty_cache()  # clear the cache\n",
    "    torch.cuda.synchronize()  # wait for the computation to be done\n",
    "    time_elapsed = (end_time - start_time) * 1000\n",
    "    token_persec = x.numel() / time_elapsed\n",
    "    print(\n",
    "        f\"for iter: {iter} | loss: {loss.item()} | norm: {norm:.2f} | lr {lr:.4e} | time: {time_elapsed:.2f}ms | tokens/mssec: {token_persec:.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ADAMW optimizer with weight decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_optimizers(model, weight_decay, learning_rate, device_type):\n",
    "    \"\"\"Create the optimizer and scheduler for training\"\"\"\n",
    "\n",
    "    # start with all of the candidate parameters (that require grad)\n",
    "    param_dict = {pn: p for pn, p in model.named_parameters()}\n",
    "    param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "\n",
    "    # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
    "    # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
    "    decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "    nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "    optim_groups = [\n",
    "        {\"params\": decay_params, \"weight_decay\": weight_decay},\n",
    "        {\"params\": nodecay_params, \"weight_decay\": 0.0},\n",
    "    ]\n",
    "\n",
    "    num_decay_params = sum(p.numel() for p in decay_params)\n",
    "    num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "\n",
    "    print(\n",
    "        f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\"\n",
    "    )\n",
    "    # Create AdamW optimizer and use the fused version if it is available\n",
    "    fused_available = \"fused\" in inspect.signature(torch.optim.AdamW).parameters\n",
    "    use_fused = fused_available and device_type == \"cuda\"\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        optim_groups, lr=learning_rate, betas=(0.9, 0.95), eps=1e-8, fused=use_fused\n",
    "    )\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# permisssion for grad_clip\n",
    "grad_clip = 0.0\n",
    "\n",
    "# model compilation\n",
    "compile = True\n",
    "\n",
    "# context manager for automatic mixed precision training\n",
    "ctx = (\n",
    "    nullcontext()\n",
    "    if device_type == \"cpu\"\n",
    "    else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    ")\n",
    "\n",
    "if ctx != nullcontext():\n",
    "    # GradScaler for automatic mixed precision training\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model = GPT(\n",
    "    GPTConfig(\n",
    "        vocab_size=50304,\n",
    "        block_size=2048,\n",
    "        n_layer=12,\n",
    "        n_head=12,\n",
    "        n_embd=768,\n",
    "    )\n",
    ")\n",
    "transformer_model.to(device)\n",
    "\n",
    "if compile:\n",
    "    transformer_model = torch.compile(transformer_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num decayed parameter tensors: 50, with 125,140,992 parameters\n"
     ]
    }
   ],
   "source": [
    "optimizer = configure_optimizers(transformer_model, 0.1, max_lr, device_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens in the file: 338025\n",
      "1 epoch will have 41 iterations given Batch size=8 and Context length=1024\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataloaderLiter(B=8, T=1024, encoder=encoder, filename=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total iterations: 82\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 2\n",
    "initial_iterations = 100\n",
    "total_iteration = num_epochs * train_loader.num_iterations\n",
    "print(f\"Total iterations: {total_iteration}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for iter: 0 | loss: 10.970207214355469 | norm: 0.00 | lr 6.0000e-05 | time: 23852.85ms | tokens/mssec: 0.34\n",
      "for iter: 1 | loss: 9.622097969055176 | norm: 0.00 | lr 1.2000e-04 | time: 34.38ms | tokens/mssec: 238.29\n",
      "for iter: 2 | loss: 9.034076690673828 | norm: 0.00 | lr 1.8000e-04 | time: 90.47ms | tokens/mssec: 90.55\n",
      "for iter: 3 | loss: 9.174388885498047 | norm: 0.00 | lr 2.4000e-04 | time: 86.06ms | tokens/mssec: 95.19\n",
      "for iter: 4 | loss: 8.857473373413086 | norm: 0.00 | lr 3.0000e-04 | time: 163.88ms | tokens/mssec: 49.99\n",
      "for iter: 5 | loss: 8.646048545837402 | norm: 0.00 | lr 3.6000e-04 | time: 66.54ms | tokens/mssec: 123.11\n",
      "for iter: 6 | loss: 8.544068336486816 | norm: 0.00 | lr 4.2000e-04 | time: 95.97ms | tokens/mssec: 85.36\n",
      "for iter: 7 | loss: 8.18879508972168 | norm: 0.00 | lr 4.8000e-04 | time: 65.38ms | tokens/mssec: 125.31\n",
      "for iter: 8 | loss: 7.78149938583374 | norm: 0.00 | lr 5.4000e-04 | time: 91.21ms | tokens/mssec: 89.82\n",
      "for iter: 9 | loss: 7.609959602355957 | norm: 0.00 | lr 6.0000e-04 | time: 62.98ms | tokens/mssec: 130.08\n",
      "for iter: 10 | loss: 7.489616394042969 | norm: 0.00 | lr 6.0000e-04 | time: 66.13ms | tokens/mssec: 123.88\n",
      "for iter: 11 | loss: 7.27499532699585 | norm: 0.00 | lr 5.9966e-04 | time: 72.67ms | tokens/mssec: 112.73\n",
      "for iter: 12 | loss: 7.183167457580566 | norm: 0.00 | lr 5.9863e-04 | time: 98.39ms | tokens/mssec: 83.26\n",
      "for iter: 13 | loss: 6.9465837478637695 | norm: 0.00 | lr 5.9692e-04 | time: 66.86ms | tokens/mssec: 122.52\n",
      "for iter: 14 | loss: 6.837279796600342 | norm: 0.00 | lr 5.9454e-04 | time: 71.92ms | tokens/mssec: 113.90\n",
      "for iter: 15 | loss: 6.659013748168945 | norm: 0.00 | lr 5.9149e-04 | time: 70.97ms | tokens/mssec: 115.43\n",
      "for iter: 16 | loss: 6.773153781890869 | norm: 0.00 | lr 5.8779e-04 | time: 70.15ms | tokens/mssec: 116.79\n",
      "for iter: 17 | loss: 6.80278205871582 | norm: 0.00 | lr 5.8344e-04 | time: 64.78ms | tokens/mssec: 126.46\n",
      "for iter: 18 | loss: 6.728963851928711 | norm: 0.00 | lr 5.7845e-04 | time: 74.43ms | tokens/mssec: 110.06\n",
      "for iter: 19 | loss: 6.580937385559082 | norm: 0.00 | lr 5.7286e-04 | time: 92.47ms | tokens/mssec: 88.59\n",
      "for iter: 20 | loss: 6.696535587310791 | norm: 0.00 | lr 5.6666e-04 | time: 92.45ms | tokens/mssec: 88.61\n",
      "for iter: 21 | loss: 6.557366371154785 | norm: 0.00 | lr 5.5989e-04 | time: 75.44ms | tokens/mssec: 108.59\n",
      "for iter: 22 | loss: 6.663853645324707 | norm: 0.00 | lr 5.5257e-04 | time: 71.21ms | tokens/mssec: 115.04\n",
      "for iter: 23 | loss: 6.541963577270508 | norm: 0.00 | lr 5.4472e-04 | time: 74.09ms | tokens/mssec: 110.58\n",
      "for iter: 24 | loss: 6.516768455505371 | norm: 0.00 | lr 5.3636e-04 | time: 97.98ms | tokens/mssec: 83.61\n",
      "for iter: 25 | loss: 6.5533881187438965 | norm: 0.00 | lr 5.2752e-04 | time: 109.15ms | tokens/mssec: 75.05\n",
      "for iter: 26 | loss: 6.784621238708496 | norm: 0.00 | lr 5.1824e-04 | time: 76.66ms | tokens/mssec: 106.87\n",
      "for iter: 27 | loss: 6.653712272644043 | norm: 0.00 | lr 5.0853e-04 | time: 69.34ms | tokens/mssec: 118.14\n",
      "for iter: 28 | loss: 6.918757915496826 | norm: 0.00 | lr 4.9844e-04 | time: 74.38ms | tokens/mssec: 110.14\n",
      "for iter: 29 | loss: 6.76171875 | norm: 0.00 | lr 4.8800e-04 | time: 74.38ms | tokens/mssec: 110.13\n",
      "for iter: 30 | loss: 6.648044109344482 | norm: 0.00 | lr 4.7724e-04 | time: 73.63ms | tokens/mssec: 111.26\n",
      "for iter: 31 | loss: 6.665220737457275 | norm: 0.00 | lr 4.6619e-04 | time: 68.66ms | tokens/mssec: 119.32\n",
      "for iter: 32 | loss: 6.492762088775635 | norm: 0.00 | lr 4.5489e-04 | time: 84.46ms | tokens/mssec: 96.99\n",
      "for iter: 33 | loss: 6.821838855743408 | norm: 0.00 | lr 4.4339e-04 | time: 76.47ms | tokens/mssec: 107.12\n",
      "for iter: 34 | loss: 6.659746170043945 | norm: 0.00 | lr 4.3170e-04 | time: 71.62ms | tokens/mssec: 114.38\n",
      "for iter: 35 | loss: 6.549410820007324 | norm: 0.00 | lr 4.1989e-04 | time: 66.23ms | tokens/mssec: 123.68\n",
      "for iter: 36 | loss: 6.605216026306152 | norm: 0.00 | lr 4.0797e-04 | time: 83.68ms | tokens/mssec: 97.90\n",
      "for iter: 37 | loss: 6.620340347290039 | norm: 0.00 | lr 3.9600e-04 | time: 71.98ms | tokens/mssec: 113.81\n",
      "for iter: 38 | loss: 6.38784122467041 | norm: 0.00 | lr 3.8400e-04 | time: 94.08ms | tokens/mssec: 87.08\n",
      "for iter: 39 | loss: 6.439962387084961 | norm: 0.00 | lr 3.7203e-04 | time: 68.83ms | tokens/mssec: 119.02\n",
      "for iter: 40 | loss: 6.649209022521973 | norm: 0.00 | lr 3.6011e-04 | time: 72.86ms | tokens/mssec: 112.44\n",
      "for iter: 41 | loss: 6.425456523895264 | norm: 0.00 | lr 3.4830e-04 | time: 66.14ms | tokens/mssec: 123.87\n",
      "for iter: 42 | loss: 6.46661376953125 | norm: 0.00 | lr 3.3661e-04 | time: 74.33ms | tokens/mssec: 110.22\n",
      "for iter: 43 | loss: 6.188109874725342 | norm: 0.00 | lr 3.2511e-04 | time: 97.63ms | tokens/mssec: 83.91\n",
      "for iter: 44 | loss: 6.100093841552734 | norm: 0.00 | lr 3.1381e-04 | time: 82.51ms | tokens/mssec: 99.29\n",
      "for iter: 45 | loss: 6.179200172424316 | norm: 0.00 | lr 3.0276e-04 | time: 65.12ms | tokens/mssec: 125.79\n",
      "for iter: 46 | loss: 6.271406650543213 | norm: 0.00 | lr 2.9200e-04 | time: 71.93ms | tokens/mssec: 113.89\n",
      "for iter: 47 | loss: 6.215875625610352 | norm: 0.00 | lr 2.8156e-04 | time: 71.91ms | tokens/mssec: 113.92\n",
      "for iter: 48 | loss: 6.116921424865723 | norm: 0.00 | lr 2.7147e-04 | time: 75.61ms | tokens/mssec: 108.35\n",
      "for iter: 49 | loss: 5.98529052734375 | norm: 0.00 | lr 2.6176e-04 | time: 68.85ms | tokens/mssec: 118.99\n",
      "for iter: 50 | loss: 6.197784423828125 | norm: 0.00 | lr 2.5248e-04 | time: 93.54ms | tokens/mssec: 87.58\n",
      "for iter: 51 | loss: 6.2641143798828125 | norm: 0.00 | lr 2.4364e-04 | time: 72.56ms | tokens/mssec: 112.90\n",
      "for iter: 52 | loss: 6.366784572601318 | norm: 0.00 | lr 2.3528e-04 | time: 78.41ms | tokens/mssec: 104.48\n",
      "for iter: 53 | loss: 6.4581756591796875 | norm: 0.00 | lr 2.2743e-04 | time: 49.19ms | tokens/mssec: 166.53\n",
      "for iter: 54 | loss: 6.340450286865234 | norm: 0.00 | lr 2.2011e-04 | time: 70.54ms | tokens/mssec: 116.13\n",
      "for iter: 55 | loss: 6.332756996154785 | norm: 0.00 | lr 2.1334e-04 | time: 66.02ms | tokens/mssec: 124.08\n",
      "for iter: 56 | loss: 6.163427352905273 | norm: 0.00 | lr 2.0714e-04 | time: 71.48ms | tokens/mssec: 114.61\n",
      "for iter: 57 | loss: 6.159090995788574 | norm: 0.00 | lr 2.0155e-04 | time: 70.67ms | tokens/mssec: 115.92\n",
      "for iter: 58 | loss: 6.245142459869385 | norm: 0.00 | lr 1.9656e-04 | time: 71.17ms | tokens/mssec: 115.11\n",
      "for iter: 59 | loss: 6.186873435974121 | norm: 0.00 | lr 1.9221e-04 | time: 71.82ms | tokens/mssec: 114.07\n",
      "for iter: 60 | loss: 6.0735859870910645 | norm: 0.00 | lr 1.8851e-04 | time: 68.62ms | tokens/mssec: 119.38\n",
      "for iter: 61 | loss: 6.176302909851074 | norm: 0.00 | lr 1.8546e-04 | time: 64.18ms | tokens/mssec: 127.64\n",
      "for iter: 62 | loss: 6.051211357116699 | norm: 0.00 | lr 1.8308e-04 | time: 67.29ms | tokens/mssec: 121.74\n",
      "for iter: 63 | loss: 6.214890003204346 | norm: 0.00 | lr 1.8137e-04 | time: 76.18ms | tokens/mssec: 107.54\n",
      "for iter: 64 | loss: 6.045392990112305 | norm: 0.00 | lr 1.8034e-04 | time: 64.51ms | tokens/mssec: 127.00\n",
      "for iter: 65 | loss: 6.032772064208984 | norm: 0.00 | lr 1.8000e-04 | time: 73.78ms | tokens/mssec: 111.04\n",
      "for iter: 66 | loss: 6.120670318603516 | norm: 0.00 | lr 1.8000e-04 | time: 176.18ms | tokens/mssec: 46.50\n",
      "for iter: 67 | loss: 6.283823490142822 | norm: 0.00 | lr 1.8000e-04 | time: 69.01ms | tokens/mssec: 118.71\n",
      "for iter: 68 | loss: 6.180187702178955 | norm: 0.00 | lr 1.8000e-04 | time: 94.37ms | tokens/mssec: 86.81\n",
      "for iter: 69 | loss: 6.462647438049316 | norm: 0.00 | lr 1.8000e-04 | time: 76.56ms | tokens/mssec: 107.01\n",
      "for iter: 70 | loss: 6.226242542266846 | norm: 0.00 | lr 1.8000e-04 | time: 78.48ms | tokens/mssec: 104.39\n",
      "for iter: 71 | loss: 6.215731620788574 | norm: 0.00 | lr 1.8000e-04 | time: 43.04ms | tokens/mssec: 190.33\n",
      "for iter: 72 | loss: 6.146755218505859 | norm: 0.00 | lr 1.8000e-04 | time: 64.87ms | tokens/mssec: 126.28\n",
      "for iter: 73 | loss: 6.007407188415527 | norm: 0.00 | lr 1.8000e-04 | time: 77.58ms | tokens/mssec: 105.59\n",
      "for iter: 74 | loss: 6.278722763061523 | norm: 0.00 | lr 1.8000e-04 | time: 66.08ms | tokens/mssec: 123.98\n",
      "for iter: 75 | loss: 6.149584770202637 | norm: 0.00 | lr 1.8000e-04 | time: 76.27ms | tokens/mssec: 107.41\n",
      "for iter: 76 | loss: 6.12563419342041 | norm: 0.00 | lr 1.8000e-04 | time: 90.76ms | tokens/mssec: 90.26\n",
      "for iter: 77 | loss: 6.117756366729736 | norm: 0.00 | lr 1.8000e-04 | time: 73.84ms | tokens/mssec: 110.94\n",
      "for iter: 78 | loss: 6.158768653869629 | norm: 0.00 | lr 1.8000e-04 | time: 65.56ms | tokens/mssec: 124.95\n",
      "for iter: 79 | loss: 5.953133583068848 | norm: 0.00 | lr 1.8000e-04 | time: 97.52ms | tokens/mssec: 84.00\n",
      "for iter: 80 | loss: 5.971558570861816 | norm: 0.00 | lr 1.8000e-04 | time: 168.49ms | tokens/mssec: 48.62\n",
      "for iter: 81 | loss: 6.266635894775391 | norm: 0.00 | lr 1.8000e-04 | time: 76.06ms | tokens/mssec: 107.70\n"
     ]
    }
   ],
   "source": [
    "transformer_model.train()\n",
    "for iter in range(total_iteration):\n",
    "    start_time = time.time()\n",
    "    x, y = train_loader.next_batch()\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    with ctx:\n",
    "        logits, loss = transformer_model(x, y)\n",
    "\n",
    "    # backward pass with GradScaler\n",
    "    scaler.scale(loss).backward()\n",
    "    norm = 0.0\n",
    "    if grad_clip != 0.0:\n",
    "        # unscales the gradients of optimizer's assigned params in-place\n",
    "        scaler.unscale_(optimizer)\n",
    "        # clip the gradients\n",
    "        norm = torch.nn.utils.clip_grad_norm(transformer_model.parameters(), grad_clip)\n",
    "\n",
    "    # get the current learning rate\n",
    "    lr = get_lr(iter, warmup_steps, max_lr, int(total_iteration * 0.8), min_lr)\n",
    "    # update the learning rate\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = lr\n",
    "\n",
    "    # Unscales the gradients and calls optimizer.step()\n",
    "    scaler.step(optimizer)\n",
    "    # Updates the scale for next iteration\n",
    "    scaler.update()\n",
    "    end_time = time.time()\n",
    "    torch.cuda.empty_cache()  # clear the cache\n",
    "    torch.cuda.synchronize()  # wait for the computation to be done\n",
    "    time_elapsed = (end_time - start_time) * 1000\n",
    "    token_persec = x.numel() / time_elapsed\n",
    "    print(\n",
    "        f\"for iter: {iter} | loss: {loss.item()} | norm: {norm:.2f} | lr {lr:.4e} | time: {time_elapsed:.2f}ms | tokens/mssec: {token_persec:.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grad Accummulation: To use larger batch sizes, we can accumulate gradients over multiple iterations before updating the model parameters. This is useful when the model is too large to fit in the memory or when the batch size is too small to get a good gradient estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad_accumulation_steps: 64\n"
     ]
    }
   ],
   "source": [
    "ideal_batch_size = 524288  # as per GPT3 paper\n",
    "permit_batch = 8\n",
    "permit_context = 1024\n",
    "\n",
    "# grad_accumulation_steps = ideal_batch_size // (permit_batch * permit_context)\n",
    "grad_accumulation_steps = ideal_batch_size // (permit_batch * permit_context)\n",
    "print(f\"grad_accumulation_steps: {grad_accumulation_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# permisssion for grad_clip\n",
    "grad_clip = 0.0\n",
    "\n",
    "# model compilation\n",
    "compile = True\n",
    "\n",
    "# context manager for automatic mixed precision training\n",
    "ctx = (\n",
    "    nullcontext()\n",
    "    if device_type == \"cpu\"\n",
    "    else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    ")\n",
    "\n",
    "if ctx != nullcontext():\n",
    "    # GradScaler for automatic mixed precision training\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model = GPT(\n",
    "    GPTConfig(\n",
    "        vocab_size=50304,\n",
    "        block_size=2048,\n",
    "        n_layer=12,\n",
    "        n_head=12,\n",
    "        n_embd=768,\n",
    "    )\n",
    ")\n",
    "transformer_model.to(device)\n",
    "\n",
    "if compile:\n",
    "    transformer_model = torch.compile(transformer_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num decayed parameter tensors: 50, with 125,140,992 parameters\n"
     ]
    }
   ],
   "source": [
    "optimizer = configure_optimizers(transformer_model, 0.1, max_lr, device_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens in the file: 338025\n",
      "1 epoch will have 41 iterations given Batch size=8 and Context length=1024\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataloaderLiter(B=8, T=1024, encoder=encoder, filename=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total iterations: 82\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 2\n",
    "initial_iterations = 100\n",
    "total_iteration = num_epochs * train_loader.num_iterations\n",
    "print(f\"Total iterations: {total_iteration}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for iter: 0 | loss: 10.905730247497559 | norm: 0.00 | lr 6.0000e-05 | time: 34065.19ms | tokens/mssec: 15.39\n",
      "for iter: 1 | loss: 9.55672836303711 | norm: 0.00 | lr 1.2000e-04 | time: 9656.55ms | tokens/mssec: 54.29\n",
      "for iter: 2 | loss: 9.191079139709473 | norm: 0.00 | lr 1.8000e-04 | time: 9645.39ms | tokens/mssec: 54.36\n",
      "for iter: 3 | loss: 9.547054290771484 | norm: 0.00 | lr 2.4000e-04 | time: 9651.59ms | tokens/mssec: 54.32\n",
      "for iter: 4 | loss: 9.03136920928955 | norm: 0.00 | lr 3.0000e-04 | time: 9643.95ms | tokens/mssec: 54.36\n",
      "for iter: 5 | loss: 8.616280555725098 | norm: 0.00 | lr 3.6000e-04 | time: 9644.40ms | tokens/mssec: 54.36\n",
      "for iter: 6 | loss: 8.32386302947998 | norm: 0.00 | lr 4.2000e-04 | time: 9634.76ms | tokens/mssec: 54.42\n",
      "for iter: 7 | loss: 8.087150573730469 | norm: 0.00 | lr 4.8000e-04 | time: 9636.81ms | tokens/mssec: 54.40\n",
      "for iter: 8 | loss: 7.742105007171631 | norm: 0.00 | lr 5.4000e-04 | time: 9635.09ms | tokens/mssec: 54.41\n",
      "for iter: 9 | loss: 7.411349296569824 | norm: 0.00 | lr 6.0000e-04 | time: 9631.29ms | tokens/mssec: 54.44\n",
      "for iter: 10 | loss: 7.149709701538086 | norm: 0.00 | lr 6.0000e-04 | time: 9618.85ms | tokens/mssec: 54.51\n",
      "for iter: 11 | loss: 6.953516483306885 | norm: 0.00 | lr 5.9966e-04 | time: 9635.47ms | tokens/mssec: 54.41\n",
      "for iter: 12 | loss: 6.74509859085083 | norm: 0.00 | lr 5.9863e-04 | time: 9627.28ms | tokens/mssec: 54.46\n",
      "for iter: 13 | loss: 6.621782302856445 | norm: 0.00 | lr 5.9692e-04 | time: 9629.51ms | tokens/mssec: 54.45\n",
      "for iter: 14 | loss: 6.512859344482422 | norm: 0.00 | lr 5.9454e-04 | time: 9626.75ms | tokens/mssec: 54.46\n",
      "for iter: 15 | loss: 6.448185443878174 | norm: 0.00 | lr 5.9149e-04 | time: 9624.12ms | tokens/mssec: 54.48\n",
      "for iter: 16 | loss: 6.428586006164551 | norm: 0.00 | lr 5.8779e-04 | time: 9631.60ms | tokens/mssec: 54.43\n",
      "for iter: 17 | loss: 6.397754669189453 | norm: 0.00 | lr 5.8344e-04 | time: 9631.02ms | tokens/mssec: 54.44\n",
      "for iter: 18 | loss: 6.394838809967041 | norm: 0.00 | lr 5.7845e-04 | time: 9634.23ms | tokens/mssec: 54.42\n",
      "for iter: 19 | loss: 6.359443187713623 | norm: 0.00 | lr 5.7286e-04 | time: 9631.39ms | tokens/mssec: 54.44\n",
      "for iter: 20 | loss: 6.385376453399658 | norm: 0.00 | lr 5.6666e-04 | time: 9628.41ms | tokens/mssec: 54.45\n",
      "for iter: 21 | loss: 6.343880653381348 | norm: 0.00 | lr 5.5989e-04 | time: 9630.34ms | tokens/mssec: 54.44\n",
      "for iter: 22 | loss: 6.3355937004089355 | norm: 0.00 | lr 5.5257e-04 | time: 9618.79ms | tokens/mssec: 54.51\n",
      "for iter: 23 | loss: 6.317145347595215 | norm: 0.00 | lr 5.4472e-04 | time: 9619.18ms | tokens/mssec: 54.50\n",
      "for iter: 24 | loss: 6.295767784118652 | norm: 0.00 | lr 5.3636e-04 | time: 9617.12ms | tokens/mssec: 54.52\n",
      "for iter: 25 | loss: 6.28631067276001 | norm: 0.00 | lr 5.2752e-04 | time: 9626.89ms | tokens/mssec: 54.46\n",
      "for iter: 26 | loss: 6.259523868560791 | norm: 0.00 | lr 5.1824e-04 | time: 9629.17ms | tokens/mssec: 54.45\n",
      "for iter: 27 | loss: 6.245904922485352 | norm: 0.00 | lr 5.0853e-04 | time: 9623.42ms | tokens/mssec: 54.48\n",
      "for iter: 28 | loss: 6.2036614418029785 | norm: 0.00 | lr 4.9844e-04 | time: 9620.83ms | tokens/mssec: 54.50\n",
      "for iter: 29 | loss: 6.211301326751709 | norm: 0.00 | lr 4.8800e-04 | time: 9610.69ms | tokens/mssec: 54.55\n",
      "for iter: 30 | loss: 6.181417942047119 | norm: 0.00 | lr 4.7724e-04 | time: 9611.34ms | tokens/mssec: 54.55\n",
      "for iter: 31 | loss: 6.170820236206055 | norm: 0.00 | lr 4.6619e-04 | time: 9615.45ms | tokens/mssec: 54.53\n",
      "for iter: 32 | loss: 6.165499210357666 | norm: 0.00 | lr 4.5489e-04 | time: 9617.94ms | tokens/mssec: 54.51\n",
      "for iter: 33 | loss: 6.155663013458252 | norm: 0.00 | lr 4.4339e-04 | time: 9622.54ms | tokens/mssec: 54.49\n",
      "for iter: 34 | loss: 6.147393226623535 | norm: 0.00 | lr 4.3170e-04 | time: 9620.94ms | tokens/mssec: 54.49\n",
      "for iter: 35 | loss: 6.1353559494018555 | norm: 0.00 | lr 4.1989e-04 | time: 9610.45ms | tokens/mssec: 54.55\n",
      "for iter: 36 | loss: 6.132938385009766 | norm: 0.00 | lr 4.0797e-04 | time: 9618.69ms | tokens/mssec: 54.51\n",
      "for iter: 37 | loss: 6.093754291534424 | norm: 0.00 | lr 3.9600e-04 | time: 9622.37ms | tokens/mssec: 54.49\n",
      "for iter: 38 | loss: 6.103639602661133 | norm: 0.00 | lr 3.8400e-04 | time: 9619.34ms | tokens/mssec: 54.50\n",
      "for iter: 39 | loss: 6.070328235626221 | norm: 0.00 | lr 3.7203e-04 | time: 9621.74ms | tokens/mssec: 54.49\n",
      "for iter: 40 | loss: 6.055922508239746 | norm: 0.00 | lr 3.6011e-04 | time: 9616.21ms | tokens/mssec: 54.52\n",
      "for iter: 41 | loss: 6.048197269439697 | norm: 0.00 | lr 3.4830e-04 | time: 9625.11ms | tokens/mssec: 54.47\n",
      "for iter: 42 | loss: 6.031949520111084 | norm: 0.00 | lr 3.3661e-04 | time: 9639.91ms | tokens/mssec: 54.39\n",
      "for iter: 43 | loss: 6.028709411621094 | norm: 0.00 | lr 3.2511e-04 | time: 9622.30ms | tokens/mssec: 54.49\n",
      "for iter: 44 | loss: 6.00192928314209 | norm: 0.00 | lr 3.1381e-04 | time: 9630.46ms | tokens/mssec: 54.44\n",
      "for iter: 45 | loss: 6.014623641967773 | norm: 0.00 | lr 3.0276e-04 | time: 9640.39ms | tokens/mssec: 54.38\n",
      "for iter: 46 | loss: 5.986992835998535 | norm: 0.00 | lr 2.9200e-04 | time: 9630.73ms | tokens/mssec: 54.44\n",
      "for iter: 47 | loss: 5.9815168380737305 | norm: 0.00 | lr 2.8156e-04 | time: 9640.39ms | tokens/mssec: 54.38\n",
      "for iter: 48 | loss: 5.967012882232666 | norm: 0.00 | lr 2.7147e-04 | time: 9635.99ms | tokens/mssec: 54.41\n",
      "for iter: 49 | loss: 5.96043062210083 | norm: 0.00 | lr 2.6176e-04 | time: 9624.81ms | tokens/mssec: 54.47\n",
      "for iter: 50 | loss: 5.945199489593506 | norm: 0.00 | lr 2.5248e-04 | time: 9643.81ms | tokens/mssec: 54.37\n",
      "for iter: 51 | loss: 5.933154106140137 | norm: 0.00 | lr 2.4364e-04 | time: 9637.11ms | tokens/mssec: 54.40\n",
      "for iter: 52 | loss: 5.938729763031006 | norm: 0.00 | lr 2.3528e-04 | time: 9641.71ms | tokens/mssec: 54.38\n",
      "for iter: 53 | loss: 5.904153347015381 | norm: 0.00 | lr 2.2743e-04 | time: 9640.97ms | tokens/mssec: 54.38\n",
      "for iter: 54 | loss: 5.924854278564453 | norm: 0.00 | lr 2.2011e-04 | time: 9646.73ms | tokens/mssec: 54.35\n",
      "for iter: 55 | loss: 5.901758193969727 | norm: 0.00 | lr 2.1334e-04 | time: 9642.78ms | tokens/mssec: 54.37\n",
      "for iter: 56 | loss: 5.895321369171143 | norm: 0.00 | lr 2.0714e-04 | time: 9648.54ms | tokens/mssec: 54.34\n",
      "for iter: 57 | loss: 5.896589279174805 | norm: 0.00 | lr 2.0155e-04 | time: 9656.35ms | tokens/mssec: 54.29\n",
      "for iter: 58 | loss: 5.885725975036621 | norm: 0.00 | lr 1.9656e-04 | time: 9652.43ms | tokens/mssec: 54.32\n",
      "for iter: 59 | loss: 5.886592864990234 | norm: 0.00 | lr 1.9221e-04 | time: 9642.91ms | tokens/mssec: 54.37\n",
      "for iter: 60 | loss: 5.8697614669799805 | norm: 0.00 | lr 1.8851e-04 | time: 9636.79ms | tokens/mssec: 54.40\n",
      "for iter: 61 | loss: 5.886807441711426 | norm: 0.00 | lr 1.8546e-04 | time: 9652.70ms | tokens/mssec: 54.32\n",
      "for iter: 62 | loss: 5.8592529296875 | norm: 0.00 | lr 1.8308e-04 | time: 9653.24ms | tokens/mssec: 54.31\n",
      "for iter: 63 | loss: 5.869042873382568 | norm: 0.00 | lr 1.8137e-04 | time: 9658.91ms | tokens/mssec: 54.28\n",
      "for iter: 64 | loss: 5.856814384460449 | norm: 0.00 | lr 1.8034e-04 | time: 9647.27ms | tokens/mssec: 54.35\n",
      "for iter: 65 | loss: 5.851565361022949 | norm: 0.00 | lr 1.8000e-04 | time: 9661.92ms | tokens/mssec: 54.26\n",
      "for iter: 66 | loss: 5.847635269165039 | norm: 0.00 | lr 1.8000e-04 | time: 9661.38ms | tokens/mssec: 54.27\n",
      "for iter: 67 | loss: 5.842685699462891 | norm: 0.00 | lr 1.8000e-04 | time: 9664.47ms | tokens/mssec: 54.25\n",
      "for iter: 68 | loss: 5.8451385498046875 | norm: 0.00 | lr 1.8000e-04 | time: 9661.78ms | tokens/mssec: 54.26\n",
      "for iter: 69 | loss: 5.8161211013793945 | norm: 0.00 | lr 1.8000e-04 | time: 9655.30ms | tokens/mssec: 54.30\n",
      "for iter: 70 | loss: 5.841326713562012 | norm: 0.00 | lr 1.8000e-04 | time: 9653.13ms | tokens/mssec: 54.31\n",
      "for iter: 71 | loss: 5.814523220062256 | norm: 0.00 | lr 1.8000e-04 | time: 9651.24ms | tokens/mssec: 54.32\n",
      "for iter: 72 | loss: 5.811751365661621 | norm: 0.00 | lr 1.8000e-04 | time: 9656.13ms | tokens/mssec: 54.30\n",
      "for iter: 73 | loss: 5.805110931396484 | norm: 0.00 | lr 1.8000e-04 | time: 9670.45ms | tokens/mssec: 54.22\n",
      "for iter: 74 | loss: 5.796032428741455 | norm: 0.00 | lr 1.8000e-04 | time: 9656.46ms | tokens/mssec: 54.29\n",
      "for iter: 75 | loss: 5.785649299621582 | norm: 0.00 | lr 1.8000e-04 | time: 9673.29ms | tokens/mssec: 54.20\n",
      "for iter: 76 | loss: 5.7797532081604 | norm: 0.00 | lr 1.8000e-04 | time: 9672.69ms | tokens/mssec: 54.20\n",
      "for iter: 77 | loss: 5.802606582641602 | norm: 0.00 | lr 1.8000e-04 | time: 9661.48ms | tokens/mssec: 54.27\n",
      "for iter: 78 | loss: 5.753467082977295 | norm: 0.00 | lr 1.8000e-04 | time: 9663.35ms | tokens/mssec: 54.26\n",
      "for iter: 79 | loss: 5.783279895782471 | norm: 0.00 | lr 1.8000e-04 | time: 9666.60ms | tokens/mssec: 54.24\n",
      "for iter: 80 | loss: 5.750962257385254 | norm: 0.00 | lr 1.8000e-04 | time: 9663.15ms | tokens/mssec: 54.26\n",
      "for iter: 81 | loss: 5.747988224029541 | norm: 0.00 | lr 1.8000e-04 | time: 9664.27ms | tokens/mssec: 54.25\n"
     ]
    }
   ],
   "source": [
    "transformer_model.train()\n",
    "for iter in range(total_iteration):\n",
    "    start_time = time.time()\n",
    "    optimizer.zero_grad()\n",
    "    loss_accum = 0.0\n",
    "    for _ in range(grad_accumulation_steps):\n",
    "        x, y = train_loader.next_batch()\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        with ctx:\n",
    "            logits, loss = transformer_model(x, y)\n",
    "\n",
    "        loss = (\n",
    "            loss / grad_accumulation_steps\n",
    "        )  # divide the loss by accumulation steps to get the average loss\n",
    "        loss_accum += loss.detach()  # accumulate the loss\n",
    "        # backward pass with GradScaler\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "    norm = 0.0\n",
    "    if grad_clip != 0.0:\n",
    "        # unscales the gradients of optimizer's assigned params in-place\n",
    "        scaler.unscale_(optimizer)\n",
    "        # clip the gradients\n",
    "        norm = torch.nn.utils.clip_grad_norm(transformer_model.parameters(), grad_clip)\n",
    "\n",
    "    # get the current learning rate\n",
    "    lr = get_lr(iter, warmup_steps, max_lr, int(total_iteration * 0.8), min_lr)\n",
    "    # update the learning rate\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = lr\n",
    "\n",
    "    # Unscales the gradients and calls optimizer.step()\n",
    "    scaler.step(optimizer)\n",
    "    # Updates the scale for next iteration\n",
    "    scaler.update()\n",
    "    end_time = time.time()\n",
    "    torch.cuda.empty_cache()  # clear the cache\n",
    "    torch.cuda.synchronize()  # wait for the computation to be done\n",
    "    time_elapsed = (end_time - start_time) * 1000\n",
    "    token_persec = (x.numel() * grad_accumulation_steps) / time_elapsed\n",
    "    print(\n",
    "        f\"for iter: {iter} | loss: {loss_accum.item()} | norm: {norm:.2f} | lr {lr:.4e} | time: {time_elapsed:.2f}ms | tokens/mssec: {token_persec:.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Cleaned up organized code with all the above optimizations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_lr = 6e-4\n",
    "# min_lr = max_lr * 0.3\n",
    "# warmup_steps = 10\n",
    "# weight_decay = 0.1\n",
    "# # basic settings\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# torch.manual_seed(42)\n",
    "# torch.cuda.manual_seed(42)\n",
    "# device_type = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# ideal_batch_size = 524288  # as per GPT3 paper\n",
    "# permit_batch = 8  # good enough 1 core gpu\n",
    "# permit_context = 1024\n",
    "\n",
    "# # grad_accumulation_steps = ideal_batch_size // (permit_batch * permit_context)\n",
    "# grad_accumulation_steps = ideal_batch_size // (permit_batch * permit_context)\n",
    "# print(f\"grad_accumulation_steps: {grad_accumulation_steps}\")\n",
    "# # permisssion for grad_clip\n",
    "# grad_clip = 0.0\n",
    "# # model compilation\n",
    "# compile = True\n",
    "\n",
    "\n",
    "# def get_schedule_lr(\n",
    "#     it, warmup_iters=500, learning_rate=0.1, lr_decay_iters=1000, min_lr=0.01\n",
    "# ):\n",
    "#     \"\"\"Get the learning rate schedule for training after cosine annealing\"\"\"\n",
    "#     if it < warmup_iters:\n",
    "#         return learning_rate * (it + 1) / warmup_iters\n",
    "#     if it > lr_decay_iters:\n",
    "#         return min_lr\n",
    "\n",
    "#     # Cosine annealing learning rate schedule\n",
    "#     decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "#     assert 0 <= decay_ratio <= 1\n",
    "#     coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
    "#     return min_lr + coeff * (learning_rate - min_lr)\n",
    "\n",
    "\n",
    "# def configure_optimizers(model, weight_decay, learning_rate, device_type):\n",
    "#     \"\"\"Create the optimizer and scheduler for training\"\"\"\n",
    "\n",
    "#     # start with all of the candidate parameters (that require grad)\n",
    "#     param_dict = {pn: p for pn, p in model.named_parameters()}\n",
    "#     param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "\n",
    "#     # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
    "#     # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
    "#     decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "#     nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "#     optim_groups = [\n",
    "#         {\"params\": decay_params, \"weight_decay\": weight_decay},\n",
    "#         {\"params\": nodecay_params, \"weight_decay\": 0.0},\n",
    "#     ]\n",
    "\n",
    "#     num_decay_params = sum(p.numel() for p in decay_params)\n",
    "#     num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "\n",
    "#     print(\n",
    "#         f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\"\n",
    "#     )\n",
    "#     print(\n",
    "#         f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\"\n",
    "#     )\n",
    "#     # Create AdamW optimizer and use the fused version if it is available\n",
    "#     fused_available = \"fused\" in inspect.signature(torch.optim.AdamW).parameters\n",
    "#     use_fused = fused_available and device_type == \"cuda\"\n",
    "\n",
    "#     optimizer = torch.optim.AdamW(\n",
    "#         optim_groups, lr=learning_rate, betas=(0.9, 0.95), eps=1e-8, fused=use_fused\n",
    "#     )\n",
    "#     return optimizer\n",
    "\n",
    "\n",
    "# def read_tokens(filename, encoder):\n",
    "#     with open(filename, \"r\") as file:\n",
    "#         lines = file.read()\n",
    "\n",
    "#     tokens = encoder.encode(lines)\n",
    "#     return tokens\n",
    "\n",
    "\n",
    "# class DataloaderLiter:\n",
    "#     def __init__(self, B, T, encoder, filename):\n",
    "#         self.B = B  # batch size\n",
    "#         self.T = T  # seq length\n",
    "#         tokens = read_tokens(filename, encoder)\n",
    "#         self.tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "\n",
    "#         print(f\"Total tokens in the file: {len(self.tokens)}\")\n",
    "#         print(\n",
    "#             f\"1 epoch will have {len(self.tokens) // (B * T)} iterations given Batch size={B} and Context length={T}\"\n",
    "#         )\n",
    "\n",
    "#         self.current_position = 0\n",
    "#         self.num_iterations = len(self.tokens) // (B * T)\n",
    "\n",
    "#     def next_batch(self):\n",
    "#         B, T = self.B, self.T\n",
    "#         current_batch = self.tokens[\n",
    "#             self.current_position : self.current_position + (B * T) + 1\n",
    "#         ]  # +1 for the target\n",
    "#         x = current_batch[:-1].view(B, T)  # input tensor of shape (B, T)\n",
    "#         y = current_batch[1:].view(B, T)  # target tensor of shape (B, T)\n",
    "\n",
    "#         # updating the position for next batch\n",
    "#         self.current_position += B * T\n",
    "\n",
    "#         # we reset the position if the next batch is OOB\n",
    "#         if self.current_position + (B * T) + 1 > len(self.tokens):\n",
    "#             self.current_position = 0\n",
    "\n",
    "#         return x, y\n",
    "\n",
    "\n",
    "# dtype = (\n",
    "#     \"bfloat16\"\n",
    "#     if torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "#     else \"float16\"\n",
    "# )\n",
    "# print(f\"Using {dtype} for automatic mixed precision training\")\n",
    "\n",
    "# # note: float16 data type will automatically use a GradScaler\n",
    "# ptdtype = {\n",
    "#     \"float32\": torch.float32,\n",
    "#     \"bfloat16\": torch.bfloat16,\n",
    "#     \"float16\": torch.float16,\n",
    "# }[dtype]\n",
    "# # context manager for automatic mixed precision training\n",
    "# ctx = (\n",
    "#     nullcontext()\n",
    "#     if device_type == \"cpu\"\n",
    "#     else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    "# )\n",
    "\n",
    "# if ctx != nullcontext():\n",
    "#     # GradScaler for automatic mixed precision training\n",
    "#     scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
    "# torch.cuda.empty_cache()\n",
    "# torch.cuda.reset_peak_memory_stats()\n",
    "# torch.cuda.synchronize()\n",
    "# transformer_model = GPT(\n",
    "#     GPTConfig(\n",
    "#         vocab_size=50304,\n",
    "#         block_size=2048,\n",
    "#         n_layer=12,\n",
    "#         n_head=12,\n",
    "#         n_embd=768,\n",
    "#     )\n",
    "# )\n",
    "# transformer_model.to(device)\n",
    "\n",
    "# if compile:\n",
    "#     transformer_model = torch.compile(transformer_model)\n",
    "# encoder = tiktoken.get_encoding(encoding_name=\"gpt2\")\n",
    "# filename = \"./data/tinyshakespeare/input.txt\"\n",
    "# optimizer = configure_optimizers(transformer_model, weight_decay, max_lr, device_type)\n",
    "# train_loader = DataloaderLiter(B=8, T=1024, encoder=encoder, filename=filename)\n",
    "# num_epochs = 2\n",
    "# initial_iterations = 100\n",
    "# total_iteration = num_epochs * train_loader.num_iterations\n",
    "# print(f\"Total iterations: {total_iteration}\")\n",
    "# transformer_model.train()\n",
    "# for iter in range(total_iteration):\n",
    "#     start_time = time.time()\n",
    "#     optimizer.zero_grad()\n",
    "#     loss_accum = 0.0\n",
    "#     for _ in range(grad_accumulation_steps):\n",
    "#         x, y = train_loader.next_batch()\n",
    "#         x, y = x.to(device), y.to(device)\n",
    "#         with ctx:\n",
    "#             logits, loss = transformer_model(x, y)\n",
    "\n",
    "#         loss = (\n",
    "#             loss / grad_accumulation_steps\n",
    "#         )  # divide the loss by accumulation steps to get the average loss\n",
    "#         loss_accum += loss.detach()  # accumulate the loss\n",
    "#         # backward pass with GradScaler\n",
    "#         scaler.scale(loss).backward()\n",
    "\n",
    "#     norm = 0.0\n",
    "#     if grad_clip != 0.0:\n",
    "#         # unscales the gradients of optimizer's assigned params in-place\n",
    "#         scaler.unscale_(optimizer)\n",
    "#         # clip the gradients\n",
    "#         norm = torch.nn.utils.clip_grad_norm(transformer_model.parameters(), grad_clip)\n",
    "\n",
    "#     # get the current learning rate\n",
    "#     lr = get_schedule_lr(iter, warmup_steps, max_lr, int(total_iteration * 0.8), min_lr)\n",
    "#     # update the learning rate\n",
    "#     for param_group in optimizer.param_groups:\n",
    "#         param_group[\"lr\"] = lr\n",
    "\n",
    "#     # Unscales the gradients and calls optimizer.step()\n",
    "#     scaler.step(optimizer)\n",
    "#     # Updates the scale for next iteration\n",
    "#     scaler.update()\n",
    "#     end_time = time.time()\n",
    "#     torch.cuda.empty_cache()  # clear the cache\n",
    "#     torch.cuda.synchronize()  # wait for the computation to be done\n",
    "#     time_elapsed = (end_time - start_time) * 1000\n",
    "#     token_persec = (x.numel() * grad_accumulation_steps) / time_elapsed\n",
    "#     print(\n",
    "#         f\"for iter: {iter} | loss: {loss_accum.item()} | norm: {norm:.2f} | lr {lr:.4e} | time: {time_elapsed:.2f}ms | tokens/mssec: {token_persec:.2f}\"\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lr = 6e-4\n",
    "min_lr = max_lr * 0.3\n",
    "warmup_steps = 10\n",
    "weight_decay = 0.1\n",
    "num_epochs = 2\n",
    "initial_iterations = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic settings\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "device_type = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad_accumulation_steps: 64\n"
     ]
    }
   ],
   "source": [
    "ideal_batch_size = 524288  # as per GPT3 paper\n",
    "permit_batch = 8  # good enough 1 core gpu\n",
    "permit_context = 1024\n",
    "# grad_accumulation_steps = ideal_batch_size // (permit_batch * permit_context)\n",
    "grad_accumulation_steps = ideal_batch_size // (permit_batch * permit_context)\n",
    "print(f\"grad_accumulation_steps: {grad_accumulation_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# permisssion for grad_clip\n",
    "grad_clip = 0.0\n",
    "# model compilation\n",
    "compile = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_schedule_lr(\n",
    "    it, warmup_iters=500, learning_rate=0.1, lr_decay_iters=1000, min_lr=0.01\n",
    "):\n",
    "    \"\"\"Get the learning rate schedule for training after cosine annealing\"\"\"\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * (it + 1) / warmup_iters\n",
    "    if it > lr_decay_iters:\n",
    "        return min_lr\n",
    "\n",
    "    # Cosine annealing learning rate schedule\n",
    "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
    "    return min_lr + coeff * (learning_rate - min_lr)\n",
    "\n",
    "\n",
    "def configure_optimizers(model, weight_decay, learning_rate, device_type):\n",
    "    \"\"\"Create the optimizer and scheduler for training\"\"\"\n",
    "\n",
    "    # start with all of the candidate parameters (that require grad)\n",
    "    param_dict = {pn: p for pn, p in model.named_parameters()}\n",
    "    param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "\n",
    "    # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
    "    # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
    "    decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "    nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "    optim_groups = [\n",
    "        {\"params\": decay_params, \"weight_decay\": weight_decay},\n",
    "        {\"params\": nodecay_params, \"weight_decay\": 0.0},\n",
    "    ]\n",
    "\n",
    "    num_decay_params = sum(p.numel() for p in decay_params)\n",
    "    num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "\n",
    "    print(\n",
    "        f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\"\n",
    "    )\n",
    "    print(\n",
    "        f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\"\n",
    "    )\n",
    "    # Create AdamW optimizer and use the fused version if it is available\n",
    "    fused_available = \"fused\" in inspect.signature(torch.optim.AdamW).parameters\n",
    "    use_fused = fused_available and device_type == \"cuda\"\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        optim_groups, lr=learning_rate, betas=(0.9, 0.95), eps=1e-8, fused=use_fused\n",
    "    )\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tokens(filename, encoder):\n",
    "    with open(filename, \"r\") as file:\n",
    "        lines = file.read()\n",
    "\n",
    "    tokens = encoder.encode(lines)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "class DataloaderLiter:\n",
    "    def __init__(self, B, T, encoder, filename):\n",
    "        self.B = B  # batch size\n",
    "        self.T = T  # seq length\n",
    "        tokens = read_tokens(filename, encoder)\n",
    "        self.tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "\n",
    "        print(f\"Total tokens in the file: {len(self.tokens)}\")\n",
    "        print(\n",
    "            f\"1 epoch will have {len(self.tokens) // (B * T)} iterations given Batch size={B} and Context length={T}\"\n",
    "        )\n",
    "\n",
    "        self.current_position = 0\n",
    "        self.num_iterations = len(self.tokens) // (B * T)\n",
    "\n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "        current_batch = self.tokens[\n",
    "            self.current_position : self.current_position + (B * T) + 1\n",
    "        ]  # +1 for the target\n",
    "        x = current_batch[:-1].view(B, T)  # input tensor of shape (B, T)\n",
    "        y = current_batch[1:].view(B, T)  # target tensor of shape (B, T)\n",
    "\n",
    "        # updating the position for next batch\n",
    "        self.current_position += B * T\n",
    "\n",
    "        # we reset the position if the next batch is OOB\n",
    "        if self.current_position + (B * T) + 1 > len(self.tokens):\n",
    "            self.current_position = 0\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using float16 for automatic mixed precision training\n"
     ]
    }
   ],
   "source": [
    "dtype = (\n",
    "    \"bfloat16\"\n",
    "    if torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "    else \"float16\"\n",
    ")\n",
    "print(f\"Using {dtype} for automatic mixed precision training\")\n",
    "\n",
    "# note: float16 data type will automatically use a GradScaler\n",
    "ptdtype = {\n",
    "    \"float32\": torch.float32,\n",
    "    \"bfloat16\": torch.bfloat16,\n",
    "    \"float16\": torch.float16,\n",
    "}[dtype]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# context manager for automatic mixed precision training\n",
    "ctx = (\n",
    "    nullcontext()\n",
    "    if device_type == \"cpu\"\n",
    "    else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    ")\n",
    "\n",
    "if ctx != nullcontext():\n",
    "    # GradScaler for automatic mixed precision training\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "transformer_model = GPT(\n",
    "    GPTConfig(\n",
    "        vocab_size=50304,\n",
    "        block_size=2048,\n",
    "        n_layer=12,\n",
    "        n_head=12,\n",
    "        n_embd=768,\n",
    "    )\n",
    ")\n",
    "transformer_model.to(device)\n",
    "\n",
    "if compile:\n",
    "    transformer_model = torch.compile(transformer_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens in the file: 338025\n",
      "1 epoch will have 41 iterations given Batch size=8 and Context length=1024\n",
      "Total iterations: 82\n"
     ]
    }
   ],
   "source": [
    "encoder = tiktoken.get_encoding(encoding_name=\"gpt2\")\n",
    "filename = \"./data/tinyshakespeare/input.txt\"\n",
    "train_loader = DataloaderLiter(B=8, T=1024, encoder=encoder, filename=filename)\n",
    "total_iteration = num_epochs * train_loader.num_iterations\n",
    "print(f\"Total iterations: {total_iteration}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num decayed parameter tensors: 50, with 125,140,992 parameters\n",
      "num non-decayed parameter tensors: 98, with 121,344 parameters\n"
     ]
    }
   ],
   "source": [
    "optimizer = configure_optimizers(transformer_model, weight_decay, max_lr, device_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for iter: 0 | loss: 10.968994140625 | norm: 0.00 | lr 6.0000e-05 | time: 35775.11ms | tokens/mssec: 14.66\n",
      "for iter: 1 | loss: 9.615639686584473 | norm: 0.00 | lr 1.2000e-04 | time: 9049.23ms | tokens/mssec: 57.94\n",
      "for iter: 2 | loss: 9.234090805053711 | norm: 0.00 | lr 1.8000e-04 | time: 9031.11ms | tokens/mssec: 58.05\n",
      "for iter: 3 | loss: 9.333982467651367 | norm: 0.00 | lr 2.4000e-04 | time: 9073.30ms | tokens/mssec: 57.78\n",
      "for iter: 4 | loss: 8.93940258026123 | norm: 0.00 | lr 3.0000e-04 | time: 9006.26ms | tokens/mssec: 58.21\n",
      "for iter: 5 | loss: 8.556042671203613 | norm: 0.00 | lr 3.6000e-04 | time: 9069.62ms | tokens/mssec: 57.81\n",
      "for iter: 6 | loss: 8.289645195007324 | norm: 0.00 | lr 4.2000e-04 | time: 9033.94ms | tokens/mssec: 58.04\n",
      "for iter: 7 | loss: 8.053885459899902 | norm: 0.00 | lr 4.8000e-04 | time: 9064.48ms | tokens/mssec: 57.84\n",
      "for iter: 8 | loss: 7.718583583831787 | norm: 0.00 | lr 5.4000e-04 | time: 9019.80ms | tokens/mssec: 58.13\n",
      "for iter: 9 | loss: 7.3848066329956055 | norm: 0.00 | lr 6.0000e-04 | time: 9055.84ms | tokens/mssec: 57.89\n",
      "for iter: 10 | loss: 7.107554912567139 | norm: 0.00 | lr 6.0000e-04 | time: 9007.43ms | tokens/mssec: 58.21\n",
      "for iter: 11 | loss: 6.925228595733643 | norm: 0.00 | lr 5.9966e-04 | time: 9060.02ms | tokens/mssec: 57.87\n",
      "for iter: 12 | loss: 6.739129066467285 | norm: 0.00 | lr 5.9863e-04 | time: 9020.75ms | tokens/mssec: 58.12\n",
      "for iter: 13 | loss: 6.620566368103027 | norm: 0.00 | lr 5.9692e-04 | time: 9082.80ms | tokens/mssec: 57.72\n",
      "for iter: 14 | loss: 6.512599468231201 | norm: 0.00 | lr 5.9454e-04 | time: 9048.26ms | tokens/mssec: 57.94\n",
      "for iter: 15 | loss: 6.444116592407227 | norm: 0.00 | lr 5.9149e-04 | time: 9080.40ms | tokens/mssec: 57.74\n",
      "for iter: 16 | loss: 6.427492141723633 | norm: 0.00 | lr 5.8779e-04 | time: 9020.56ms | tokens/mssec: 58.12\n",
      "for iter: 17 | loss: 6.409012794494629 | norm: 0.00 | lr 5.8344e-04 | time: 9023.69ms | tokens/mssec: 58.10\n",
      "for iter: 18 | loss: 6.417464733123779 | norm: 0.00 | lr 5.7845e-04 | time: 9070.98ms | tokens/mssec: 57.80\n",
      "for iter: 19 | loss: 6.3874945640563965 | norm: 0.00 | lr 5.7286e-04 | time: 9044.73ms | tokens/mssec: 57.97\n",
      "for iter: 20 | loss: 6.411595821380615 | norm: 0.00 | lr 5.6666e-04 | time: 9008.16ms | tokens/mssec: 58.20\n",
      "for iter: 21 | loss: 6.387474060058594 | norm: 0.00 | lr 5.5989e-04 | time: 9035.48ms | tokens/mssec: 58.03\n",
      "for iter: 22 | loss: 6.384653568267822 | norm: 0.00 | lr 5.5257e-04 | time: 9075.06ms | tokens/mssec: 57.77\n",
      "for iter: 23 | loss: 6.363269805908203 | norm: 0.00 | lr 5.4472e-04 | time: 9010.23ms | tokens/mssec: 58.19\n",
      "for iter: 24 | loss: 6.331719875335693 | norm: 0.00 | lr 5.3636e-04 | time: 9064.81ms | tokens/mssec: 57.84\n",
      "for iter: 25 | loss: 6.3109822273254395 | norm: 0.00 | lr 5.2752e-04 | time: 9010.91ms | tokens/mssec: 58.18\n",
      "for iter: 26 | loss: 6.270181655883789 | norm: 0.00 | lr 5.1824e-04 | time: 9054.47ms | tokens/mssec: 57.90\n",
      "for iter: 27 | loss: 6.262033462524414 | norm: 0.00 | lr 5.0853e-04 | time: 9005.06ms | tokens/mssec: 58.22\n",
      "for iter: 28 | loss: 6.2109198570251465 | norm: 0.00 | lr 4.9844e-04 | time: 9081.08ms | tokens/mssec: 57.73\n",
      "for iter: 29 | loss: 6.223969459533691 | norm: 0.00 | lr 4.8800e-04 | time: 9007.53ms | tokens/mssec: 58.21\n",
      "for iter: 30 | loss: 6.1936845779418945 | norm: 0.00 | lr 4.7724e-04 | time: 9077.16ms | tokens/mssec: 57.76\n",
      "for iter: 31 | loss: 6.189316749572754 | norm: 0.00 | lr 4.6619e-04 | time: 9018.12ms | tokens/mssec: 58.14\n",
      "for iter: 32 | loss: 6.184817314147949 | norm: 0.00 | lr 4.5489e-04 | time: 9047.65ms | tokens/mssec: 57.95\n",
      "for iter: 33 | loss: 6.173177242279053 | norm: 0.00 | lr 4.4339e-04 | time: 9023.14ms | tokens/mssec: 58.10\n",
      "for iter: 34 | loss: 6.162118434906006 | norm: 0.00 | lr 4.3170e-04 | time: 8999.99ms | tokens/mssec: 58.25\n",
      "for iter: 35 | loss: 6.152835369110107 | norm: 0.00 | lr 4.1989e-04 | time: 9076.09ms | tokens/mssec: 57.77\n",
      "for iter: 36 | loss: 6.153388023376465 | norm: 0.00 | lr 4.0797e-04 | time: 9032.46ms | tokens/mssec: 58.04\n",
      "for iter: 37 | loss: 6.1192169189453125 | norm: 0.00 | lr 3.9600e-04 | time: 9006.42ms | tokens/mssec: 58.21\n",
      "for iter: 38 | loss: 6.119556903839111 | norm: 0.00 | lr 3.8400e-04 | time: 11321.22ms | tokens/mssec: 46.31\n",
      "for iter: 39 | loss: 6.092499256134033 | norm: 0.00 | lr 3.7203e-04 | time: 21787.60ms | tokens/mssec: 24.06\n",
      "for iter: 40 | loss: 6.073812007904053 | norm: 0.00 | lr 3.6011e-04 | time: 9006.17ms | tokens/mssec: 58.21\n",
      "for iter: 41 | loss: 6.0667219161987305 | norm: 0.00 | lr 3.4830e-04 | time: 9055.99ms | tokens/mssec: 57.89\n",
      "for iter: 42 | loss: 6.048290729522705 | norm: 0.00 | lr 3.3661e-04 | time: 9064.03ms | tokens/mssec: 57.84\n",
      "for iter: 43 | loss: 6.04016637802124 | norm: 0.00 | lr 3.2511e-04 | time: 11233.32ms | tokens/mssec: 46.67\n",
      "for iter: 44 | loss: 6.013952255249023 | norm: 0.00 | lr 3.1381e-04 | time: 8987.53ms | tokens/mssec: 58.34\n",
      "for iter: 45 | loss: 6.025516986846924 | norm: 0.00 | lr 3.0276e-04 | time: 9072.23ms | tokens/mssec: 57.79\n",
      "for iter: 46 | loss: 5.999321937561035 | norm: 0.00 | lr 2.9200e-04 | time: 9003.83ms | tokens/mssec: 58.23\n",
      "for iter: 47 | loss: 5.994870185852051 | norm: 0.00 | lr 2.8156e-04 | time: 9021.44ms | tokens/mssec: 58.12\n",
      "for iter: 48 | loss: 5.98402738571167 | norm: 0.00 | lr 2.7147e-04 | time: 9032.31ms | tokens/mssec: 58.05\n",
      "for iter: 49 | loss: 5.97666072845459 | norm: 0.00 | lr 2.6176e-04 | time: 9006.64ms | tokens/mssec: 58.21\n",
      "for iter: 50 | loss: 5.966038227081299 | norm: 0.00 | lr 2.5248e-04 | time: 9052.06ms | tokens/mssec: 57.92\n",
      "for iter: 51 | loss: 5.956713676452637 | norm: 0.00 | lr 2.4364e-04 | time: 9011.94ms | tokens/mssec: 58.18\n",
      "for iter: 52 | loss: 5.960346221923828 | norm: 0.00 | lr 2.3528e-04 | time: 9010.70ms | tokens/mssec: 58.19\n",
      "for iter: 53 | loss: 5.9267258644104 | norm: 0.00 | lr 2.2743e-04 | time: 9001.99ms | tokens/mssec: 58.24\n",
      "for iter: 54 | loss: 5.9430437088012695 | norm: 0.00 | lr 2.2011e-04 | time: 9051.60ms | tokens/mssec: 57.92\n",
      "for iter: 55 | loss: 5.918300151824951 | norm: 0.00 | lr 2.1334e-04 | time: 9004.76ms | tokens/mssec: 58.22\n",
      "for iter: 56 | loss: 5.9079060554504395 | norm: 0.00 | lr 2.0714e-04 | time: 9109.07ms | tokens/mssec: 57.56\n",
      "for iter: 57 | loss: 5.909153938293457 | norm: 0.00 | lr 2.0155e-04 | time: 8995.02ms | tokens/mssec: 58.29\n",
      "for iter: 58 | loss: 5.9012298583984375 | norm: 0.00 | lr 1.9656e-04 | time: 9061.07ms | tokens/mssec: 57.86\n",
      "for iter: 59 | loss: 5.925540447235107 | norm: 0.00 | lr 1.9221e-04 | time: 9019.47ms | tokens/mssec: 58.13\n",
      "for iter: 60 | loss: 5.887314796447754 | norm: 0.00 | lr 1.8851e-04 | time: 9080.56ms | tokens/mssec: 57.74\n",
      "for iter: 61 | loss: 5.945149898529053 | norm: 0.00 | lr 1.8546e-04 | time: 8987.72ms | tokens/mssec: 58.33\n",
      "for iter: 62 | loss: 5.901706695556641 | norm: 0.00 | lr 1.8308e-04 | time: 9033.14ms | tokens/mssec: 58.04\n",
      "for iter: 63 | loss: 5.9374589920043945 | norm: 0.00 | lr 1.8137e-04 | time: 9053.13ms | tokens/mssec: 57.91\n",
      "for iter: 64 | loss: 5.923667907714844 | norm: 0.00 | lr 1.8034e-04 | time: 9057.23ms | tokens/mssec: 57.89\n",
      "for iter: 65 | loss: 5.912867546081543 | norm: 0.00 | lr 1.8000e-04 | time: 9006.88ms | tokens/mssec: 58.21\n",
      "for iter: 66 | loss: 5.914050102233887 | norm: 0.00 | lr 1.8000e-04 | time: 9068.01ms | tokens/mssec: 57.82\n",
      "for iter: 67 | loss: 5.8965630531311035 | norm: 0.00 | lr 1.8000e-04 | time: 9010.62ms | tokens/mssec: 58.19\n",
      "for iter: 68 | loss: 5.901540756225586 | norm: 0.00 | lr 1.8000e-04 | time: 9050.96ms | tokens/mssec: 57.93\n",
      "for iter: 69 | loss: 5.873265743255615 | norm: 0.00 | lr 1.8000e-04 | time: 9018.07ms | tokens/mssec: 58.14\n",
      "for iter: 70 | loss: 5.8989667892456055 | norm: 0.00 | lr 1.8000e-04 | time: 9074.36ms | tokens/mssec: 57.78\n",
      "for iter: 71 | loss: 5.875817775726318 | norm: 0.00 | lr 1.8000e-04 | time: 9033.27ms | tokens/mssec: 58.04\n",
      "for iter: 72 | loss: 5.871921062469482 | norm: 0.00 | lr 1.8000e-04 | time: 9058.47ms | tokens/mssec: 57.88\n",
      "for iter: 73 | loss: 5.8637919425964355 | norm: 0.00 | lr 1.8000e-04 | time: 8995.68ms | tokens/mssec: 58.28\n",
      "for iter: 74 | loss: 5.857000827789307 | norm: 0.00 | lr 1.8000e-04 | time: 9055.87ms | tokens/mssec: 57.89\n",
      "for iter: 75 | loss: 5.851868152618408 | norm: 0.00 | lr 1.8000e-04 | time: 9004.04ms | tokens/mssec: 58.23\n",
      "for iter: 76 | loss: 5.847036361694336 | norm: 0.00 | lr 1.8000e-04 | time: 9056.60ms | tokens/mssec: 57.89\n",
      "for iter: 77 | loss: 5.856337547302246 | norm: 0.00 | lr 1.8000e-04 | time: 9009.46ms | tokens/mssec: 58.19\n",
      "for iter: 78 | loss: 5.8245978355407715 | norm: 0.00 | lr 1.8000e-04 | time: 9114.30ms | tokens/mssec: 57.52\n",
      "for iter: 79 | loss: 5.8388166427612305 | norm: 0.00 | lr 1.8000e-04 | time: 8992.30ms | tokens/mssec: 58.30\n",
      "for iter: 80 | loss: 5.818424224853516 | norm: 0.00 | lr 1.8000e-04 | time: 9062.21ms | tokens/mssec: 57.85\n",
      "for iter: 81 | loss: 5.810398101806641 | norm: 0.00 | lr 1.8000e-04 | time: 8989.20ms | tokens/mssec: 58.32\n"
     ]
    }
   ],
   "source": [
    "transformer_model.train()\n",
    "for iter in range(total_iteration): # total_iteration = num_epochs * train_loader.num_iterations\n",
    "    start_time = time.time()\n",
    "    optimizer.zero_grad()\n",
    "    loss_accum = 0.0\n",
    "    for _ in range(grad_accumulation_steps): # grad_accumulation_steps = ideal_batch_size // (permit_batch * permit_context)\n",
    "        # get the next batch\n",
    "        x, y = train_loader.next_batch()\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        # forward pass\n",
    "        with ctx:\n",
    "            logits, loss = transformer_model(x, y)\n",
    "\n",
    "        loss = (\n",
    "            loss / grad_accumulation_steps\n",
    "        )  # divide the loss by accumulation steps to get the average loss\n",
    "        loss_accum += loss.detach()  # accumulate the loss\n",
    "        \n",
    "        # backward pass with GradScaler\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "    norm = 0.0\n",
    "    if grad_clip != 0.0:\n",
    "        # unscales the gradients of optimizer's assigned params in-place\n",
    "        scaler.unscale_(optimizer)\n",
    "        # clip the gradients\n",
    "        norm = torch.nn.utils.clip_grad_norm(transformer_model.parameters(), grad_clip)\n",
    "\n",
    "    # get the current learning rate\n",
    "    lr = get_schedule_lr(iter, warmup_steps, max_lr, int(total_iteration * 0.8), min_lr)\n",
    "    # update the learning rate\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = lr\n",
    "\n",
    "    # Unscales the gradients and calls optimizer.step()\n",
    "    scaler.step(optimizer)\n",
    "    # Updates the scale for next iteration\n",
    "    scaler.update()\n",
    "    end_time = time.time()\n",
    "    torch.cuda.empty_cache()  # clear the cache\n",
    "    torch.cuda.synchronize()  # wait for the computation to be done\n",
    "    time_elapsed = (end_time - start_time) * 1000\n",
    "    token_persec = (x.numel() * grad_accumulation_steps) / time_elapsed\n",
    "    print(\n",
    "        f\"for iter: {iter} | loss: {loss_accum.item()} | norm: {norm:.2f} | lr {lr:.4e} | time: {time_elapsed:.2f}ms | tokens/mssec: {token_persec:.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rest we will continue in the next version"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
