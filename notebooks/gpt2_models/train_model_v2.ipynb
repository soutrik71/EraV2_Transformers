{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will continue with the learnings form the v1 and apply it to the scale of -\n",
    "- training and evaluation\n",
    "- dataloader modified according to the both train and test data\n",
    "- longer runs with more epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autosave 300\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\n",
    "    \"/mnt/batch/tasks/shared/LS_root/mounts/clusters/insights-model-run2/code/Users/soutrik.chowdhury/EraV2_Transformers\"\n",
    ")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "from notebooks.gpt2_models.dummy_model import GPT, GPTConfig\n",
    "from torch.nn import functional as F\n",
    "import time\n",
    "from contextlib import nullcontext\n",
    "import inspect\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from typing import Tuple, List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New dataset producing both train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(os.getcwd(), \"data/tinyshakespeare\")\n",
    "encoder = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_train_val_token_array(data_dir, filename, encoder, split_ratio=0.9):\n",
    "    \"\"\"Read the file, encode it, split it into train and val, and save the token array as binary files\"\"\"\n",
    "    file_path = os.path.join(data_dir, filename)\n",
    "    print(f\"Reading file from {file_path}\")\n",
    "    with open(file_path, \"r\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    max_len = len(text)\n",
    "    train_str = text[: int(split_ratio * max_len)]\n",
    "    val_str = text[int(split_ratio * max_len) :]\n",
    "\n",
    "    print(f\"Train length: {len(train_str)}\")\n",
    "    print(f\"Val length: {len(val_str)}\")\n",
    "\n",
    "    train_tokens = encoder.encode(train_str)\n",
    "    val_tokens = encoder.encode(val_str)\n",
    "\n",
    "    print(f\"Train tokens: {len(train_tokens)}\")\n",
    "    print(f\"Val tokens: {len(val_tokens)}\")\n",
    "\n",
    "    train_ids = np.array(train_tokens, dtype=np.uint16)\n",
    "    val_ids = np.array(val_tokens, dtype=np.uint16)\n",
    "    train_ids.tofile(os.path.join(data_dir, \"train.bin\"))\n",
    "    val_ids.tofile(os.path.join(data_dir, \"val.bin\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_train_val_token_array(data_dir, \"input.txt\", encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_token_array(data_dir, filename):\n",
    "    \"\"\"Load the token array from binary file\"\"\"\n",
    "    token_data = np.memmap(\n",
    "        os.path.join(data_dir, f\"{filename}.bin\"), dtype=np.uint16, mode=\"r\"\n",
    "    )\n",
    "    token_data = torch.from_numpy(token_data.astype(np.int64))\n",
    "    return token_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_tokens = load_token_array(data_dir, \"train\")\n",
    "# val_tokens = load_token_array(data_dir, \"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataloaderLite:\n",
    "    def __init__(self, B: int, T: int, data_dir: str, filename: str):\n",
    "        self.B = B  # batch size\n",
    "        self.T = T  # seq length\n",
    "        self.data_dir = data_dir\n",
    "        self.filename = filename\n",
    "        assert filename in [\"train\", \"val\"], \"Only 'train' and 'val' files are allowed\"\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        self.tokens = load_token_array(self.data_dir, self.filename)\n",
    "        print(f\"Total tokens in the file: {len(self.tokens)}\")\n",
    "        self.current_position = 0\n",
    "        self.num_iterations = len(self.tokens) // (self.B * self.T)\n",
    "        print(\n",
    "            f\"1 epoch will have {self.num_iterations} iterations given Batch size={self.B} and Context length={self.T}\"\n",
    "        )\n",
    "\n",
    "    def next_batch(self) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        B, T = self.B, self.T\n",
    "        # print(\n",
    "        #     f\"Current position: {self.current_position} and total tokens: {len(self.tokens)}\"\n",
    "        # )\n",
    "        if self.current_position + B * T + 1 > len(self.tokens):\n",
    "            # print(\n",
    "            #     \"Resetting the position for the next batch as current iter exhausts the data file\"\n",
    "            # )\n",
    "            self.reset()\n",
    "\n",
    "        end_position = self.current_position + B * T + 1\n",
    "        current_batch = (\n",
    "            self.tokens[self.current_position : end_position].clone().detach()\n",
    "        )  # clone and detach to avoid memory leak\n",
    "\n",
    "        # reshaping the tensor to (B, T) shape and creating input and target tensors\n",
    "        x = current_batch[:-1].view(B, T)  # input tensor of shape (B, T)\n",
    "        y = current_batch[1:].view(B, T)  # target tensor of shape (B, T)\n",
    "\n",
    "        # updating the position for the next batch\n",
    "        self.current_position += B * T\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lr = 6e-4\n",
    "min_lr = max_lr * 0.3\n",
    "warmup_steps = 10\n",
    "weight_decay = 0.1\n",
    "num_epochs = 2\n",
    "initial_iterations = 100\n",
    "min_val_iter = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic settings\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "device_type = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\n",
    "    f\" Device: {device_type} and Device count: {torch.cuda.device_count()} and Device name: {torch.cuda.get_device_name()}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ideal_batch_size = 524288  # as per GPT3 paper\n",
    "permit_batch = 8  # good enough 1 core gpu\n",
    "permit_context = 1024\n",
    "# grad_accumulation_steps = ideal_batch_size // (permit_batch * permit_context)\n",
    "grad_accumulation_steps = ideal_batch_size // (permit_batch * permit_context)\n",
    "print(f\"grad_accumulation_steps: {grad_accumulation_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# permisssion for grad_clip\n",
    "grad_clip = 0.0\n",
    "# model compilation\n",
    "compile = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataloaderLite(permit_batch, permit_context, data_dir, \"train\")\n",
    "val_dataloader = DataloaderLite(permit_batch, permit_context, data_dir, \"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(37):\n",
    "#     x, y = train_dataloader.next_batch()\n",
    "#     print(x.shape, y.shape)\n",
    "#     print(x[0, :10])\n",
    "#     print(y[0, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(5):\n",
    "#     x, y = val_dataloader.next_batch()\n",
    "#     print(x.shape, y.shape)\n",
    "#     print(x[0, :10])\n",
    "#     print(y[0, :10])\n",
    "#     print(\"===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_schedule_lr(\n",
    "    it, warmup_iters=500, learning_rate=0.1, lr_decay_iters=1000, min_lr=0.01\n",
    "):\n",
    "    \"\"\"Get the learning rate schedule for training after cosine annealing\"\"\"\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * (it + 1) / warmup_iters\n",
    "    if it > lr_decay_iters:\n",
    "        return min_lr\n",
    "\n",
    "    # Cosine annealing learning rate schedule\n",
    "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
    "    return min_lr + coeff * (learning_rate - min_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_optimizers(model, weight_decay, learning_rate, device_type):\n",
    "    \"\"\"Create the optimizer and scheduler for training\"\"\"\n",
    "\n",
    "    # start with all of the candidate parameters (that require grad)\n",
    "    param_dict = {pn: p for pn, p in model.named_parameters()}\n",
    "    param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "\n",
    "    # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
    "    # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
    "    decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "    nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "    optim_groups = [\n",
    "        {\"params\": decay_params, \"weight_decay\": weight_decay},\n",
    "        {\"params\": nodecay_params, \"weight_decay\": 0.0},\n",
    "    ]\n",
    "\n",
    "    num_decay_params = sum(p.numel() for p in decay_params)\n",
    "    num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "\n",
    "    print(\n",
    "        f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\"\n",
    "    )\n",
    "    print(\n",
    "        f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\"\n",
    "    )\n",
    "    # Create AdamW optimizer and use the fused version if it is available\n",
    "    fused_available = \"fused\" in inspect.signature(torch.optim.AdamW).parameters\n",
    "    use_fused = fused_available and device_type == \"cuda\"\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        optim_groups, lr=learning_rate, betas=(0.9, 0.95), eps=1e-8, fused=use_fused\n",
    "    )\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = (\n",
    "    \"bfloat16\"\n",
    "    if torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "    else \"float16\"\n",
    ")\n",
    "print(f\"Using {dtype} for automatic mixed precision training\")\n",
    "\n",
    "# note: float16 data type will automatically use a GradScaler\n",
    "ptdtype = {\n",
    "    \"float32\": torch.float32,\n",
    "    \"bfloat16\": torch.bfloat16,\n",
    "    \"float16\": torch.float16,\n",
    "}[dtype]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# context manager for automatic mixed precision training\n",
    "ctx = (\n",
    "    nullcontext()\n",
    "    if device_type == \"cpu\"\n",
    "    else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    ")\n",
    "\n",
    "if ctx != nullcontext():\n",
    "    # GradScaler for automatic mixed precision training\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "transformer_model = GPT(\n",
    "    GPTConfig(\n",
    "        vocab_size=50304,\n",
    "        block_size=1024,\n",
    "        n_layer=12,\n",
    "        n_head=12,\n",
    "        n_embd=768,\n",
    "    )\n",
    ")\n",
    "transformer_model.to(device)\n",
    "\n",
    "if compile:\n",
    "    transformer_model = torch.compile(transformer_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_iteration = num_epochs * train_dataloader.num_iterations\n",
    "print(f\"Total iterations: {total_iteration}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = configure_optimizers(transformer_model, weight_decay, max_lr, device_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iter in range(\n",
    "    total_iteration\n",
    "):  # total_iteration = num_epochs * train_loader.num_iterations\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # validate the model after every epoch\n",
    "    if iter % train_dataloader.num_iterations == 0:\n",
    "        print(f\"Epoch: {iter // train_dataloader.num_iterations}\")\n",
    "        print(\"Model Evaluation.....\")\n",
    "        transformer_model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss_accum = 0.0\n",
    "            iter_range = min(min_val_iter, val_dataloader.num_iterations)\n",
    "            for _ in range(iter_range):\n",
    "                x, y = val_dataloader.next_batch()\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                with ctx:\n",
    "                    logits, loss = transformer_model(x, y)\n",
    "                loss = (\n",
    "                    loss / iter_range\n",
    "                )  # divide the loss by accumulation steps to get the average loss\n",
    "                val_loss_accum += loss.detach()  # accumulate the loss\n",
    "\n",
    "        print(f\"validation loss: {val_loss_accum.item():.4f}\")\n",
    "\n",
    "    print(\"Model Training.....\")\n",
    "    transformer_model.train()\n",
    "    optimizer.zero_grad()\n",
    "    loss_accum = 0.0\n",
    "    for micro_step in range(\n",
    "        grad_accumulation_steps\n",
    "    ):  # grad_accumulation_steps = ideal_batch_size // (permit_batch * permit_context)\n",
    "        # get the next batch\n",
    "        x, y = train_dataloader.next_batch()\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        # forward pass\n",
    "        with ctx:\n",
    "            logits, loss = transformer_model(x, y)\n",
    "\n",
    "        loss = (\n",
    "            loss / grad_accumulation_steps\n",
    "        )  # divide the loss by accumulation steps to get the average loss\n",
    "        loss_accum += loss.detach()  # accumulate the loss\n",
    "\n",
    "        # backward pass with GradScaler\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "    norm = 0.0\n",
    "    if grad_clip != 0.0:\n",
    "        # unscales the gradients of optimizer's assigned params in-place\n",
    "        scaler.unscale_(optimizer)\n",
    "        # clip the gradients\n",
    "        norm = torch.nn.utils.clip_grad_norm(transformer_model.parameters(), grad_clip)\n",
    "\n",
    "    # get the current learning rate\n",
    "    lr = get_schedule_lr(iter, warmup_steps, max_lr, int(total_iteration * 0.8), min_lr)\n",
    "    # update the learning rate\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = lr\n",
    "\n",
    "    # Unscales the gradients and calls optimizer.step()\n",
    "    scaler.step(optimizer)\n",
    "    # Updates the scale for next iteration\n",
    "    scaler.update()\n",
    "    end_time = time.time()\n",
    "    torch.cuda.empty_cache()  # clear the cache\n",
    "    torch.cuda.synchronize()  # wait for the computation to be done\n",
    "    time_elapsed = (end_time - start_time) * 1000\n",
    "    token_persec = (x.numel() * grad_accumulation_steps) / time_elapsed\n",
    "    print(\n",
    "        f\"for iter: {iter} | trainloss: {loss_accum.item()} | validation loss {val_loss_accum.item()} | norm: {norm:.2f} | lr {lr:.4e} | time: {time_elapsed:.2f}ms | tokens/mssec: {token_persec:.2f}\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
