{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebpook, we will discuss about different tokenization techniques available while training a model:\n",
    "- Word Tokenization\n",
    "- Sentence Tokenization\n",
    "- Byte Pair Encoding (BPE)\n",
    "- SentencePiece\n",
    "- Tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "IPython.notebook.set_autosave_interval(300000)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 300 seconds\n"
     ]
    }
   ],
   "source": [
    "%autosave 300\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/batch/tasks/shared/LS_root/mounts/clusters/insights-model-run2/code/Users/soutrik.chowdhury/EraV2_Transformers\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\n",
    "    \"/mnt/batch/tasks/shared/LS_root/mounts/clusters/insights-model-run2/code/Users/soutrik.chowdhury/EraV2_Transformers\"\n",
    ")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimal (byte-level) Byte Pair Encoding tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('data/taylor1.txt', <http.client.HTTPMessage at 0x7efde44c4eb0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "url = 'https://raw.githubusercontent.com/karpathy/minbpe/master/tests/taylorswift.txt'\n",
    "filename = 'data/taylor1.txt'\n",
    "urllib.request.urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/taylor1.txt', 'r') as file:\n",
    "    data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185561\n"
     ]
    }
   ],
   "source": [
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "merges = {} # (int, int) -> int\n",
    "pattern = \"\" # str\n",
    "special_tokens = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _build_vocab():\n",
    "    \"\"\"Position wise byte encoding for all 256 bytes\"\"\"\n",
    "    # vocab is simply and deterministically derived from merges\n",
    "    vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "    for (p0, p1), idx in merges.items():\n",
    "        vocab[idx] = vocab[p0] + vocab[p1]\n",
    "    for special, idx in special_tokens.items():\n",
    "        vocab[idx] = special.encode(\"utf-8\")\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = _build_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(ids, counts=None):\n",
    "    \"\"\"\n",
    "    Given a list of integers, return a dictionary of counts of consecutive pairs\n",
    "    Example: [1, 2, 3, 1, 2] -> {(1, 2): 2, (2, 3): 1, (3, 1): 1}\n",
    "    Optionally allows to update an existing dictionary of counts\n",
    "    \"\"\"\n",
    "    counts = {} if counts is None else counts\n",
    "    for pair in zip(ids, ids[1:]):  # iterate consecutive elements\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "\n",
    "def merge(ids, pair, idx):\n",
    "    \"\"\"\n",
    "    In the list of integers (ids), replace all consecutive occurrences\n",
    "    of pair with the new integer token idx\n",
    "    Example: ids=[1, 2, 3, 1, 2], pair=(1, 2), idx=4 -> [4, 3, 4]\n",
    "    \"\"\"\n",
    "    newids = []\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        # if not at the very last position AND the pair matches, replace it\n",
    "        if ids[i] == pair[0] and i < len(ids) - 1 and ids[i + 1] == pair[1]:\n",
    "            newids.append(idx)\n",
    "            i += 2\n",
    "        else:\n",
    "            newids.append(ids[i])\n",
    "            i += 1\n",
    "    return newids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_control_characters(s: str) -> str:\n",
    "    # we don't want to print control characters\n",
    "    # which distort the output (e.g. \\n or much worse)\n",
    "    # https://stackoverflow.com/questions/4324790/removing-control-characters-from-a-string-in-python/19016117#19016117\n",
    "    # http://www.unicode.org/reports/tr44/#GC_Values_Table\n",
    "    chars = []\n",
    "    for ch in s:\n",
    "        if unicodedata.category(ch)[0] != \"C\":\n",
    "            chars.append(ch)  # this character is ok\n",
    "        else:\n",
    "            chars.append(f\"\\\\u{ord(ch):04x}\")  # escape\n",
    "    return \"\".join(chars)\n",
    "\n",
    "\n",
    "def render_token(t: bytes) -> str:\n",
    "    # pretty print a token, escaping control characters\n",
    "    s = t.decode(\"utf-8\", errors=\"replace\")\n",
    "    s = replace_control_characters(s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(text, vocab_size, verbose=False):\n",
    "    assert vocab_size >= 256\n",
    "    # the number of merges is equal to the vocab size minus the number of bytes\n",
    "    num_merges = vocab_size - 256\n",
    "\n",
    "    # input text preprocessing\n",
    "    text_bytes = text.encode(\"utf-8\")  # raw bytes\n",
    "    ids = list(text_bytes)  # list of integers in range 0..255\n",
    "\n",
    "    # iteratively merge the most common pairs to create new tokens\n",
    "    merges = {}  # (int, int) -> int\n",
    "    vocab = {idx: bytes([idx]) for idx in range(256)}  # int -> bytes\n",
    "    for i in range(num_merges):\n",
    "        # count up the number of times every consecutive pair appears\n",
    "        stats = get_stats(ids)\n",
    "        # find the pair with the highest count\n",
    "        pair = max(stats, key=stats.get)\n",
    "        # mint a new token: assign it the next available id\n",
    "        idx = 256 + i\n",
    "        # replace all occurrences of pair in ids with idx\n",
    "        ids = merge(ids, pair, idx)\n",
    "        # save the merge\n",
    "        merges[pair] = idx\n",
    "        vocab[idx] = vocab[pair[0]] + vocab[pair[1]]\n",
    "        # prints\n",
    "        if verbose:\n",
    "            print(\n",
    "                f\"merge {i+1}/{num_merges}: {pair} -> {idx} ({vocab[idx]}) had {stats[pair]} occurrences\"\n",
    "            )\n",
    "\n",
    "    # save class variables\n",
    "    merges = merges  # pair of unicodes merged, used in encode()\n",
    "    vocab = vocab  # index position : letter/subword used in decode()\n",
    "\n",
    "    return vocab, merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merge 1/256: (101, 32) -> 256 (b'e ') had 2981 occurrences\n",
      "merge 2/256: (44, 32) -> 257 (b', ') had 2961 occurrences\n",
      "merge 3/256: (100, 32) -> 258 (b'd ') had 2617 occurrences\n",
      "merge 4/256: (46, 32) -> 259 (b'. ') had 2560 occurrences\n",
      "merge 5/256: (114, 32) -> 260 (b'r ') had 2428 occurrences\n",
      "merge 6/256: (50, 48) -> 261 (b'20') had 2365 occurrences\n",
      "merge 7/256: (115, 32) -> 262 (b's ') had 2053 occurrences\n",
      "merge 8/256: (105, 110) -> 263 (b'in') had 2006 occurrences\n",
      "merge 9/256: (111, 110) -> 264 (b'on') had 1815 occurrences\n",
      "merge 10/256: (114, 105) -> 265 (b'ri') had 1805 occurrences\n",
      "merge 11/256: (116, 32) -> 266 (b't ') had 1802 occurrences\n",
      "merge 12/256: (116, 104) -> 267 (b'th') had 1737 occurrences\n",
      "merge 13/256: (101, 258) -> 268 (b'ed ') had 1736 occurrences\n",
      "merge 14/256: (257, 261) -> 269 (b', 20') had 1705 occurrences\n",
      "merge 15/256: (97, 110) -> 270 (b'an') had 1487 occurrences\n",
      "merge 16/256: (97, 114) -> 271 (b'ar') had 1360 occurrences\n",
      "merge 17/256: (101, 260) -> 272 (b'er ') had 1356 occurrences\n",
      "merge 18/256: (121, 32) -> 273 (b'y ') had 1248 occurrences\n",
      "merge 19/256: (97, 108) -> 274 (b'al') had 1164 occurrences\n",
      "merge 20/256: (267, 256) -> 275 (b'the ') had 1142 occurrences\n",
      "merge 21/256: (118, 268) -> 276 (b'ved ') had 1104 occurrences\n",
      "merge 22/256: (119, 105) -> 277 (b'wi') had 1049 occurrences\n",
      "merge 23/256: (101, 114) -> 278 (b'er') had 897 occurrences\n",
      "merge 24/256: (264, 32) -> 279 (b'on ') had 880 occurrences\n",
      "merge 25/256: (277, 102) -> 280 (b'wif') had 871 occurrences\n",
      "merge 26/256: (82, 101) -> 281 (b'Re') had 870 occurrences\n",
      "merge 27/256: (83, 280) -> 282 (b'Swif') had 867 occurrences\n",
      "merge 28/256: (111, 260) -> 283 (b'or ') had 859 occurrences\n",
      "merge 29/256: (99, 104) -> 284 (b'ch') had 816 occurrences\n",
      "merge 30/256: (269, 49) -> 285 (b', 201') had 811 occurrences\n",
      "merge 31/256: (111, 109) -> 286 (b'om') had 789 occurrences\n",
      "merge 32/256: (98, 272) -> 287 (b'ber ') had 752 occurrences\n",
      "merge 33/256: (32, 275) -> 288 (b' the ') had 748 occurrences\n",
      "merge 34/256: (97, 121) -> 289 (b'ay') had 744 occurrences\n",
      "merge 35/256: (101, 110) -> 290 (b'en') had 740 occurrences\n",
      "merge 36/256: (111, 114) -> 291 (b'or') had 737 occurrences\n",
      "merge 37/256: (274, 32) -> 292 (b'al ') had 705 occurrences\n",
      "merge 38/256: (101, 109) -> 293 (b'em') had 703 occurrences\n",
      "merge 39/256: (46, 10) -> 294 (b'.\\n') had 685 occurrences\n",
      "merge 40/256: (265, 101) -> 295 (b'rie') had 685 occurrences\n",
      "merge 41/256: (263, 103) -> 296 (b'ing') had 684 occurrences\n",
      "merge 42/256: (269, 50) -> 297 (b', 202') had 673 occurrences\n",
      "merge 43/256: (116, 105) -> 298 (b'ti') had 666 occurrences\n",
      "merge 44/256: (289, 108) -> 299 (b'ayl') had 654 occurrences\n",
      "merge 45/256: (34, 259) -> 300 (b'\". ') had 651 occurrences\n",
      "merge 46/256: (108, 108) -> 301 (b'll') had 649 occurrences\n",
      "merge 47/256: (84, 299) -> 302 (b'Tayl') had 647 occurrences\n",
      "merge 48/256: (116, 295) -> 303 (b'trie') had 644 occurrences\n",
      "merge 49/256: (294, 32) -> 304 (b'.\\n ') had 643 occurrences\n",
      "merge 50/256: (116, 111) -> 305 (b'to') had 642 occurrences\n",
      "merge 51/256: (259, 281) -> 306 (b'. Re') had 640 occurrences\n",
      "merge 52/256: (306, 303) -> 307 (b'. Retrie') had 639 occurrences\n",
      "merge 53/256: (307, 276) -> 308 (b'. Retrieved ') had 639 occurrences\n",
      "merge 54/256: (302, 283) -> 309 (b'Taylor ') had 611 occurrences\n",
      "merge 55/256: (101, 115) -> 310 (b'es') had 606 occurrences\n",
      "merge 56/256: (309, 282) -> 311 (b'Taylor Swif') had 598 occurrences\n",
      "merge 57/256: (117, 115) -> 312 (b'us') had 561 occurrences\n",
      "merge 58/256: (114, 286) -> 313 (b'rom') had 532 occurrences\n",
      "merge 59/256: (293, 287) -> 314 (b'ember ') had 528 occurrences\n",
      "merge 60/256: (41, 259) -> 315 (b'). ') had 524 occurrences\n",
      "merge 61/256: (65, 114) -> 316 (b'Ar') had 509 occurrences\n",
      "merge 62/256: (102, 313) -> 317 (b'from') had 503 occurrences\n",
      "merge 63/256: (315, 34) -> 318 (b'). \"') had 499 occurrences\n",
      "merge 64/256: (270, 258) -> 319 (b'and ') had 498 occurrences\n",
      "merge 65/256: (114, 101) -> 320 (b're') had 495 occurrences\n",
      "merge 66/256: (111, 117) -> 321 (b'ou') had 487 occurrences\n",
      "merge 67/256: (111, 265) -> 322 (b'ori') had 469 occurrences\n",
      "merge 68/256: (111, 102) -> 323 (b'of') had 466 occurrences\n",
      "merge 69/256: (103, 263) -> 324 (b'gin') had 465 occurrences\n",
      "merge 70/256: (296, 32) -> 325 (b'ing ') had 464 occurrences\n",
      "merge 71/256: (284, 105) -> 326 (b'chi') had 458 occurrences\n",
      "merge 72/256: (93, 32) -> 327 (b'] ') had 458 occurrences\n",
      "merge 73/256: (324, 292) -> 328 (b'ginal ') had 453 occurrences\n",
      "merge 74/256: (317, 288) -> 329 (b'from the ') had 447 occurrences\n",
      "merge 75/256: (322, 328) -> 330 (b'original ') had 446 occurrences\n",
      "merge 76/256: (104, 256) -> 331 (b'he ') had 440 occurrences\n",
      "merge 77/256: (316, 326) -> 332 (b'Archi') had 440 occurrences\n",
      "merge 78/256: (332, 276) -> 333 (b'Archived ') had 440 occurrences\n",
      "merge 79/256: (329, 330) -> 334 (b'from the original ') had 440 occurrences\n",
      "merge 80/256: (333, 334) -> 335 (b'Archived from the original ') had 439 occurrences\n",
      "merge 81/256: (335, 279) -> 336 (b'Archived from the original on ') had 438 occurrences\n",
      "merge 82/256: (259, 336) -> 337 (b'. Archived from the original on ') had 433 occurrences\n",
      "merge 83/256: (97, 32) -> 338 (b'a ') had 420 occurrences\n",
      "merge 84/256: (115, 116) -> 339 (b'st') had 409 occurrences\n",
      "merge 85/256: (105, 99) -> 340 (b'ic') had 406 occurrences\n",
      "merge 86/256: (46, 91) -> 341 (b'.[') had 381 occurrences\n",
      "merge 87/256: (101, 99) -> 342 (b'ec') had 374 occurrences\n",
      "merge 88/256: (105, 301) -> 343 (b'ill') had 367 occurrences\n",
      "merge 89/256: (39, 262) -> 344 (b\"'s \") had 367 occurrences\n",
      "merge 90/256: (311, 266) -> 345 (b'Taylor Swift ') had 352 occurrences\n",
      "merge 91/256: (111, 118) -> 346 (b'ov') had 343 occurrences\n",
      "merge 92/256: (97, 116) -> 347 (b'at') had 334 occurrences\n",
      "merge 93/256: (97, 262) -> 348 (b'as ') had 315 occurrences\n",
      "merge 94/256: (101, 262) -> 349 (b'es ') had 309 occurrences\n",
      "merge 95/256: (74, 117) -> 350 (b'Ju') had 307 occurrences\n",
      "merge 96/256: (323, 32) -> 351 (b'of ') had 306 occurrences\n",
      "merge 97/256: (305, 32) -> 352 (b'to ') had 284 occurrences\n",
      "merge 98/256: (117, 109) -> 353 (b'um') had 281 occurrences\n",
      "merge 99/256: (84, 331) -> 354 (b'The ') had 277 occurrences\n",
      "merge 100/256: (271, 100) -> 355 (b'ard') had 277 occurrences\n",
      "merge 101/256: (263, 32) -> 356 (b'in ') had 276 occurrences\n",
      "merge 102/256: (270, 32) -> 357 (b'an ') had 276 occurrences\n",
      "merge 103/256: (101, 108) -> 358 (b'el') had 275 occurrences\n",
      "merge 104/256: (297, 51) -> 359 (b', 2023') had 271 occurrences\n",
      "merge 105/256: (271, 273) -> 360 (b'ary ') had 259 occurrences\n",
      "merge 106/256: (267, 32) -> 361 (b'th ') had 258 occurrences\n",
      "merge 107/256: (97, 109) -> 362 (b'am') had 257 occurrences\n",
      "merge 108/256: (108, 273) -> 363 (b'ly ') had 250 occurrences\n",
      "merge 109/256: (111, 112) -> 364 (b'op') had 249 occurrences\n",
      "merge 110/256: (311, 116) -> 365 (b'Taylor Swift') had 246 occurrences\n",
      "merge 111/256: (116, 114) -> 366 (b'tr') had 243 occurrences\n",
      "merge 112/256: (105, 115) -> 367 (b'is') had 234 occurrences\n",
      "merge 113/256: (104, 272) -> 368 (b'her ') had 232 occurrences\n",
      "merge 114/256: (111, 32) -> 369 (b'o ') had 225 occurrences\n",
      "merge 115/256: (117, 360) -> 370 (b'uary ') had 225 occurrences\n",
      "merge 116/256: (78, 346) -> 371 (b'Nov') had 222 occurrences\n",
      "merge 117/256: (312, 340) -> 372 (b'usic') had 221 occurrences\n",
      "merge 118/256: (371, 314) -> 373 (b'November ') had 221 occurrences\n",
      "merge 119/256: (101, 119) -> 374 (b'ew') had 219 occurrences\n",
      "merge 120/256: (97, 266) -> 375 (b'at ') had 219 occurrences\n",
      "merge 121/256: (108, 32) -> 376 (b'l ') had 218 occurrences\n",
      "merge 122/256: (58, 32) -> 377 (b': ') had 213 occurrences\n",
      "merge 123/256: (98, 111) -> 378 (b'bo') had 210 occurrences\n",
      "merge 124/256: (282, 266) -> 379 (b'Swift ') had 208 occurrences\n",
      "merge 125/256: (68, 342) -> 380 (b'Dec') had 207 occurrences\n",
      "merge 126/256: (105, 116) -> 381 (b'it') had 206 occurrences\n",
      "merge 127/256: (105, 103) -> 382 (b'ig') had 205 occurrences\n",
      "merge 128/256: (66, 343) -> 383 (b'Bill') had 205 occurrences\n",
      "merge 129/256: (49, 48) -> 384 (b'10') had 204 occurrences\n",
      "merge 130/256: (97, 115) -> 385 (b'as') had 203 occurrences\n",
      "merge 131/256: (264, 103) -> 386 (b'ong') had 202 occurrences\n",
      "merge 132/256: (79, 99) -> 387 (b'Oc') had 200 occurrences\n",
      "merge 133/256: (97, 298) -> 388 (b'ati') had 199 occurrences\n",
      "merge 134/256: (83, 116) -> 389 (b'St') had 198 occurrences\n",
      "merge 135/256: (387, 305) -> 390 (b'Octo') had 198 occurrences\n",
      "merge 136/256: (390, 287) -> 391 (b'October ') had 198 occurrences\n",
      "merge 137/256: (97, 99) -> 392 (b'ac') had 197 occurrences\n",
      "merge 138/256: (111, 119) -> 393 (b'ow') had 196 occurrences\n",
      "merge 139/256: (380, 314) -> 394 (b'December ') had 194 occurrences\n",
      "merge 140/256: (383, 378) -> 395 (b'Billbo') had 191 occurrences\n",
      "merge 141/256: (97, 100) -> 396 (b'ad') had 190 occurrences\n",
      "merge 142/256: (108, 101) -> 397 (b'le') had 190 occurrences\n",
      "merge 143/256: (117, 114) -> 398 (b'ur') had 188 occurrences\n",
      "merge 144/256: (102, 283) -> 399 (b'for ') had 188 occurrences\n",
      "merge 145/256: (32, 40) -> 400 (b' (') had 187 occurrences\n",
      "merge 146/256: (297, 50) -> 401 (b', 2022') had 187 occurrences\n",
      "merge 147/256: (117, 103) -> 402 (b'ug') had 185 occurrences\n",
      "merge 148/256: (284, 32) -> 403 (b'ch ') had 184 occurrences\n",
      "merge 149/256: (115, 266) -> 404 (b'st ') had 181 occurrences\n",
      "merge 150/256: (321, 110) -> 405 (b'oun') had 176 occurrences\n",
      "merge 151/256: (98, 353) -> 406 (b'bum') had 172 occurrences\n",
      "merge 152/256: (111, 108) -> 407 (b'ol') had 171 occurrences\n",
      "merge 153/256: (312, 266) -> 408 (b'ust ') had 171 occurrences\n",
      "merge 154/256: (101, 98) -> 409 (b'eb') had 170 occurrences\n",
      "merge 155/256: (77, 97) -> 410 (b'Ma') had 170 occurrences\n",
      "merge 156/256: (350, 363) -> 411 (b'July ') had 170 occurrences\n",
      "merge 157/256: (318, 345) -> 412 (b'). \"Taylor Swift ') had 169 occurrences\n",
      "merge 158/256: (107, 32) -> 413 (b'k ') had 165 occurrences\n",
      "merge 159/256: (278, 115) -> 414 (b'ers') had 164 occurrences\n",
      "merge 160/256: (93, 91) -> 415 (b'][') had 164 occurrences\n",
      "merge 161/256: (65, 402) -> 416 (b'Aug') had 164 occurrences\n",
      "merge 162/256: (416, 408) -> 417 (b'August ') had 163 occurrences\n",
      "merge 163/256: (105, 100) -> 418 (b'id') had 161 occurrences\n",
      "merge 164/256: (297, 49) -> 419 (b', 2021') had 160 occurrences\n",
      "merge 165/256: (109, 101) -> 420 (b'me') had 159 occurrences\n",
      "merge 166/256: (101, 112) -> 421 (b'ep') had 156 occurrences\n",
      "merge 167/256: (261, 49) -> 422 (b'201') had 149 occurrences\n",
      "merge 168/256: (50, 51) -> 423 (b'23') had 145 occurrences\n",
      "merge 169/256: (285, 50) -> 424 (b', 2012') had 144 occurrences\n",
      "merge 170/256: (101, 271) -> 425 (b'ear') had 140 occurrences\n",
      "merge 171/256: (269, 261) -> 426 (b', 2020') had 140 occurrences\n",
      "merge 172/256: (73, 110) -> 427 (b'In') had 139 occurrences\n",
      "merge 173/256: (102, 105) -> 428 (b'fi') had 139 occurrences\n",
      "merge 174/256: (110, 256) -> 429 (b'ne ') had 139 occurrences\n",
      "merge 175/256: (395, 355) -> 430 (b'Billboard') had 136 occurrences\n",
      "merge 176/256: (265, 116) -> 431 (b'rit') had 134 occurrences\n",
      "merge 177/256: (104, 105) -> 432 (b'hi') had 133 occurrences\n",
      "merge 178/256: (372, 32) -> 433 (b'usic ') had 133 occurrences\n",
      "merge 179/256: (304, 34) -> 434 (b'.\\n \"') had 133 occurrences\n",
      "merge 180/256: (78, 374) -> 435 (b'New') had 131 occurrences\n",
      "merge 181/256: (100, 105) -> 436 (b'di') had 130 occurrences\n",
      "merge 182/256: (65, 112) -> 437 (b'Ap') had 130 occurrences\n",
      "merge 183/256: (285, 57) -> 438 (b', 2019') had 129 occurrences\n",
      "merge 184/256: (114, 111) -> 439 (b'ro') had 128 occurrences\n",
      "merge 185/256: (39, 32) -> 440 (b\"' \") had 128 occurrences\n",
      "merge 186/256: (115, 257) -> 441 (b's, ') had 127 occurrences\n",
      "merge 187/256: (350, 429) -> 442 (b'June ') had 127 occurrences\n",
      "merge 188/256: (323, 288) -> 443 (b'of the ') had 126 occurrences\n",
      "merge 189/256: (99, 291) -> 444 (b'cor') had 126 occurrences\n",
      "merge 190/256: (50, 49) -> 445 (b'21') had 126 occurrences\n",
      "merge 191/256: (49, 57) -> 446 (b'19') had 124 occurrences\n",
      "merge 192/256: (105, 109) -> 447 (b'im') had 123 occurrences\n",
      "merge 193/256: (290, 32) -> 448 (b'en ') had 123 occurrences\n",
      "merge 194/256: (409, 114) -> 449 (b'ebr') had 122 occurrences\n",
      "merge 195/256: (290, 116) -> 450 (b'ent') had 121 occurrences\n",
      "merge 196/256: (111, 301) -> 451 (b'oll') had 121 occurrences\n",
      "merge 197/256: (77, 271) -> 452 (b'Mar') had 120 occurrences\n",
      "merge 198/256: (265, 99) -> 453 (b'ric') had 120 occurrences\n",
      "merge 199/256: (277, 361) -> 454 (b'with ') had 120 occurrences\n",
      "merge 200/256: (44, 91) -> 455 (b',[') had 118 occurrences\n",
      "merge 201/256: (70, 449) -> 456 (b'Febr') had 118 occurrences\n",
      "merge 202/256: (456, 370) -> 457 (b'February ') had 118 occurrences\n",
      "merge 203/256: (365, 344) -> 458 (b\"Taylor Swift's \") had 118 occurrences\n",
      "merge 204/256: (300, 430) -> 459 (b'\". Billboard') had 118 occurrences\n",
      "merge 205/256: (101, 97) -> 460 (b'ea') had 116 occurrences\n",
      "merge 206/256: (285, 54) -> 461 (b', 2016') had 116 occurrences\n",
      "merge 207/256: (421, 116) -> 462 (b'ept') had 115 occurrences\n",
      "merge 208/256: (410, 273) -> 463 (b'May ') had 115 occurrences\n",
      "merge 209/256: (285, 53) -> 464 (b', 2015') had 115 occurrences\n",
      "merge 210/256: (437, 265) -> 465 (b'Apri') had 115 occurrences\n",
      "merge 211/256: (465, 376) -> 466 (b'April ') had 115 occurrences\n",
      "merge 212/256: (108, 256) -> 467 (b'le ') had 113 occurrences\n",
      "merge 213/256: (65, 119) -> 468 (b'Aw') had 112 occurrences\n",
      "merge 214/256: (388, 264) -> 469 (b'ation') had 112 occurrences\n",
      "merge 215/256: (83, 462) -> 470 (b'Sept') had 112 occurrences\n",
      "merge 216/256: (470, 314) -> 471 (b'September ') had 112 occurrences\n",
      "merge 217/256: (114, 97) -> 472 (b'ra') had 111 occurrences\n",
      "merge 218/256: (274, 406) -> 473 (b'album') had 111 occurrences\n",
      "merge 219/256: (67, 104) -> 474 (b'Ch') had 110 occurrences\n",
      "merge 220/256: (118, 256) -> 475 (b've ') had 109 occurrences\n",
      "merge 221/256: (310, 266) -> 476 (b'est ') had 108 occurrences\n",
      "merge 222/256: (74, 270) -> 477 (b'Jan') had 108 occurrences\n",
      "merge 223/256: (50, 50) -> 478 (b'22') had 107 occurrences\n",
      "merge 224/256: (477, 370) -> 479 (b'January ') had 107 occurrences\n",
      "merge 225/256: (405, 366) -> 480 (b'ountr') had 106 occurrences\n",
      "merge 226/256: (382, 104) -> 481 (b'igh') had 106 occurrences\n",
      "merge 227/256: (300, 354) -> 482 (b'\". The ') had 106 occurrences\n",
      "merge 228/256: (359, 304) -> 483 (b', 2023.\\n ') had 106 occurrences\n",
      "merge 229/256: (49, 51) -> 484 (b'13') had 105 occurrences\n",
      "merge 230/256: (65, 108) -> 485 (b'Al') had 105 occurrences\n",
      "merge 231/256: (101, 116) -> 486 (b'et') had 105 occurrences\n",
      "merge 232/256: (310, 115) -> 487 (b'ess') had 103 occurrences\n",
      "merge 233/256: (452, 403) -> 488 (b'March ') had 103 occurrences\n",
      "merge 234/256: (117, 116) -> 489 (b'ut') had 102 occurrences\n",
      "merge 235/256: (119, 431) -> 490 (b'writ') had 101 occurrences\n",
      "merge 236/256: (108, 111) -> 491 (b'lo') had 99 occurrences\n",
      "merge 237/256: (115, 386) -> 492 (b'song') had 97 occurrences\n",
      "merge 238/256: (226, 128) -> 493 (b'\\xe2\\x80') had 97 occurrences\n",
      "merge 239/256: (271, 258) -> 494 (b'ard ') had 97 occurrences\n",
      "merge 240/256: (48, 32) -> 495 (b'0 ') had 97 occurrences\n",
      "merge 241/256: (117, 108) -> 496 (b'ul') had 96 occurrences\n",
      "merge 242/256: (50, 52) -> 497 (b'24') had 95 occurrences\n",
      "merge 243/256: (105, 262) -> 498 (b'is ') had 94 occurrences\n",
      "merge 244/256: (298, 99) -> 499 (b'tic') had 93 occurrences\n",
      "merge 245/256: (97, 103) -> 500 (b'ag') had 93 occurrences\n",
      "merge 246/256: (34, 32) -> 501 (b'\" ') had 93 occurrences\n",
      "merge 247/256: (65, 110) -> 502 (b'An') had 93 occurrences\n",
      "merge 248/256: (49, 56) -> 503 (b'18') had 93 occurrences\n",
      "merge 249/256: (102, 291) -> 504 (b'for') had 90 occurrences\n",
      "merge 250/256: (480, 273) -> 505 (b'ountry ') had 89 occurrences\n",
      "merge 251/256: (65, 420) -> 506 (b'Ame') had 88 occurrences\n",
      "merge 252/256: (506, 453) -> 507 (b'Americ') had 88 occurrences\n",
      "merge 253/256: (32, 84) -> 508 (b' T') had 88 occurrences\n",
      "merge 254/256: (115, 296) -> 509 (b'sing') had 87 occurrences\n",
      "merge 255/256: (119, 348) -> 510 (b'was ') had 86 occurrences\n",
      "merge 256/256: (49, 50) -> 511 (b'12') had 86 occurrences\n"
     ]
    }
   ],
   "source": [
    "vocab, merges = train(data, 512, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(file_prefix, pattern, special_tokens, merges, vocab):\n",
    "    \"\"\"\n",
    "    Saves two files: file_prefix.vocab and file_prefix.model\n",
    "    This is inspired (but not equivalent to!) sentencepiece's model saving:\n",
    "    - model file is the critical one, intended for load()\n",
    "    - vocab file is just a pretty printed version for human inspection only\n",
    "    \"\"\"\n",
    "    # write the model: to be used in load() later\n",
    "    model_file = file_prefix + \".model\"\n",
    "    with open(model_file, \"w\") as f:\n",
    "        # write the version, pattern and merges, that's all that's needed\n",
    "        f.write(\"minbpe v1\\n\")\n",
    "        f.write(f\"{pattern}\\n\")\n",
    "        # write the special tokens, first the number of them, then each one\n",
    "        f.write(f\"{len(special_tokens)}\\n\")\n",
    "        for special, idx in special_tokens.items():\n",
    "            f.write(f\"{special} {idx}\\n\")\n",
    "        # the merges dict\n",
    "        for idx1, idx2 in merges:\n",
    "            f.write(f\"{idx1} {idx2}\\n\")\n",
    "\n",
    "    # write the vocab: for the human to look at\n",
    "    vocab_file = file_prefix + \".vocab\"\n",
    "    inverted_merges = {idx: pair for pair, idx in merges.items()}\n",
    "    with open(vocab_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for idx, token in vocab.items():\n",
    "            # note: many tokens may be partial utf-8 sequences\n",
    "            # and cannot be decoded into valid strings. Here we're using\n",
    "            # errors='replace' to replace them with the replacement char �.\n",
    "            # this also means that we couldn't possibly use .vocab in load()\n",
    "            # because decoding in this way is a lossy operation!\n",
    "            s = render_token(token)\n",
    "            # find the children of this token, if any\n",
    "            if idx in inverted_merges:\n",
    "                # if this token has children, render it nicely as a merge\n",
    "                idx0, idx1 = inverted_merges[idx]\n",
    "                s0 = render_token(vocab[idx0])\n",
    "                s1 = render_token(vocab[idx1])\n",
    "                f.write(f\"[{s0}][{s1}] -> [{s}] {idx}\\n\")\n",
    "            else:\n",
    "                # otherwise this is leaf token, just print it\n",
    "                # (this should just be the first 256 tokens, the bytes)\n",
    "                f.write(f\"[{s}] {idx}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"tokenizers\", exist_ok=True)\n",
    "path = \"tokenizers/taylor1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "save(path, pattern, special_tokens, merges, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(model_file):\n",
    "    \"\"\"Inverse of save() but only for the model file\"\"\"\n",
    "    assert model_file.endswith(\".model\")\n",
    "    # read the model file\n",
    "    merges = {}\n",
    "    special_tokens = {}\n",
    "    idx = 256\n",
    "    with open(model_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        # read the version\n",
    "        version = f.readline().strip()\n",
    "        assert version == \"minbpe v1\"\n",
    "        # read the pattern\n",
    "        pattern = f.readline().strip()\n",
    "        # read the special tokens\n",
    "        num_special = int(f.readline().strip())\n",
    "        for _ in range(num_special):\n",
    "            special, special_idx = f.readline().strip().split()\n",
    "            special_tokens[special] = int(special_idx)\n",
    "        # read the merges\n",
    "        for line in f:\n",
    "            idx1, idx2 = map(int, line.split())\n",
    "            merges[(idx1, idx2)] = idx\n",
    "            idx += 1\n",
    "    merges = merges\n",
    "    special_tokens = special_tokens\n",
    "    vocab = _build_vocab()\n",
    "\n",
    "    return vocab, merges, special_tokens, pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = \"tokenizers/taylor1.model\"\n",
    "vocab, merges, special_tokens, pattern = load(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(ids):\n",
    "    # given ids (list of integers), return Python string\n",
    "    text_bytes = b\"\".join(vocab[idx] for idx in ids)\n",
    "    text = text_bytes.decode(\"utf-8\", errors=\"replace\")\n",
    "    return text\n",
    "\n",
    "\n",
    "def encode(text):\n",
    "    # given a string text, return the token ids\n",
    "    text_bytes = text.encode(\"utf-8\")  # raw bytes\n",
    "    ids = list(text_bytes)  # list of integers in range 0..255\n",
    "    while len(ids) >= 2:\n",
    "        # find the pair with the lowest merge index\n",
    "        stats = get_stats(ids)\n",
    "        pair = min(stats, key=lambda p: merges.get(p, float(\"inf\")))\n",
    "        # subtle: if there are no more merges available, the key will\n",
    "        # result in an inf for every single pair, and the min will be\n",
    "        # just the first pair in the list, arbitrarily\n",
    "        # we can detect this terminating case by a membership check\n",
    "        if pair not in merges:\n",
    "            break  # nothing else can be merged anymore\n",
    "        # otherwise let's merge the best pair (lowest merge index)\n",
    "        idx = merges[pair]\n",
    "        ids = merge(ids, pair, idx)\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"I love you Puchu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[73, 32, 108, 346, 256, 121, 321, 32, 80, 117, 284, 117]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I love you Puchu'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(encode(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
