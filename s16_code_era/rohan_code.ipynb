{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "IPython.notebook.set_autosave_interval(300000)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 300 seconds\n"
     ]
    }
   ],
   "source": [
    "%autosave 300\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/batch/tasks/shared/LS_root/mounts/clusters/insights-model-run2/code/Users/soutrik.chowdhury/EraV2_Transformers\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"/mnt/batch/tasks/shared/LS_root/mounts/clusters/insights-model-run2/code/Users/soutrik.chowdhury/EraV2_Transformers\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from s16_code_era.model import build_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from s16_code_era.dataset import BillingualDataset, casual_mask\n",
    "# from s16_code_era.config_file import get_config, get_weights_file_path\n",
    "\n",
    "# import torchtext.datasets as datasets\n",
    "# import torch\n",
    "\n",
    "# torch.cuda.amp.autocast(enabled=True)\n",
    "\n",
    "# import torch.nn as nn\n",
    "# from torch.utils.data import Dataset, DataLoader, random_split\n",
    "# from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "# import warnings\n",
    "# from tqdm import tqdm\n",
    "# import os\n",
    "# from pathlib import Path\n",
    "\n",
    "# from datasets import load_dataset\n",
    "# from tokenizers import Tokenizer\n",
    "# from tokenizers.models import WordLevel\n",
    "# from tokenizers.trainers import WordLevelTrainer\n",
    "# from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "# import torchmetrics\n",
    "# from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()\n",
    "# os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:12240\"\n",
    "# config = get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torch.utils.data import Dataset\n",
    "\n",
    "# class BilingualDataset(Dataset):\n",
    "\n",
    "#     def __init__(self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len):\n",
    "#         super().__init__()\n",
    "#         self.seq_len = seq_len\n",
    "\n",
    "#         self.ds = ds\n",
    "#         self.tokenizer_src = tokenizer_src\n",
    "#         self.tokenizer_tgt = tokenizer_tgt\n",
    "#         self.src_lang = src_lang\n",
    "#         self.tgt_lang = tgt_lang\n",
    "\n",
    "#         self.sos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[SOS]\")], dtype=torch.int64)\n",
    "#         self.eos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[EOS]\")], dtype=torch.int64)\n",
    "#         self.pad_token = torch.tensor([tokenizer_tgt.token_to_id(\"[PAD]\")], dtype=torch.int64)\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.ds)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         src_target_pair = self.ds[idx]\n",
    "#         src_text = src_target_pair['translation'][self.src_lang]\n",
    "#         tgt_text = src_target_pair['translation'][self.tgt_lang]\n",
    "\n",
    "#         # Transform the text into tokens\n",
    "#         enc_input_tokens = self.tokenizer_src.encode(src_text).ids\n",
    "#         dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n",
    "\n",
    "#         # Add sos, eos and padding to each sentence\n",
    "#         enc_num_padding_tokens = self.seq_len - len(enc_input_tokens) - 2  # We will add <s> and </s>\n",
    "#         # We will only add <s>, and </s> only on the label\n",
    "#         dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1\n",
    "\n",
    "#         # Make sure the number of padding tokens is not negative. If it is, the sentence is too long\n",
    "#         if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:\n",
    "#             raise ValueError(\"Sentence is too long\")\n",
    "\n",
    "#         # Add <s> and </s> token\n",
    "#         encoder_input = torch.cat(\n",
    "#             [\n",
    "#                 self.sos_token,\n",
    "#                 torch.tensor(enc_input_tokens, dtype=torch.int64),\n",
    "#                 self.eos_token,\n",
    "#                 torch.tensor([self.pad_token] * enc_num_padding_tokens, dtype=torch.int64),\n",
    "#             ],\n",
    "#             dim=0,\n",
    "#         )\n",
    "\n",
    "#         # Add only <s> token\n",
    "#         decoder_input = torch.cat(\n",
    "#             [\n",
    "#                 self.sos_token,\n",
    "#                 torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
    "#                 torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64),\n",
    "#             ],\n",
    "#             dim=0,\n",
    "#         )\n",
    "\n",
    "#         # Add only </s> token\n",
    "#         label = torch.cat(\n",
    "#             [\n",
    "#                 torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
    "#                 self.eos_token,\n",
    "#                 torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64),\n",
    "#             ],\n",
    "#             dim=0,\n",
    "#         )\n",
    "\n",
    "#         # Double check the size of the tensors to make sure they are all seq_len long\n",
    "#         assert encoder_input.size(0) == self.seq_len\n",
    "#         assert decoder_input.size(0) == self.seq_len\n",
    "#         assert label.size(0) == self.seq_len\n",
    "\n",
    "#         return {\n",
    "#             \"encoder_input\": encoder_input,  # (seq_len)\n",
    "#             \"decoder_input\": decoder_input,  # (seq_len)\n",
    "#             \"encoder_mask\": (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(), # (1, 1, seq_len)\n",
    "#             \"decoder_mask\": (decoder_input != self.pad_token).unsqueeze(0).int() & causal_mask(decoder_input.size(0)), # (1, seq_len) & (1, seq_len, seq_len),\n",
    "#             \"label\": label,  # (seq_len)\n",
    "#             \"src_text\": src_text,\n",
    "#             \"tgt_text\": tgt_text,\n",
    "#         }\n",
    "    \n",
    "# def causal_mask(size):\n",
    "#     mask = torch.triu(torch.ones((1, size, size)), diagonal=1).type(torch.int)\n",
    "#     return mask == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_all_sentences(ds, lang):\n",
    "#     for item in ds:\n",
    "#         yield item[\"translation\"][lang]\n",
    "\n",
    "\n",
    "# def get_or_build_tokenizer(config, ds, lang):\n",
    "#     tokenizer_path = Path(config[\"tokenizer_file\"].format(lang))\n",
    "#     if not Path.exists(tokenizer_path):\n",
    "#         # Most code taken from: https://huggingface.co/docs/tokenizers/quicktour\n",
    "#         tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "#         tokenizer.pre_tokenizer = Whitespace()\n",
    "#         trainer = WordLevelTrainer(\n",
    "#             special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], min_frequency=2\n",
    "#         )\n",
    "#         tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer=trainer)\n",
    "#         tokenizer.save(str(tokenizer_path))\n",
    "#     else:\n",
    "#         tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
    "#     return tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def get_ds(config):\n",
    "#     # It only has the train split, so we divide it overselves\n",
    "#     ds_raw = load_dataset(\n",
    "#         f\"{config['datasource']}\",\n",
    "#         f\"{config['lang_src']}-{config['lang_tgt']}\",\n",
    "#         split=\"train\",\n",
    "#     )\n",
    "\n",
    "#     # Build tokenizers\n",
    "#     tokenizer_src = get_or_build_tokenizer(config, ds_raw, config[\"lang_src\"])\n",
    "#     tokenizer_tgt = get_or_build_tokenizer(config, ds_raw, config[\"lang_tgt\"])\n",
    "\n",
    "#     # Keep 90% for training, 10% for validation\n",
    "#     train_ds_size = int(0.9 * len(ds_raw))\n",
    "#     val_ds_size = len(ds_raw) - train_ds_size\n",
    "#     train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size])\n",
    "\n",
    "#     train_ds = BilingualDataset(\n",
    "#         train_ds_raw,\n",
    "#         tokenizer_src,\n",
    "#         tokenizer_tgt,\n",
    "#         config[\"lang_src\"],\n",
    "#         config[\"lang_tgt\"],\n",
    "#         config[\"seq_len\"],\n",
    "#     )\n",
    "#     val_ds = BilingualDataset(\n",
    "#         val_ds_raw,\n",
    "#         tokenizer_src,\n",
    "#         tokenizer_tgt,\n",
    "#         config[\"lang_src\"],\n",
    "#         config[\"lang_tgt\"],\n",
    "#         config[\"seq_len\"],\n",
    "#     )\n",
    "\n",
    "#     # Find the maximum length of each sentence in the source and target sentence\n",
    "#     max_len_src = 0\n",
    "#     max_len_tgt = 0\n",
    "\n",
    "#     for item in ds_raw:\n",
    "#         src_ids = tokenizer_src.encode(item[\"translation\"][config[\"lang_src\"]]).ids\n",
    "#         tgt_ids = tokenizer_tgt.encode(item[\"translation\"][config[\"lang_tgt\"]]).ids\n",
    "#         max_len_src = max(max_len_src, len(src_ids))\n",
    "#         max_len_tgt = max(max_len_tgt, len(tgt_ids))\n",
    "\n",
    "#     print(f\"Max length of source sentence: {max_len_src}\")\n",
    "#     print(f\"Max length of target sentence: {max_len_tgt}\")\n",
    "\n",
    "#     train_dataloader = DataLoader(\n",
    "#         train_ds, batch_size=config[\"batch_size\"], shuffle=True\n",
    "#     )\n",
    "#     val_dataloader = DataLoader(val_ds, batch_size=1, shuffle=True)\n",
    "\n",
    "#     return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for item in train_dataloader:\n",
    "#     print(item[\"encoder_input\"].shape)\n",
    "#     print(item[\"decoder_input\"].shape)\n",
    "#     print(item[\"encoder_mask\"].shape)\n",
    "#     print(item[\"decoder_mask\"].shape)\n",
    "\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_model(config, vocab_src_len, vocab_tgt_len):\n",
    "#     model = build_transformer(\n",
    "#         vocab_src_len,\n",
    "#         vocab_tgt_len,\n",
    "#         config[\"seq_len\"],\n",
    "#         config[\"seq_len\"],\n",
    "#         d_model=config[\"d_model\"],\n",
    "#     )\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_model(config):\n",
    "#     # Define the device\n",
    "#     device = (\n",
    "#         \"cuda\"\n",
    "#         if torch.cuda.is_available()\n",
    "#         else \"mps\" if torch.has_mps or torch.backends.mps.is_available() else \"cpu\"\n",
    "#     )\n",
    "#     print(\"Using device:\", device)\n",
    "#     if device == \"cuda\":\n",
    "#         print(f\"Device name: {torch.cuda.get_device_name(device.index)}\")\n",
    "#         print(\n",
    "#             f\"Device memory: {torch.cuda.get_device_properties(device.index).total_memory / 1024 ** 3} GB\"\n",
    "#         )\n",
    "#     elif device == \"mps\":\n",
    "#         print(f\"Device name: <mps>\")\n",
    "#     else:\n",
    "#         print(\"NOTE: If you have a GPU, consider using it for training.\")\n",
    "#         print(\n",
    "#             \"      On a Windows machine with NVidia GPU, check this video: https://www.youtube.com/watch?v=GMSjDTU8Zlc\"\n",
    "#         )\n",
    "#         print(\n",
    "#             \"      On a Mac machine, run: pip3 install --pre torch torchvision torchaudio torchtext --index-url https://download.pytorch.org/whl/nightly/cpu\"\n",
    "#         )\n",
    "#     device = torch.device(device)\n",
    "\n",
    "#     # Make sure the weights folder exists\n",
    "#     Path(f\"{config['datasource']}_{config['model_folder']}\").mkdir(\n",
    "#         parents=True, exist_ok=True\n",
    "#     )\n",
    "\n",
    "#     train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n",
    "#     model = get_model(\n",
    "#         config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()\n",
    "#     ).to(device)\n",
    "#     # Tensorboard\n",
    "#     writer = SummaryWriter(config[\"experiment_name\"])\n",
    "\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=config[\"lr\"], eps=1e-9)\n",
    "\n",
    "#     # If the user specified a model to preload before training, load it\n",
    "#     initial_epoch = 0\n",
    "#     global_step = 0\n",
    "#     preload = config[\"preload\"]\n",
    "#     model_filename = (\n",
    "#         latest_weights_file_path(config)\n",
    "#         if preload == \"latest\"\n",
    "#         else get_weights_file_path(config, preload) if preload else None\n",
    "#     )\n",
    "#     if model_filename:\n",
    "#         print(f\"Preloading model {model_filename}\")\n",
    "#         state = torch.load(model_filename)\n",
    "#         model.load_state_dict(state[\"model_state_dict\"])\n",
    "#         initial_epoch = state[\"epoch\"] + 1\n",
    "#         optimizer.load_state_dict(state[\"optimizer_state_dict\"])\n",
    "#         global_step = state[\"global_step\"]\n",
    "#     else:\n",
    "#         print(\"No model to preload, starting from scratch\")\n",
    "\n",
    "#     loss_fn = nn.CrossEntropyLoss(\n",
    "#         ignore_index=tokenizer_src.token_to_id(\"[PAD]\"), label_smoothing=0.1\n",
    "#     ).to(device)\n",
    "\n",
    "#     for epoch in range(initial_epoch, config[\"num_epochs\"]):\n",
    "#         torch.cuda.empty_cache()\n",
    "#         model.train()\n",
    "#         batch_iterator = tqdm(train_dataloader, desc=f\"Processing Epoch {epoch:02d}\")\n",
    "#         for batch in batch_iterator:\n",
    "\n",
    "#             encoder_input = batch[\"encoder_input\"].to(device)  # (b, seq_len)\n",
    "#             decoder_input = batch[\"decoder_input\"].to(device)  # (B, seq_len)\n",
    "#             encoder_mask = batch[\"encoder_mask\"].to(device)  # (B, 1, 1, seq_len)\n",
    "#             decoder_mask = batch[\"decoder_mask\"].to(device)  # (B, 1, seq_len, seq_len)\n",
    "\n",
    "#             # Run the tensors through the encoder, decoder and the projection layer\n",
    "#             encoder_output = model.encode(\n",
    "#                 encoder_input, encoder_mask\n",
    "#             )  # (B, seq_len, d_model)\n",
    "#             decoder_output = model.decode(\n",
    "#                 encoder_output, encoder_mask, decoder_input, decoder_mask\n",
    "#             )  # (B, seq_len, d_model)\n",
    "#             proj_output = model.project(decoder_output)  # (B, seq_len, vocab_size)\n",
    "\n",
    "#             # Compare the output with the label\n",
    "#             label = batch[\"label\"].to(device)  # (B, seq_len)\n",
    "\n",
    "#             # Compute the loss using a simple cross entropy\n",
    "#             loss = loss_fn(\n",
    "#                 proj_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1)\n",
    "#             )\n",
    "#             batch_iterator.set_postfix({\"loss\": f\"{loss.item():6.3f}\"})\n",
    "\n",
    "#             # Log the loss\n",
    "#             writer.add_scalar(\"train loss\", loss.item(), global_step)\n",
    "#             writer.flush()\n",
    "\n",
    "#             # Backpropagate the loss\n",
    "#             loss.backward()\n",
    "\n",
    "#             # Update the weights\n",
    "#             optimizer.step()\n",
    "#             optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "#             global_step += 1\n",
    "\n",
    "#         # Run validation at the end of every epoch\n",
    "#         run_validation(\n",
    "#             model,\n",
    "#             val_dataloader,\n",
    "#             tokenizer_src,\n",
    "#             tokenizer_tgt,\n",
    "#             config[\"seq_len\"],\n",
    "#             device,\n",
    "#             lambda msg: batch_iterator.write(msg),\n",
    "#             global_step,\n",
    "#             writer,\n",
    "#         )\n",
    "\n",
    "#         # Save the model at the end of every epoch\n",
    "#         model_filename = get_weights_file_path(config, f\"{epoch:02d}\")\n",
    "#         torch.save(\n",
    "#             {\n",
    "#                 \"epoch\": epoch,\n",
    "#                 \"model_state_dict\": model.state_dict(),\n",
    "#                 \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "#                 \"global_step\": global_step,\n",
    "#             },\n",
    "#             model_filename,\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_model(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##################################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copying the code and testing it - https://github.com/soutrik71/pytorch-transformer/tree/main "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "\n",
    "class LayerNormalization(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, eps: float = 10**-6) -> None:\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.alpha = nn.Parameter(\n",
    "            torch.ones(features)\n",
    "        )  # alpha is a learnable parameter\n",
    "        self.bias = nn.Parameter(torch.zeros(features))  # bias is a learnable parameter\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, hidden_size)\n",
    "        # Keep the dimension for broadcasting\n",
    "        mean = x.mean(dim=-1, keepdim=True)  # (batch, seq_len, 1)\n",
    "        # Keep the dimension for broadcasting\n",
    "        std = x.std(dim=-1, keepdim=True)  # (batch, seq_len, 1)\n",
    "        # eps is to prevent dividing by zero or when std is very small\n",
    "        return self.alpha * (x - mean) / (std + self.eps) + self.bias\n",
    "\n",
    "\n",
    "class FeedForwardBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff)  # w1 and b1\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model)  # w2 and b2\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, d_ff) --> (batch, seq_len, d_model)\n",
    "        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))\n",
    "\n",
    "\n",
    "class InputEmbeddings(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, vocab_size: int) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch, seq_len) --> (batch, seq_len, d_model)\n",
    "        # Multiply by sqrt(d_model) to scale the embeddings according to the paper\n",
    "        return self.embedding(x) * math.sqrt(self.d_model)\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Create a matrix of shape (seq_len, d_model)\n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "        # Create a vector of shape (seq_len)\n",
    "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(\n",
    "            1\n",
    "        )  # (seq_len, 1)\n",
    "        # Create a vector of shape (d_model)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
    "        )  # (d_model / 2)\n",
    "        # Apply sine to even indices\n",
    "        pe[:, 0::2] = torch.sin(\n",
    "            position * div_term\n",
    "        )  # sin(position * (10000 ** (2i / d_model))\n",
    "        # Apply cosine to odd indices\n",
    "        pe[:, 1::2] = torch.cos(\n",
    "            position * div_term\n",
    "        )  # cos(position * (10000 ** (2i / d_model))\n",
    "        # Add a batch dimension to the positional encoding\n",
    "        pe = pe.unsqueeze(0)  # (1, seq_len, d_model)\n",
    "        # Register the positional encoding as a buffer\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + (self.pe[:, : x.shape[1], :]).requires_grad_(\n",
    "            False\n",
    "        )  # (batch, seq_len, d_model)\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class ResidualConnection(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = LayerNormalization(features)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "\n",
    "\n",
    "class MultiHeadAttentionBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, h: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model  # Embedding vector size\n",
    "        self.h = h  # Number of heads\n",
    "        # Make sure d_model is divisible by h\n",
    "        assert d_model % h == 0, \"d_model is not divisible by h\"\n",
    "\n",
    "        self.d_k = d_model // h  # Dimension of vector seen by each head\n",
    "        self.w_q = nn.Linear(d_model, d_model, bias=False)  # Wq\n",
    "        self.w_k = nn.Linear(d_model, d_model, bias=False)  # Wk\n",
    "        self.w_v = nn.Linear(d_model, d_model, bias=False)  # Wv\n",
    "        self.w_o = nn.Linear(d_model, d_model, bias=False)  # Wo\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    @staticmethod\n",
    "    def attention(query, key, value, mask, dropout: nn.Dropout):\n",
    "        d_k = query.shape[-1]\n",
    "        # Just apply the formula from the paper\n",
    "        # (batch, h, seq_len, d_k) --> (batch, h, seq_len, seq_len)\n",
    "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            # Write a very low value (indicating -inf) to the positions where mask == 0\n",
    "            attention_scores.masked_fill_(mask == 0, -1e9)\n",
    "        attention_scores = attention_scores.softmax(\n",
    "            dim=-1\n",
    "        )  # (batch, h, seq_len, seq_len) # Apply softmax\n",
    "        if dropout is not None:\n",
    "            attention_scores = dropout(attention_scores)\n",
    "        # (batch, h, seq_len, seq_len) --> (batch, h, seq_len, d_k)\n",
    "        # return attention scores which can be used for visualization\n",
    "        return (attention_scores @ value), attention_scores\n",
    "\n",
    "    def forward(self, q, k, v, mask):\n",
    "        query = self.w_q(q)  # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        key = self.w_k(k)  # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        value = self.w_v(v)  # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, h, d_k) --> (batch, h, seq_len, d_k)\n",
    "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(\n",
    "            1, 2\n",
    "        )\n",
    "        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(\n",
    "            1, 2\n",
    "        )\n",
    "\n",
    "        # Calculate attention\n",
    "        x, self.attention_scores = MultiHeadAttentionBlock.attention(\n",
    "            query, key, value, mask, self.dropout\n",
    "        )\n",
    "\n",
    "        # Combine all the heads together\n",
    "        # (batch, h, seq_len, d_k) --> (batch, seq_len, h, d_k) --> (batch, seq_len, d_model)\n",
    "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
    "\n",
    "        # Multiply by Wo\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        return self.w_o(x)\n",
    "\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        features: int,\n",
    "        self_attention_block: MultiHeadAttentionBlock,\n",
    "        feed_forward_block: FeedForwardBlock,\n",
    "        dropout: float,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connections = nn.ModuleList(\n",
    "            [ResidualConnection(features, dropout) for _ in range(2)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x, src_mask):\n",
    "        x = self.residual_connections[0](\n",
    "            x, lambda x: self.self_attention_block(x, x, x, src_mask)\n",
    "        )\n",
    "        x = self.residual_connections[1](x, self.feed_forward_block)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization(features)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        features: int,\n",
    "        self_attention_block: MultiHeadAttentionBlock,\n",
    "        cross_attention_block: MultiHeadAttentionBlock,\n",
    "        feed_forward_block: FeedForwardBlock,\n",
    "        dropout: float,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.cross_attention_block = cross_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connections = nn.ModuleList(\n",
    "            [ResidualConnection(features, dropout) for _ in range(3)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        x = self.residual_connections[0](\n",
    "            x, lambda x: self.self_attention_block(x, x, x, tgt_mask)\n",
    "        )\n",
    "        x = self.residual_connections[1](\n",
    "            x,\n",
    "            lambda x: self.cross_attention_block(\n",
    "                x, encoder_output, encoder_output, src_mask\n",
    "            ),\n",
    "        )\n",
    "        x = self.residual_connections[2](x, self.feed_forward_block)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization(features)\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "\n",
    "class ProjectionLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, vocab_size) -> None:\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x) -> None:\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, vocab_size)\n",
    "        return self.proj(x)\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder: Encoder,\n",
    "        decoder: Decoder,\n",
    "        src_embed: InputEmbeddings,\n",
    "        tgt_embed: InputEmbeddings,\n",
    "        src_pos: PositionalEncoding,\n",
    "        tgt_pos: PositionalEncoding,\n",
    "        projection_layer: ProjectionLayer,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.src_pos = src_pos\n",
    "        self.tgt_pos = tgt_pos\n",
    "        self.projection_layer = projection_layer\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        # (batch, seq_len, d_model)\n",
    "        src = self.src_embed(src)\n",
    "        src = self.src_pos(src)\n",
    "        return self.encoder(src, src_mask)\n",
    "\n",
    "    def decode(\n",
    "        self,\n",
    "        encoder_output: torch.Tensor,\n",
    "        src_mask: torch.Tensor,\n",
    "        tgt: torch.Tensor,\n",
    "        tgt_mask: torch.Tensor,\n",
    "    ):\n",
    "        # (batch, seq_len, d_model)\n",
    "        tgt = self.tgt_embed(tgt)\n",
    "        tgt = self.tgt_pos(tgt)\n",
    "        return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
    "\n",
    "    def project(self, x):\n",
    "        # (batch, seq_len, vocab_size)\n",
    "        return self.projection_layer(x)\n",
    "\n",
    "\n",
    "def build_transformer(\n",
    "    src_vocab_size: int,\n",
    "    tgt_vocab_size: int,\n",
    "    src_seq_len: int,\n",
    "    tgt_seq_len: int,\n",
    "    d_model: int = 512,\n",
    "    N: int = 6,\n",
    "    h: int = 8,\n",
    "    dropout: float = 0.1,\n",
    "    d_ff: int = 2048,\n",
    ") -> Transformer:\n",
    "    # Create the embedding layers\n",
    "    src_embed = InputEmbeddings(d_model, src_vocab_size)\n",
    "    tgt_embed = InputEmbeddings(d_model, tgt_vocab_size)\n",
    "\n",
    "    # Create the positional encoding layers\n",
    "    src_pos = PositionalEncoding(d_model, src_seq_len, dropout)\n",
    "    tgt_pos = PositionalEncoding(d_model, tgt_seq_len, dropout)\n",
    "\n",
    "    # Create the encoder blocks\n",
    "    encoder_blocks = []\n",
    "    for _ in range(N):\n",
    "        encoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "        encoder_block = EncoderBlock(\n",
    "            d_model, encoder_self_attention_block, feed_forward_block, dropout\n",
    "        )\n",
    "        encoder_blocks.append(encoder_block)\n",
    "\n",
    "    # Create the decoder blocks\n",
    "    decoder_blocks = []\n",
    "    for _ in range(N):\n",
    "        decoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        decoder_cross_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "        decoder_block = DecoderBlock(\n",
    "            d_model,\n",
    "            decoder_self_attention_block,\n",
    "            decoder_cross_attention_block,\n",
    "            feed_forward_block,\n",
    "            dropout,\n",
    "        )\n",
    "        decoder_blocks.append(decoder_block)\n",
    "\n",
    "    # Create the encoder and decoder\n",
    "    encoder = Encoder(d_model, nn.ModuleList(encoder_blocks))\n",
    "    decoder = Decoder(d_model, nn.ModuleList(decoder_blocks))\n",
    "\n",
    "    # Create the projection layer\n",
    "    projection_layer = ProjectionLayer(d_model, tgt_vocab_size)\n",
    "\n",
    "    # Create the transformer\n",
    "    transformer = Transformer(\n",
    "        encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer\n",
    "    )\n",
    "\n",
    "    # Initialize the parameters\n",
    "    for p in transformer.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "\n",
    "    return transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class BilingualDataset(Dataset):\n",
    "\n",
    "    def __init__(self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        self.ds = ds\n",
    "        self.tokenizer_src = tokenizer_src\n",
    "        self.tokenizer_tgt = tokenizer_tgt\n",
    "        self.src_lang = src_lang\n",
    "        self.tgt_lang = tgt_lang\n",
    "\n",
    "        self.sos_token = torch.tensor(\n",
    "            [tokenizer_tgt.token_to_id(\"[SOS]\")], dtype=torch.int64\n",
    "        )\n",
    "        self.eos_token = torch.tensor(\n",
    "            [tokenizer_tgt.token_to_id(\"[EOS]\")], dtype=torch.int64\n",
    "        )\n",
    "        self.pad_token = torch.tensor(\n",
    "            [tokenizer_tgt.token_to_id(\"[PAD]\")], dtype=torch.int64\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_target_pair = self.ds[idx]\n",
    "        src_text = src_target_pair[\"translation\"][self.src_lang]\n",
    "        tgt_text = src_target_pair[\"translation\"][self.tgt_lang]\n",
    "\n",
    "        # Transform the text into tokens\n",
    "        enc_input_tokens = self.tokenizer_src.encode(src_text).ids\n",
    "        dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n",
    "\n",
    "        # Add sos, eos and padding to each sentence\n",
    "        enc_num_padding_tokens = (\n",
    "            self.seq_len - len(enc_input_tokens) - 2\n",
    "        )  # We will add <s> and </s>\n",
    "        # We will only add <s>, and </s> only on the label\n",
    "        dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1\n",
    "\n",
    "        # Make sure the number of padding tokens is not negative. If it is, the sentence is too long\n",
    "        if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:\n",
    "            raise ValueError(\"Sentence is too long\")\n",
    "\n",
    "        # Add <s> and </s> token\n",
    "        encoder_input = torch.cat(\n",
    "            [\n",
    "                self.sos_token,\n",
    "                torch.tensor(enc_input_tokens, dtype=torch.int64),\n",
    "                self.eos_token,\n",
    "                torch.tensor(\n",
    "                    [self.pad_token] * enc_num_padding_tokens, dtype=torch.int64\n",
    "                ),\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        # Add only <s> token\n",
    "        decoder_input = torch.cat(\n",
    "            [\n",
    "                self.sos_token,\n",
    "                torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
    "                torch.tensor(\n",
    "                    [self.pad_token] * dec_num_padding_tokens, dtype=torch.int64\n",
    "                ),\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        # Add only </s> token\n",
    "        label = torch.cat(\n",
    "            [\n",
    "                torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
    "                self.eos_token,\n",
    "                torch.tensor(\n",
    "                    [self.pad_token] * dec_num_padding_tokens, dtype=torch.int64\n",
    "                ),\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        # Double check the size of the tensors to make sure they are all seq_len long\n",
    "        assert encoder_input.size(0) == self.seq_len\n",
    "        assert decoder_input.size(0) == self.seq_len\n",
    "        assert label.size(0) == self.seq_len\n",
    "\n",
    "        return {\n",
    "            \"encoder_input\": encoder_input,  # (seq_len)\n",
    "            \"decoder_input\": decoder_input,  # (seq_len)\n",
    "            \"encoder_mask\": (encoder_input != self.pad_token)\n",
    "            .unsqueeze(0)\n",
    "            .unsqueeze(0)\n",
    "            .int(),  # (1, 1, seq_len)\n",
    "            \"decoder_mask\": (decoder_input != self.pad_token).unsqueeze(0).int()\n",
    "            & causal_mask(\n",
    "                decoder_input.size(0)\n",
    "            ),  # (1, seq_len) & (1, seq_len, seq_len),\n",
    "            \"label\": label,  # (seq_len)\n",
    "            \"src_text\": src_text,\n",
    "            \"tgt_text\": tgt_text,\n",
    "        }\n",
    "\n",
    "\n",
    "def causal_mask(size):\n",
    "    mask = torch.triu(torch.ones((1, size, size)), diagonal=1).type(torch.int)\n",
    "    return mask == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def get_config():\n",
    "    return {\n",
    "        \"batch_size\": 16,\n",
    "        \"num_epochs\": 10,\n",
    "        \"lr\": 10**-4,\n",
    "        \"seq_len\": 350,\n",
    "        \"d_model\": 512,\n",
    "        \"datasource\": \"opus_books\",\n",
    "        \"lang_src\": \"en\",\n",
    "        \"lang_tgt\": \"it\",\n",
    "        \"model_folder\": \"weights\",\n",
    "        \"model_basename\": \"tmodel_\",\n",
    "        \"preload\": None,\n",
    "        \"tokenizer_file\": \"tokenizer_{0}.json\",\n",
    "        \"experiment_name\": \"runs/tmodel\",\n",
    "    }\n",
    "\n",
    "\n",
    "def get_weights_file_path(config, epoch: str):\n",
    "    model_folder = config[\"model_folder\"]\n",
    "    model_basename = config[\"model_basename\"]\n",
    "    model_filename = f\"{model_basename}{epoch}.pt\"\n",
    "    return str(Path(\".\") / model_folder / model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext.datasets as datasets\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Huggingface datasets and tokenizers\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "import torchmetrics\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "def greedy_decode(\n",
    "    model, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device\n",
    "):\n",
    "    sos_idx = tokenizer_tgt.token_to_id(\"[SOS]\")\n",
    "    eos_idx = tokenizer_tgt.token_to_id(\"[EOS]\")\n",
    "\n",
    "    # Precompute the encoder output and reuse it for every step\n",
    "    encoder_output = model.encode(source, source_mask)\n",
    "    # Initialize the decoder input with the sos token\n",
    "    decoder_input = torch.empty(1, 1).fill_(sos_idx).type_as(source).to(device)\n",
    "    while True:\n",
    "        if decoder_input.size(1) == max_len:\n",
    "            break\n",
    "\n",
    "        # build mask for target\n",
    "        decoder_mask = (\n",
    "            causal_mask(decoder_input.size(1)).type_as(source_mask).to(device)\n",
    "        )\n",
    "\n",
    "        # calculate output\n",
    "        out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n",
    "\n",
    "        # get next token\n",
    "        prob = model.project(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        decoder_input = torch.cat(\n",
    "            [\n",
    "                decoder_input,\n",
    "                torch.empty(1, 1).type_as(source).fill_(next_word.item()).to(device),\n",
    "            ],\n",
    "            dim=1,\n",
    "        )\n",
    "\n",
    "        if next_word == eos_idx:\n",
    "            break\n",
    "\n",
    "    return decoder_input.squeeze(0)\n",
    "\n",
    "\n",
    "def run_validation(\n",
    "    model,\n",
    "    validation_ds,\n",
    "    tokenizer_src,\n",
    "    tokenizer_tgt,\n",
    "    max_len,\n",
    "    device,\n",
    "    print_msg,\n",
    "    global_step,\n",
    "    writer,\n",
    "    num_examples=2,\n",
    "):\n",
    "    model.eval()\n",
    "    count = 0\n",
    "\n",
    "    source_texts = []\n",
    "    expected = []\n",
    "    predicted = []\n",
    "\n",
    "    try:\n",
    "        # get the console window width\n",
    "        with os.popen(\"stty size\", \"r\") as console:\n",
    "            _, console_width = console.read().split()\n",
    "            console_width = int(console_width)\n",
    "    except:\n",
    "        # If we can't get the console width, use 80 as default\n",
    "        console_width = 80\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in validation_ds:\n",
    "            count += 1\n",
    "            encoder_input = batch[\"encoder_input\"].to(device)  # (b, seq_len)\n",
    "            encoder_mask = batch[\"encoder_mask\"].to(device)  # (b, 1, 1, seq_len)\n",
    "\n",
    "            # check that the batch size is 1\n",
    "            assert encoder_input.size(0) == 1, \"Batch size must be 1 for validation\"\n",
    "\n",
    "            model_out = greedy_decode(\n",
    "                model,\n",
    "                encoder_input,\n",
    "                encoder_mask,\n",
    "                tokenizer_src,\n",
    "                tokenizer_tgt,\n",
    "                max_len,\n",
    "                device,\n",
    "            )\n",
    "\n",
    "            source_text = batch[\"src_text\"][0]\n",
    "            target_text = batch[\"tgt_text\"][0]\n",
    "            model_out_text = tokenizer_tgt.decode(model_out.detach().cpu().numpy())\n",
    "\n",
    "            source_texts.append(source_text)\n",
    "            expected.append(target_text)\n",
    "            predicted.append(model_out_text)\n",
    "\n",
    "            # Print the source, target and model output\n",
    "            print_msg(\"-\" * console_width)\n",
    "            print_msg(f\"{f'SOURCE: ':>12}{source_text}\")\n",
    "            print_msg(f\"{f'TARGET: ':>12}{target_text}\")\n",
    "            print_msg(f\"{f'PREDICTED: ':>12}{model_out_text}\")\n",
    "\n",
    "            if count == num_examples:\n",
    "                print_msg(\"-\" * console_width)\n",
    "                break\n",
    "\n",
    "    if writer:\n",
    "        # Evaluate the character error rate\n",
    "        # Compute the char error rate\n",
    "        metric = torchmetrics.CharErrorRate()\n",
    "        cer = metric(predicted, expected)\n",
    "        writer.add_scalar(\"validation cer\", cer, global_step)\n",
    "        writer.flush()\n",
    "\n",
    "        # Compute the word error rate\n",
    "        metric = torchmetrics.WordErrorRate()\n",
    "        wer = metric(predicted, expected)\n",
    "        writer.add_scalar(\"validation wer\", wer, global_step)\n",
    "        writer.flush()\n",
    "\n",
    "        # Compute the BLEU metric\n",
    "        metric = torchmetrics.BLEUScore()\n",
    "        bleu = metric(predicted, expected)\n",
    "        writer.add_scalar(\"validation BLEU\", bleu, global_step)\n",
    "        writer.flush()\n",
    "\n",
    "\n",
    "def get_all_sentences(ds, lang):\n",
    "    for item in ds:\n",
    "        yield item[\"translation\"][lang]\n",
    "\n",
    "\n",
    "def get_or_build_tokenizer(config, ds, lang):\n",
    "    tokenizer_path = Path(config[\"tokenizer_file\"].format(lang))\n",
    "    if not Path.exists(tokenizer_path):\n",
    "        # Most code taken from: https://huggingface.co/docs/tokenizers/quicktour\n",
    "        tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "        tokenizer.pre_tokenizer = Whitespace()\n",
    "        trainer = WordLevelTrainer(\n",
    "            special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], min_frequency=2\n",
    "        )\n",
    "        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer=trainer)\n",
    "        tokenizer.save(str(tokenizer_path))\n",
    "    else:\n",
    "        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "def get_ds(config):\n",
    "    # It only has the train split, so we divide it overselves\n",
    "    ds_raw = load_dataset(\n",
    "        f\"{config['datasource']}\",\n",
    "        f\"{config['lang_src']}-{config['lang_tgt']}\",\n",
    "        split=\"train\",\n",
    "    )\n",
    "\n",
    "    # Build tokenizers\n",
    "    tokenizer_src = get_or_build_tokenizer(config, ds_raw, config[\"lang_src\"])\n",
    "    tokenizer_tgt = get_or_build_tokenizer(config, ds_raw, config[\"lang_tgt\"])\n",
    "\n",
    "    # Keep 90% for training, 10% for validation\n",
    "    train_ds_size = int(0.9 * len(ds_raw))\n",
    "    val_ds_size = len(ds_raw) - train_ds_size\n",
    "    train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size])\n",
    "\n",
    "    train_ds = BilingualDataset(\n",
    "        train_ds_raw,\n",
    "        tokenizer_src,\n",
    "        tokenizer_tgt,\n",
    "        config[\"lang_src\"],\n",
    "        config[\"lang_tgt\"],\n",
    "        config[\"seq_len\"],\n",
    "    )\n",
    "    val_ds = BilingualDataset(\n",
    "        val_ds_raw,\n",
    "        tokenizer_src,\n",
    "        tokenizer_tgt,\n",
    "        config[\"lang_src\"],\n",
    "        config[\"lang_tgt\"],\n",
    "        config[\"seq_len\"],\n",
    "    )\n",
    "\n",
    "    # Find the maximum length of each sentence in the source and target sentence\n",
    "    max_len_src = 0\n",
    "    max_len_tgt = 0\n",
    "\n",
    "    for item in ds_raw:\n",
    "        src_ids = tokenizer_src.encode(item[\"translation\"][config[\"lang_src\"]]).ids\n",
    "        tgt_ids = tokenizer_tgt.encode(item[\"translation\"][config[\"lang_tgt\"]]).ids\n",
    "        max_len_src = max(max_len_src, len(src_ids))\n",
    "        max_len_tgt = max(max_len_tgt, len(tgt_ids))\n",
    "\n",
    "    print(f\"Max length of source sentence: {max_len_src}\")\n",
    "    print(f\"Max length of target sentence: {max_len_tgt}\")\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_ds, batch_size=config[\"batch_size\"], shuffle=True\n",
    "    )\n",
    "    val_dataloader = DataLoader(val_ds, batch_size=1, shuffle=True)\n",
    "\n",
    "    return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt\n",
    "\n",
    "\n",
    "def get_model(config, vocab_src_len, vocab_tgt_len):\n",
    "    model = build_transformer(\n",
    "        vocab_src_len,\n",
    "        vocab_tgt_len,\n",
    "        config[\"seq_len\"],\n",
    "        config[\"seq_len\"],\n",
    "        d_model=config[\"d_model\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_model(config):\n",
    "    # Define the device\n",
    "    device = (\n",
    "        \"cuda\"\n",
    "        if torch.cuda.is_available()\n",
    "        else \"mps\" if torch.has_mps or torch.backends.mps.is_available() else \"cpu\"\n",
    "    )\n",
    "    print(\"Using device:\", device)\n",
    "    if device == \"cuda\":\n",
    "        print(f\"Device name: {torch.cuda.get_device_name(device.index)}\")\n",
    "        print(\n",
    "            f\"Device memory: {torch.cuda.get_device_properties(device.index).total_memory / 1024 ** 3} GB\"\n",
    "        )\n",
    "    elif device == \"mps\":\n",
    "        print(f\"Device name: <mps>\")\n",
    "    else:\n",
    "        print(\"NOTE: If you have a GPU, consider using it for training.\")\n",
    "        print(\n",
    "            \"      On a Windows machine with NVidia GPU, check this video: https://www.youtube.com/watch?v=GMSjDTU8Zlc\"\n",
    "        )\n",
    "        print(\n",
    "            \"      On a Mac machine, run: pip3 install --pre torch torchvision torchaudio torchtext --index-url https://download.pytorch.org/whl/nightly/cpu\"\n",
    "        )\n",
    "    device = torch.device(device)\n",
    "\n",
    "    # Make sure the weights folder exists\n",
    "    Path(f\"{config['datasource']}_{config['model_folder']}\").mkdir(\n",
    "        parents=True, exist_ok=True\n",
    "    )\n",
    "\n",
    "    train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n",
    "    model = get_model(\n",
    "        config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()\n",
    "    ).to(device)\n",
    "    # Tensorboard\n",
    "    writer = SummaryWriter(config[\"experiment_name\"])\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config[\"lr\"], eps=1e-9)\n",
    "\n",
    "    # If the user specified a model to preload before training, load it\n",
    "    initial_epoch = 0\n",
    "    global_step = 0\n",
    "    preload = config[\"preload\"]\n",
    "    model_filename = (\n",
    "        latest_weights_file_path(config)\n",
    "        if preload == \"latest\"\n",
    "        else get_weights_file_path(config, preload) if preload else None\n",
    "    )\n",
    "    if model_filename:\n",
    "        print(f\"Preloading model {model_filename}\")\n",
    "        state = torch.load(model_filename)\n",
    "        model.load_state_dict(state[\"model_state_dict\"])\n",
    "        initial_epoch = state[\"epoch\"] + 1\n",
    "        optimizer.load_state_dict(state[\"optimizer_state_dict\"])\n",
    "        global_step = state[\"global_step\"]\n",
    "    else:\n",
    "        print(\"No model to preload, starting from scratch\")\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss(\n",
    "        ignore_index=tokenizer_src.token_to_id(\"[PAD]\"), label_smoothing=0.1\n",
    "    ).to(device)\n",
    "\n",
    "    for epoch in range(initial_epoch, config[\"num_epochs\"]):\n",
    "        torch.cuda.empty_cache()\n",
    "        model.train()\n",
    "        batch_iterator = tqdm(train_dataloader, desc=f\"Processing Epoch {epoch:02d}\")\n",
    "        for batch in batch_iterator:\n",
    "\n",
    "            encoder_input = batch[\"encoder_input\"].to(device)  # (b, seq_len)\n",
    "            decoder_input = batch[\"decoder_input\"].to(device)  # (B, seq_len)\n",
    "            encoder_mask = batch[\"encoder_mask\"].to(device)  # (B, 1, 1, seq_len)\n",
    "            decoder_mask = batch[\"decoder_mask\"].to(device)  # (B, 1, seq_len, seq_len)\n",
    "\n",
    "            # Run the tensors through the encoder, decoder and the projection layer\n",
    "            encoder_output = model.encode(\n",
    "                encoder_input, encoder_mask\n",
    "            )  # (B, seq_len, d_model)\n",
    "            decoder_output = model.decode(\n",
    "                encoder_output, encoder_mask, decoder_input, decoder_mask\n",
    "            )  # (B, seq_len, d_model)\n",
    "            proj_output = model.project(decoder_output)  # (B, seq_len, vocab_size)\n",
    "\n",
    "            # Compare the output with the label\n",
    "            label = batch[\"label\"].to(device)  # (B, seq_len)\n",
    "\n",
    "            # Compute the loss using a simple cross entropy\n",
    "            loss = loss_fn(\n",
    "                proj_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1)\n",
    "            )\n",
    "            batch_iterator.set_postfix({\"loss\": f\"{loss.item():6.3f}\"})\n",
    "\n",
    "            # Log the loss\n",
    "            writer.add_scalar(\"train loss\", loss.item(), global_step)\n",
    "            writer.flush()\n",
    "\n",
    "            # Backpropagate the loss\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the weights\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            global_step += 1\n",
    "\n",
    "        # Run validation at the end of every epoch\n",
    "        run_validation(\n",
    "            model,\n",
    "            val_dataloader,\n",
    "            tokenizer_src,\n",
    "            tokenizer_tgt,\n",
    "            config[\"seq_len\"],\n",
    "            device,\n",
    "            lambda msg: batch_iterator.write(msg),\n",
    "            global_step,\n",
    "            writer,\n",
    "        )\n",
    "\n",
    "        # # Save the model at the end of every epoch\n",
    "        # model_filename = get_weights_file_path(config, f\"{epoch:02d}\")\n",
    "        # torch.save(\n",
    "        #     {\n",
    "        #         \"epoch\": epoch,\n",
    "        #         \"model_state_dict\": model.state_dict(),\n",
    "        #         \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        #         \"global_step\": global_step,\n",
    "        #     },\n",
    "        #     model_filename,\n",
    "        # )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Device name: Tesla V100-PCIE-16GB\n",
      "Device memory: 15.7725830078125 GB\n",
      "Max length of source sentence: 309\n",
      "Max length of target sentence: 274\n",
      "No model to preload, starting from scratch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 00: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1819/1819 [08:24<00:00,  3.61it/s, loss=5.947]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: As in the game of cat and mouse, the arms that were raised to allow him to get inside the circle were at once lowered to prevent Anna from entering.\n",
      "    TARGET: Come nel giuoco del gatto e del topo, le braccia sollevate per lui, si abbassavano immediatamente dinanzi ad Anna.\n",
      " PREDICTED: Il suo momento , e il suo momento , e il suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo suo sorriso .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Wonderful civility this!\n",
      "    TARGET: Che gentilezza sorprendente!\n",
      " PREDICTED: Ma non è mai ?\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/torch_env/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:62: FutureWarning: Importing `CharErrorRate` from `torchmetrics` was deprecated and will be removed in 2.0. Import `CharErrorRate` from `torchmetrics.text` instead.\n",
      "  _future_warning(\n",
      "/anaconda/envs/torch_env/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:62: FutureWarning: Importing `WordErrorRate` from `torchmetrics` was deprecated and will be removed in 2.0. Import `WordErrorRate` from `torchmetrics.text` instead.\n",
      "  _future_warning(\n",
      "/anaconda/envs/torch_env/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:62: FutureWarning: Importing `BLEUScore` from `torchmetrics` was deprecated and will be removed in 2.0. Import `BLEUScore` from `torchmetrics.text` instead.\n",
      "  _future_warning(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Parent directory weights does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[18], line 354\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# Save the model at the end of every epoch\u001b[39;00m\n\u001b[1;32m    353\u001b[0m model_filename \u001b[38;5;241m=\u001b[39m get_weights_file_path(config, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m02d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 354\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_state_dict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moptimizer_state_dict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mglobal_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mglobal_step\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/torch_env/lib/python3.10/site-packages/torch/serialization.py:628\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    625\u001b[0m _check_save_filelike(f)\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m--> 628\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    629\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)\n\u001b[1;32m    630\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m/anaconda/envs/torch_env/lib/python3.10/site-packages/torch/serialization.py:502\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[0;34m(name_or_buffer)\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    501\u001b[0m     container \u001b[38;5;241m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[0;32m--> 502\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/torch_env/lib/python3.10/site-packages/torch/serialization.py:473\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mPyTorchFileWriter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream))\n\u001b[1;32m    472\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 473\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Parent directory weights does not exist."
     ]
    }
   ],
   "source": [
    "train_model(get_config())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
